### 安装指定版本的mariadb
[mariadb]
name = MariaDB-10.3.18
#baseurl=http://yum.mariadb.org/10.3.18/centos7-amd64
baseurl=http://archive.mariadb.org/mariadb-10.3.18/yum/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=0


####### empty database ########
change master to master_host = '10.67.38.253', master_user = 'repl', master_password = 'copy', master_port = 3306, master_use_gtid = slave_pos;
start slave;


同步失败, 恢复slave
# on master
set @@global.read_only=1;
# on slave
stop slave;
set @@global.gtid_slave_pos='0-1-46733'
change master to master_host = '10.67.38.252', master_user = 'repl', master_password = 'copy', master_port = 3306, master_use_gtid = slave_pos;
start slave;
# on master
set @@global.read_only=0;


###### mysql partitioning #######
#  mysql -uroot -pFoxconn123 zabbix <partition_call.sql
#  mysql -uroot -pFoxconn123 zabbix <partition_all.sql
#  ./partition.sh


########  maxscale  ##########
docker run -d --name mxs \
-v:/root/maxscale.cnf:/etc/maxscale.cnf.d/my-maxscale.cnf \
-p 8989:8989 \
-p 4006:4006 \
mariadb/maxscale:2.3

# Creating a user account for MaxScale
CREATE USER 'maxscale'@'%' IDENTIFIED BY 'vSTJ456';
GRANT SELECT ON mysql.user TO 'maxscale'@'%';
GRANT SELECT ON mysql.db TO 'maxscale'@'%';
GRANT SELECT ON mysql.tables_priv TO 'maxscale'@'%';
GRANT SELECT ON mysql.roles_mapping TO 'maxscale'@'%';
GRANT SHOW DATABASES ON *.* TO 'maxscale'@'%';
# Creating a monitor user account for MaxScale
CREATE USER 'monitor_user'@'%' IDENTIFIED BY 'my_password';
GRANT REPLICATION CLIENT on *.* to 'monitor_user'@'%';
# If the automatic failover of the MariaDB Monitor will used, the user will require additional grants. 
GRANT SUPER, RELOAD on *.* to 'monitor_user'@'%';

RESET SLAVE makes the slave forget its replication position in the master's binary log. This statement is meant to be used for a clean start. 
It deletes the master.info and relay-log.info files, all the relay log files, and starts a new relay log file.

###### set maxscale passive ########
[root@worker2 ~]# docker exec mxs maxctrl show maxscale |grep passive
│              │     "passive": false,                                                │
[root@worker2 ~]# docker exec mxs maxctrl alter maxscale passive true
OK
[root@worker2 ~]# docker exec mxs maxctrl show maxscale |grep passive
│              │     "passive": true,                                                 │



################# galera 集群 5月2日 整体挂掉#############
故障原因    galear三台节点磁盘100% full
解决过程:
1 先扩容
2 找到能成为primary的db02，  show status like 'wsrep_last_committed'，db02拥有最高的数值， galera_new_cluster
3  db01的mariadb服务无法启动，原因galera同步800G数据肯定超时
 解决  先删除2,3,4月份的监控数据，然后增加override.conf 
 /etc/systemd/system/mariadb.service.d/override.conf
[Service]
TimeoutStartSec=24min
4  db01服务成功启动,  wsrep状态为syncd,  cluster size为2
5 db02的mariadb服务无法启动，提示:
May  4 16:54:09 vstjzabdb03 kernel: mysqld[2365]: segfault at fffffffffffffffb ip 000055950dc5c9d4 sp 00007fc7d462e280 error 5 in mysqld[55950d2bb000+12c4000]
May  5 08:55:46 vstjzabdb03 kernel: mysqld[3446]: segfault at fffffffffffffffb ip 00005605164a89d4 sp 00007fa5a0402280 error 5 in mysqld[560515b07000+12c4000]
怎么重启mariadb服务都会出现段错误，接着进行下面的调试，mariadb意外启动成功
#################### gdb 调试  #######
[root@vstjzabdb03 log]# gdb /usr/sbin/mysqld
GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-115.el7
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-redhat-linux-gnu".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /usr/sbin/mysqld...Reading symbols from /usr/sbin/mysqld...(no debugging symbols found)...done.
(no debugging symbols found)...done.
Missing separate debuginfos, use: debuginfo-install MariaDB-server-10.3.18-1.el7.centos.x86_64
(gdb) run --defaults-file=/etc/my.cnf.d/server.cnf --datadir=/data/mysql --user=mysql --pid-file=/data/mysql/vstjzabdb03.pid --sock=/data/mysql/mysql.sock --port=3306 --wsrep_start_position=8d212702-f4a8-11e9-a02f-fbab2c96aed2:275806858
Starting program: /usr/sbin/mysqld --defaults-file=/etc/my.cnf.d/server.cnf --datadir=/data/mysql --user=mysql --pid-file=/data/mysql/vstjzabdb03.pid --sock=/data/mysql/mysql.sock --port=3306 --wsrep_start_position=8d212702-f4a8-11e9-a02f-fbab2c96aed2:275806858
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib64/libthread_db.so.1".
2020-05-05 16:07:53 0 [Note] Using unique option prefix 'sock' is error-prone and can break in the future. Please use the full name 'socket' instead.
2020-05-05 16:07:53 0 [Note] /usr/sbin/mysqld (mysqld 10.3.18-MariaDB-log) starting as process 4081 ...
[New Thread 0x7ffff3006700 (LWP 4085)]

########## vstjszbdb01的mariadb无法启动 #########
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [ERROR] InnoDB: Unable to lock /data/mysql/ibdata1 error: 11
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [Note] InnoDB: Check that you do not already have another mysqld process using the same InnoDB data or log files.
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [Note] InnoDB: Unable to open the first data file
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [ERROR] InnoDB: Operating system error number 11 in a file operation.
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [ERROR] InnoDB: Error number 11 means 'Resource temporarily unavailable'
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [Note] InnoDB: Some operating system error numbers are described at https://mariadb.com/kb/en/library/operating-syste
m-error-codes/
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [ERROR] InnoDB: Cannot open datafile '/data/mysql/ibdata1'
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [ERROR] InnoDB: Could not open or create the system tablespace. If you tried to add new data files to the system tabl
espace, and it failed here, you should now edit innodb_data_file_path in my.cnf back to what it was, and remove the new ibdata files InnoDB created in this failed attempt.
 InnoDB only wrote those files full of zeros, but did not yet use them in any way. But be careful: do not remove old data files which contain your precious data!
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:44 0 [ERROR] InnoDB: Plugin initialization aborted with error Cannot open a file
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:45 0 [Note] InnoDB: Starting shutdown...
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:46 0 [ERROR] Plugin 'InnoDB' init function returned error.
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:46 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:46 0 [Note] Plugin 'FEEDBACK' is disabled.
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:46 0 [ERROR] Unknown/unsupported storage engine: InnoDB
May 11 13:59:46 vstjzabdb01 sh: 2020-05-11 13:59:46 0 [ERROR] Aborting'


#######################Full Cluster Recovery#################

In this scenario, all nodes failed or did not gracefully shutdown. Total loss of quorum occurred and the cluster is not accepting any SQL requests. After a hard crash such as this, 
even if all the nodes come back online the MariaDB service will be unable to start. This is due to the unclean shutdown and none of the nodes were able to do the last commit. 
A Galera cluster can crash in various ways resulting in different methods to recover from a full crash.  

Recovery Based On Highest seqno Value
This method is helpful when there is a slight possibility that at least one node was able to gracefully shutdown during the crash. The node with the latest data will have the highest seqno
 value among all the crashed nodes. We can find the clue in the content of which/var/lib/mysql/grastate.dat will show the value of seqno. Depending on the nature of crash either all of the
 nodes will have identical negative seqno value or one of the nodes will have the highest positive seqno value.

Following shows the content of grastate.dat in node 3. This node has negative seqno and no group ID. This the case when a node crashes during Data Definition Language (DDL) processing:

$ cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid: 00000000-0000-0000-0000-000000000000
seqno: -1
safe_to_bootstrap: 0
Following shows the content of grastate.dat in node 2. This node crashed during transaction processing resulting in negative seqno but with group ID:

$ cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid: 886dd8da-3d07-11e8-a109-8a3c80cebab4
seqno: -1
safe_to_bootstrap: 0
Following is the content of grastate.dat on node 1 with highest seqno value:

$ cat /var/lib/mysql/grastate.dat
# GALERA saved state
version: 2.1
uuid: 886dd8da-3d07-11e8-a109-8a3c80cebab4
seqno: 31929
safe_to_bootstrap: 1
Note that a node will only have positive highest seqno value when the node was able to gracefully shutdown. This is the node need to be recovered first.

If all the nodes contain the value of -1 for seqno and 0 for safe_to_bootstrap, that is an indication that a full cluster crash has occurred. At this point, 
we could start the cluster using the command galera_new_cluster. But it is not recommended at all since there is no way to know that each node has an identical copy of the database data. 

Before restarting the node 1 we need to make a change in the cluster configuration file /etc/my.cnf.d/server.cnf to remove the mention of IPs of cluster nodes. 
Following is the content of [galera] section of the configuration before any changes:

[galera]
# Mandatory settings
wsrep_on=ON
wsrep_provider=/usr/lib64/galera/libgalera_smm.so
wsrep_cluster_address="gcomm://10.0.0.51,10.0.0.52,10.0.0.53"
wsrep_cluster_name='galeraCluster01'
wsrep_node_address='10.0.0.51'
wsrep_node_name='galera-01'
wsrep_sst_method=rsync
binlog_format=row
default_storage_engine=InnoDB
innodb_autoinc_lock_mode=2
Note that wsrep_cluster_address shows the IP of all member nodes. We need to remove the addresses as follows:

wsrep_cluster_address="gcomm://"
We can now restart the mariadb service in this node:

$ systemctl restart mariadb
 Only after verifying that the service started successfully we can proceed to restart services on the other nodes one at a time. Only after all nodes are successfully running,
 we need to edit the cluster configuration on node 1 to add the IP addresses of all the member nodes and restart the service:

wsrep_cluster_address="gcomm://10.8.8.53,10.8.8.54,10.8.8.55"
The Galera cluster should be up and running at this point and all nodes should be syncing with the surviving node.  

Recovery Based On Last Committed
This is the worst case scenario of a Galera cluster crash where all nodes have completely crashed resulting seqno value of -1. As mentioned earlier, resist the temptation of 
running the command galera_new_cluster on a node then trying to rejoin rest of the nodes to the cluster before checking which node has the latest commit. When galera_new_cluster command
 is used, it actually creates a new cluster with a new set of IDs then all other nodes join into it starting clean sync. 

To check which node has the last commit we can check the value of wsrep_last_commit on each node separately. The node with the highest value is the one with the latest commit. 
We can bootstrap that node to start the cluster then join other member nodes. This process is similar to bootstraping the node with highest seqno as we have seen in the previous section. 

Stop mariadb service:

$ systemctl stop mariadb
Edit wsrep_cluster_address in [galera] section of /etc/my.cnf.d/server.cnf to remove mention of member nodes:

wsrep_cluster_address="gcomm://"
Restart mariadb service:

$ systemctl start mariadb
From database shell check the last committed value:

MariaDB [(none)]> show status like 'wsrep_last_committed';
+----------------------+---------+
| Variable_name | Value |
+----------------------+---------+
| wsrep_last_committed | 319589 |
+----------------------+---------+
1 row in set (0.01 sec)
Repeat the process on all nodes to retrieve the last committed value. The node with the latest data will have the highest value. Create a new cluster on the node with the highest committed value:

$ galera_new_cluster
Change the value of wsrep_cluster_address on rest of the nodes to mention IP addresses, then restart mariadb service one node at a time. The cluster should be up and running and data sync 
should commit all the changes. Check the last committed value on all nodes after a while to ensure nodes are now in sync. 


#######
zbxdb01 
wsrep_gtid_mode = on
wsrep_gtid_domain_id =0
server-id =01
log_slave_updates = on
log-bin = /data/mysql/master-bin
log-bin-index = /data/mysql/master-bin.index
gtid_domain_id = 1

zbxdb02
wsrep_gtid_mode = on
wsrep_gtid_domain_id =0
server-id =01
log_slave_updates = on
log-bin = /data/mysql/master-bin
log-bin-index = /data/mysql/master-bin.index
gtid_domain_id = 2

zbxdb03
wsrep_gtid_mode = on
wsrep_gtid_domain_id =0
server-id =01
log_slave_updates = on
log-bin = /data/mysql/master-bin
log-bin-index = /data/mysql/master-bin.index
gtid_domain_id = 3

slave
#Replication slave to Galera Cluster
server-id       = 02
relay-log-index = /data/mysql/slave-relay-bin.index
relay-log       = /data/mysql/slave-relay-bin
gtid_domain_id  = 99

log-bin         = /data/mysql/slave-bin
log-bin-index   = /data/mysql/slave-bin.index
binlog_format   = mixed