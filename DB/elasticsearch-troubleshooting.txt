#########search_phase_execution_exception', 'Fielddata is disabled on text fields by default. Set fielddata=true on [pattern] in order to load fielddata in memory
 by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.#############
 
发现mapping里beat.hostname  type有的是text 有的是keyword
在kibana中选择6月28日  type为text  使用beat.hostname.keyword有数据
在kibana中选择7月11日  type为keyword  使用beat.hostname.keyword无数据
根本原因: filebeat-6.8.1的模板出问题了, 导致mapping不正确   

[chensen@cobbler ~]$ curl -XGET '10.67.51.150:9200/filebeat-*/_mapping/doc/field/beat.hostname?pretty'
{
  "filebeat-6.8.1-2019.04.09" : {
    "mappings" : {
      "doc" : {
        "beat.hostname" : {
          "full_name" : "beat.hostname",
          "mapping" : {
            "hostname" : {
              "type" : "keyword",
              "ignore_above" : 1024
            }
          }
        }
      }
    }
  },
  "filebeat-6.8.1-2019.06.27" : {
    "mappings" : {
      "doc" : {
        "beat.hostname" : {
          "full_name" : "beat.hostname",
          "mapping" : {
            "hostname" : {
              "type" : "text",
              "fields" : {
                "keyword" : {
                  "type" : "keyword",
                  "ignore_above" : 256
                }
              }
            }
          }
        }
      }
    }
  },
  "filebeat-6.8.1-2019.04.02" : {
    "mappings" : {
      "doc" : {
        "beat.hostname" : {
          "full_name" : "beat.hostname",
          "mapping" : {
            "hostname" : {
              "type" : "keyword",
              "ignore_above" : 1024
            }
          }
        }
      }
    }
  },
  "filebeat-6.8.1-2019.06.28" : {
    "mappings" : {
      "doc" : {
        "beat.hostname" : {
          "full_name" : "beat.hostname",
          "mapping" : {
            "hostname" : {
              "type" : "text",
              "fields" : {
                "keyword" : {
                  "type" : "keyword",
                  "ignore_above" : 256
                }
              }
            } 
查看模板
curl -XGET http://10.67.51.150:9200/_template/filebeat-6.8.1 |python -m json.tool
删除模板
curl -XDELETE http://10.67.51.150:9200/_template/filebeat-6.8.1
重新上传模板
filebeat setup --template -E 'output.kafka.enabled=false' -E 'output.elasticsearch.hosts=["10.67.51.150:9200"]'
分别查看7月10日前和10日后的mapping, 可看出mapping不一样
[chensen@cobbler ~]$ curl -XGET '10.67.51.150:9200/filebeat-6.8.1-2019.07.0*/_mapping/doc/field/beat.hostname?pretty'
[chensen@cobbler ~]$ curl -XGET '10.67.51.150:9200/filebeat-6.8.1-2019.07.1*/_mapping/doc/field/beat.hostname?pretty'


######## elasticsearch无法启动##########
rm -f /var/lib/elasticsearch/nodes/0/indices/GAePsmg5RICuBdRS-xwLvw/_state/state-39.st
rm -f /var/lib/elasticsearch/nodes/0/indices/5z3QXemeQO-aJ7ZfnaPi2w/_state/state-45.st
rm -f /var/lib/elasticsearch/nodes/0/indices/9wMLZLv5RxGGDLDeRrb9DA/_state/state-44.st
rm -f /var/lib/elasticsearch/nodes/0/indices/Glk6dbJGSfuwh_rIuWHtXg/_state/state-41.st
rm -f /var/lib/elasticsearch/nodes/0/indices/ys5omB6OR9ykfb_QAELRfg/_state/state-46.st
rm -f /var/lib/elasticsearch/nodes/0/indices/5I9mMF3QT7yd9IQmJrFIXQ/_state/state-46.st
rm -f /var/lib/elasticsearch/nodes/0/indices/8KAsZIAcTZW9AS24OkXnig/_state/state-42.st
rm -f /var/lib/elasticsearch/nodes/0/indices/je_eLHSJR9-fD92AErDRvw/_state/state-45.st
如上，删除了很多indice的state文件，仍然会在每次重启ES后报如下错误
org.elasticsearch.bootstrap.StartupException: ElasticsearchException[java.io.IOException: 
failed to read [id:45, file:/var/lib/elasticsearch/nodes/0/indices/je_eLHSJR9-fD92AErDRvw/_state/state-45.st]]; 
nested: IOException[failed to read [id:45, file:/var/lib/elasticsearch/nodes/0/indices/je_eLHSJR9-fD92AErDRvw/_state/state-45.st]]; 
nested: XContentParseException[[-1:1123] [rollover_info] failed to parse field [met_conditions]]; 
nested: XContentParseException[[-1:1123] [met_conditions] failed to parse field [max_docs]]; 
nested: NamedObjectNotFoundException[named objects are not supported for this parser];
没有办法找到所有有问题的state文件，最后清空了/var/lib/elasticsearch/nodes/0/indices底下的所有数据，重新启动ES成功
注意模板数据会丢失


########3台ES 因配置文件错误，一段时间服务都起不来#########
服务起来后， cluster health为red

很多unassigned shards 正在恢复
[root@es1 elasticsearch]# curl -X GET 'http://10.67.36.53:9200/_cat/health?v'
epoch      timestamp cluster status node.total node.data shards  pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1559112260 14:44:20  logging yellow          3         3   6647 4211    0    6     1774            31              33.4m                 78.9%
查看任务
[root@es1 elasticsearch]# curl -X GET 'http://10.67.36.53:9200/_cat/pending_tasks?v'
insertOrder timeInQueue priority source
       8148        2.4s URGENT   shard-started StartedShardEntry{shardId [[rke-2019.03.16][3]], allocationId [XEYmklxiSiakIk74GdYZrA], message [after peer recovery]}
       8149        2.4s URGENT   shard-started StartedShardEntry{shardId [[winlogbeat-6.4.0-2019.01.29][0]], allocationId [HeqAEAZcTxuZwQ8vhiKw0Q], message [after peer recovery]}
       8150        2.2s URGENT   shard-started StartedShardEntry{shardId [[filebeat-6.4.0-2019.03.16][0]], allocationId [QQmHATusTuqZn9O5zIbcOg], message [after peer recovery]}
         86       31.4m HIGH     shard-failed
       8151        1.5s URGENT   shard-started StartedShardEntry{shardId [[winlogbeat-6.4.0-2019.01.29][0]], allocationId [HeqAEAZcTxuZwQ8vhiKw0Q], message [master {es1}{O-HeVTTtR8a4tRkZIrOx7A}{BVcWUHXdSeqLL4i9FlOvPg}{10.67.36.53}{10.67.36.53:9300}{ml.machine_memory=16657641472, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}
       8158       276ms URGENT   shard-started StartedShardEntry{shardId [[winlogbeat-6.4.0-2019.02.20][2]], allocationId [0S80dS6rQmWA9sSNNh6nWA], message [after peer recovery]}
         88       31.4m HIGH     shard-failed
         92       31.4m HIGH     shard-failed
       8154        1.3s URGENT   shard-started StartedShardEntry{shardId [[filebeat-6.4.0-2019.03.16][0]], allocationId [QQmHATusTuqZn9O5zIbcOg], message [master {es1}{O-HeVTTtR8a4tRkZIrOx7A}{BVcWUHXdSeqLL4i9FlOvPg}{10.67.36.53}{10.67.36.53:9300}{ml.machine_memory=16657641472, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}
       8152        1.4s URGENT   shard-started StartedShardEntry{shardId [[winlogbeat-6.4.0-2018.11.25][2]], allocationId [l8p94m4EQ-iufCcUXUZckw], message [after peer recovery]}
       1516         28m HIGH     shard-failed
       2278       26.3m HIGH     shard-failed
         89       31.4m HIGH     shard-failed
         91       31.4m HIGH     shard-failed
        885       29.3m HIGH     shard-failed
       1880       27.2m HIGH     shard-failed
       1551         28m HIGH     shard-failed
        884       29.3m HIGH     shard-failed
       8155       944ms URGENT   shard-started StartedShardEntry{shardId [[winlogbeat-6.4.0-2019.01.28][1]], allocationId [iS_BqV6tR466KTPeBM3Qxg], message [after peer recovery]}
       8153        1.3s URGENT   shard-started StartedShardEntry{shardId [[rke-2019.03.16][3]], allocationId [XEYmklxiSiakIk74GdYZrA], message [master {es1}{O-HeVTTtR8a4tRkZIrOx7A}{BVcWUHXdSeqLL4i9FlOvPg}{10.67.36.53}{10.67.36.53:9300}{ml.machine_memory=16657641472, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}
       1588       27.9m HIGH     shard-failed
       4999       17.3m HIGH     shard-failed
       6606        8.6m HIGH     shard-failed
       3548       23.1m HIGH     shard-failed
         93       31.4m HIGH     shard-failed
         90       31.4m HIGH     shard-failed
        480       30.1m HIGH     shard-failed
       1624       27.8m HIGH     shard-failed
       1838       27.3m HIGH     shard-failed
       8079       24.9s HIGH     put-mapping
       5054       16.9m HIGH     shard-failed
       4763       19.7m HIGH     cluster_reroute(async_shard_fetch)
       8096       19.9s HIGH     put-mapping
       6989        6.6m HIGH     shard-failed
       2279       26.3m HIGH     shard-failed
       8141        4.8s HIGH     put-mapping
       8124        9.8s HIGH     put-mapping
       8123        9.9s HIGH     put-mapping
       8156       455ms URGENT   shard-started StartedShardEntry{shardId [[winlogbeat-6.4.0-2019.01.28][1]], allocationId [iS_BqV6tR466KTPeBM3Qxg], message [master {es1}{O-HeVTTtR8a4tRkZIrOx7A}{BVcWUHXdSeqLL4i9FlOvPg}{10.67.36.53}{10.67.36.53:9300}{ml.machine_memory=16657641472, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}
       8157       353ms URGENT   shard-started StartedShardEntry{shardId [[winlogbeat-6.4.0-2019.01.29][2]], allocationId [18HsezJMQ6uECPxpZxXrug], message [after peer recovery]}
       8066       29.9s HIGH     put-mapping

[root@es1 elasticsearch]# curl -sX GET 'http://10.67.36.53:9200/_cat/recovery/k8s-*?v'
index          shard time  type           stage source_host source_node target_host target_node repository snapshot files files_recovered files_percent files_total bytes   bytes_recovered bytes_percent bytes_total translog_ops translog_ops_recovered translog_ops_percent
k8s-2019.03.11 0     384ms existing_store done  n/a         n/a         10.67.36.52 es2         n/a        n/a      0     0               100.0%        15          0       0               100.0%        9848947     0            0                      100.0%
k8s-2019.03.11 0     406ms peer           done  10.67.36.52 es2         10.67.36.51 es3         n/a        n/a      0     0               0.0%          0           0       0               0.0%          0           0            0                      100.0%
k8s-2019.03.11 1     1.8s  peer           done  10.67.36.53 es1         10.67.36.52 es2         n/a        n/a      0     0               0.0%          0           0       0               0.0%          0           0            0                      100.0%
k8s-2019.03.11 1     405ms existing_store done  n/a         n/a         10.67.36.53 es1         n/a        n/a      0     0               100.0%        33          0       0               100.0%        9918250     0            0                      100.0%
k8s-2019.03.11 2     443ms peer           done  10.67.36.51 es3         10.67.36.53 es1         n/a        n/a      0     0               0.0%          0           0       0               0.0%          0           0            0                      100.0%
k8s-2019.03.11 2     229ms existing_store done  n/a         n/a         10.67.36.51 es3         n/a        n/a      0     0               100.0%        15          0       0               100.0%        9892739     0            0                      100.0%
k8s-2019.03.11 3     510ms existing_store done  n/a         n/a         10.67.36.52 es2         n/a        n/a      0     0               100.0%        39          0       0               100.0%        10109809    0            0                      100.0%
k8s-2019.03.11 3     275ms peer           done  10.67.36.52 es2         10.67.36.51 es3         n/a        n/a      0     0               0.0%          0           0       0               0.0%          0           0            0                      100.0%
k8s-2019.03.11 4     475ms existing_store done  n/a         n/a         10.67.36.52 es2         n/a        n/a      0     0               100.0%        30          0       0               100.0%        9913191     0            0                      100.0%
k8s-2019.03.11 4     245ms peer           done  10.67.36.52 es2         10.67.36.53 es1         n/a        n/a      0     0               0.0%          0           0       0               0.0%          0           0            0                      100.0%
k8s-2019.03.12 0     608ms existing_store done  n/a         n/a         10.67.36.53 es1         n/a        n/a      0     0               100.0%        30          0       0               100.0%        9790596     0            0                      100.0%
k8s-2019.03.12 0     790ms peer           done  10.67.36.53 es1         10.67.36.51 es3         n/a        n/a      0     0               0.0%          0           0       0               0.0%          0           0            0                      100.0%

	   
[root@es1 elasticsearch]# curl -sX GET 'http://10.67.36.53:9200/_cat/shards/k8s-*?v'
index          shard prirep state           docs   store ip          node
k8s-2019.01.31 4     p      STARTED         3811   2.3mb 10.67.36.52 es2
k8s-2019.01.31 4     r      STARTED         3811   2.4mb 10.67.36.51 es3
k8s-2019.01.31 3     p      STARTED         3708   2.3mb 10.67.36.53 es1
k8s-2019.01.31 3     r      UNASSIGNED
k8s-2019.01.31 2     p      STARTED         3682   2.2mb 10.67.36.51 es3
k8s-2019.01.31 2     r      UNASSIGNED
k8s-2019.01.31 1     p      STARTED         3869   2.3mb 10.67.36.53 es1
k8s-2019.01.31 1     r      UNASSIGNED
k8s-2019.01.31 0     p      STARTED         3882   2.3mb 10.67.36.53 es1
k8s-2019.01.31 0     r      UNASSIGNED
k8s-2019.03.14 3     r      STARTED        97600  33.8mb 10.67.36.52 es2
k8s-2019.03.14 3     p      STARTED        97600  33.8mb 10.67.36.53 es1
k8s-2019.03.14 2     p      STARTED        97103  33.6mb 10.67.36.52 es2
k8s-2019.03.14 2     r      STARTED        97103  33.6mb 10.67.36.51 es3
k8s-2019.03.14 4     r      STARTED        97244  33.5mb 10.67.36.53 es1
k8s-2019.03.14 4     p      STARTED        97244  33.6mb 10.67.36.51 es3

[root@es1 elasticsearch]# curl -sX GET 'http://10.67.36.53:9200/_cat/segments?v' 
index                             shard prirep ip          segment generation docs.count docs.deleted     size size.memory committed searchable version compound
k8s-2019.04.03                    0     p      10.67.36.52 _9fq         12230      19427            0    8.5mb       27993 true      true       7.4.0   false
k8s-2019.04.03                    0     p      10.67.36.52 _9fr         12231          1            0   19.4kb       10953 true      true       7.4.0   true
k8s-2019.04.03                    0     p      10.67.36.52 _9fs         12232          2            0   20.9kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    0     p      10.67.36.52 _9ft         12233          2            0   20.3kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    0     p      10.67.36.52 _9fu         12234          1            0   19.9kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    0     p      10.67.36.52 _9fv         12235          2            0     21kb       11981 true      true       7.4.0   true
k8s-2019.04.03                    0     p      10.67.36.52 _9fw         12236          3            0   21.1kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    0     p      10.67.36.52 _9fx         12237          2            0   20.9kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    0     r      10.67.36.51 _9n8         12500      19440            0    8.5mb       28345 true      true       7.4.0   false
k8s-2019.04.03                    1     p      10.67.36.53 _9di         12150      19387            0    8.4mb       29719 true      true       7.4.0   false
k8s-2019.04.03                    1     p      10.67.36.53 _9dj         12151          2            0   20.4kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    1     p      10.67.36.53 _9dk         12152          1            0   19.9kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    1     p      10.67.36.53 _9dl         12153          1            0   20.5kb       11981 true      true       7.4.0   true
k8s-2019.04.03                    1     p      10.67.36.53 _9dm         12154          1            0   19.3kb       10953 true      true       7.4.0   true
k8s-2019.04.03                    1     p      10.67.36.53 _9dn         12155          1            0   19.9kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    1     r      10.67.36.51 _92y         11770      19386            0    8.4mb       29575 true      true       7.4.0   false
k8s-2019.04.03                    1     r      10.67.36.51 _92z         11771          1            0   19.9kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    1     r      10.67.36.51 _930         11772          2            0   20.4kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    1     r      10.67.36.51 _931         11773          2            0   24.7kb       13009 true      true       7.4.0   true
k8s-2019.04.03                    1     r      10.67.36.51 _932         11774          1            0   19.3kb       10953 true      true       7.4.0   true
k8s-2019.04.03                    1     r      10.67.36.51 _933         11775          1            0   19.9kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    2     r      10.67.36.52 _9jc         12360      19291            0    8.4mb       29355 true      true       7.4.0   false
k8s-2019.04.03                    2     r      10.67.36.52 _9jd         12361          2            0   20.2kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    2     r      10.67.36.52 _9je         12362          3            0   21.1kb       11507 true      true       7.4.0   true
k8s-2019.04.03                    2     r      10.67.36.52 _9jf         12363          2            0   20.8kb       11467 true      true       7.4.0   true
k8s-2019.04.03                    2     r      10.67.36.52 _9jg         12364          1            0   19.4kb       10953 true      true       7.4.0   true
k8s-2019.04.03                    2     r      10.67.36.52 _9jh         12365          3            0   21.2kb       11467 true      true       7.4.0   true


############################## 磁盘满导致 index read-only ########################
Hi I am getting below error in Logstash again and again.
[2018-03-29T09:56:20,280][INFO ][logstash.outputs.elasticsearch] retrying failed action with response code: 403 
({"type"=>"cluster_block_exception", "reason"=>"blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];"})
解决
curl -XPUT -H "Content-Type: application/json" http://10.67.51.150:9200/_cluster/settings -d '{ "transient": { "cluster.routing.allocation.disk.threshold_enabled": false } }'
[root@stjes1 ~]# curl -XPUT -H "Content-Type: application/json" http://10.67.51.150:9200/_all/_settings -d '{"index.blocks.read_only_allow_delete": false}'
{"acknowledged":true}
期间发现kafka的consumer group没有member了，reset topic的offset，重启logstash, 于是有member了，但是没有作用，因为403的原因，无法写入elasticsearch
根本原因还是index read only了
#kafka reset offset命令
reset offset命令只能在没有member或者consumer group inactive的情况下执行
[root@kafka1 ~]# kafka-consumer-groups --bootstrap-server 10.67.51.144:9092 --describe --group logstash --members
Consumer group 'logstash' has no active members.

[root@kafka1 ~]# kafka-consumer-groups --bootstrap-server 10.67.51.144:9092 --group logstash --reset-offsets --topic filebeat --to-earliest --execute                                                    
TOPIC                          PARTITION  NEW-OFFSET
filebeat                       9          1406145104
filebeat                       8          1302473835
filebeat                       7          1343041792
filebeat                       11         1275441988
filebeat                       0          1349368314
filebeat                       3          1310703548
filebeat                       2          1311228515
filebeat                       6          1309955338
filebeat                       4          1306143927
filebeat                       10         1422168310
filebeat                       5          1320760060
filebeat                       1          1266680804

[root@kafka1 ~]# kafka-consumer-groups --bootstrap-server 10.67.51.144:9092 --describe --group logstash --members
Warning: Consumer group 'logstash' is rebalancing.

CONSUMER-ID                                     HOST            CLIENT-ID       #PARTITIONS
logstash-0-5e5216e4-87d5-4a57-b07f-63aaba364986 /10.67.49.246   logstash-0      0
logstash-0-ee6524b4-c187-4034-9fc9-0b0467ce3825 /10.67.49.247   logstash-0      0
logstash-0-2918e495-173b-4395-94b3-a8f0307ca086 /10.67.51.2     logstash-0      0
logstash-0-728a2664-4c0d-45d0-8ff8-543031e16a80 /10.67.49.244   logstash-0      0
logstash-0-c354eb6c-d846-481f-a323-e97408f75897 /10.67.48.194   logstash-0      0
logstash-0-62019356-2018-4d9e-a1fa-2a3a326b59de /10.67.49.245   logstash-0      0
logstash-0-f60dd446-b04d-4580-afa6-2c70e500e153 /10.67.50.200   logstash-0      0
