###########  zabbix 5.4 + timescaledb-2 安装和配置

安装timescaledb-2插件
# yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-$(rpm -E %{rhel})-x86_64/pgdg-redhat-repo-latest.noarch.rpm
# yum install timescaledb-2-postgresql-13
$ timescaledb-tune --pg-config=/usr/pgsql-13/bin/pg_config
# systemctl restart postgresql-13
$ createuser --pwprompt zabbix
$ createdb -O zabbix -E Unicode -T template0 zabbix
$ pg_restore -d zabbix /tmp/zabbix.dump


#使用timescaledb脚本，创建hyphertable
-bash-4.2$ zcat /tmp/timescaledb.sql.gz | psql zabbix
NOTICE:  PostgreSQL version 13.4 is valid
NOTICE:  TimescaleDB extension is detected
NOTICE:  TimescaleDB version 2.5.0 is valid
NOTICE:  migrating data to chunks
DETAIL:  Migration might take a while depending on the amount of data.
NOTICE:  migrating data to chunks
DETAIL:  Migration might take a while depending on the amount of data.
NOTICE:  migrating data to chunks
DETAIL:  Migration might take a while depending on the amount of data.
NOTICE:  migrating data to chunks
DETAIL:  Migration might take a while depending on the amount of data.
NOTICE:  TimescaleDB is configured successfully
********************SQL中创建hypertable的语句
        PERFORM create_hypertable('history', 'clock', chunk_time_interval => 86400, migrate_data => true);       # 一个chunk为一天数据
        PERFORM create_hypertable('history_uint', 'clock', chunk_time_interval => 86400, migrate_data => true);
        PERFORM create_hypertable('history_log', 'clock', chunk_time_interval => 86400, migrate_data => true);
        PERFORM create_hypertable('history_text', 'clock', chunk_time_interval => 86400, migrate_data => true);
        PERFORM create_hypertable('history_str', 'clock', chunk_time_interval => 86400, migrate_data => true);
        PERFORM create_hypertable('trends', 'clock', chunk_time_interval => 2592000, migrate_data => true);        # 一个chunk为一月数据
        PERFORM create_hypertable('trends_uint', 'clock', chunk_time_interval => 2592000, migrate_data => true);
        UPDATE config SET db_extension='timescaledb',hk_history_global=1,hk_trends_global=1;
        UPDATE config SET compression_status=1,compress_older='7d';
        RAISE NOTICE 'TimescaleDB is configured successfully';

********************
# 已经看到很多chunk了
-bash-4.2$ oid2name -d zabbix -i |grep chunk|grep hyper
     33035                                                 _hyper_1_1_chunk
     33036                                       _hyper_1_1_chunk_history_1
     31716                                                 _hyper_1_2_chunk
     31723                                       _hyper_1_2_chunk_history_1
     31724                                                 _hyper_1_3_chunk
     31731                                       _hyper_1_3_chunk_history_1
     31732                                                 _hyper_1_4_chunk
....

# 查看有哪些hypertable
zabbix=# SELECT * FROM timescaledb_information.hypertables;
 hypertable_schema | hypertable_name | owner  | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces
-------------------+-----------------+--------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
 public            | history         | zabbix |              1 |          8 | t                   | f              |                    |            |
 public            | history_uint    | zabbix |              1 |          8 | t                   | f              |                    |            |
 public            | history_log     | zabbix |              1 |          0 | t                   | f              |                    |            |
 public            | history_text    | zabbix |              1 |          1 | t                   | f              |                    |            |
 public            | history_str     | zabbix |              1 |          1 | t                   | f              |                    |            |
 public            | trends          | zabbix |              1 |          5 | t                   | f              |                    |            |
 public            | trends_uint     | zabbix |              1 |          5 | t                   | f              |                    |            |
(7 rows)

# 查看有哪些chunks
zabbix=# select * from timescaledb_information.chunks ;
 hypertable_schema | hypertable_name |     chunk_schema      |    chunk_name     | primary_dimension | primary_dimension_type | range_start | range_end | range_start_integer | range_end_integer | is_compressed | chunk_tablespace | data_nodes
-------------------+-----------------+-----------------------+-------------------+-------------------+------------------------+-------------+-----------+---------------------+-------------------+---------------+------------------+------------
 public            | history         | _timescaledb_internal | _hyper_1_1_chunk  | clock             | integer                |             |           |          1635206400 |        1635292800 | t             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_2_chunk  | clock             | integer                |             |           |          1635465600 |        1635552000 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_3_chunk  | clock             | integer                |             |           |          1635379200 |        1635465600 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_4_chunk  | clock             | integer                |             |           |          1634947200 |        1635033600 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_5_chunk  | clock             | integer                |             |           |          1635033600 |        1635120000 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_6_chunk  | clock             | integer                |             |           |          1635120000 |        1635206400 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_7_chunk  | clock             | integer                |             |           |          1635292800 |        1635379200 | f             |                  |


# 查看history表的chunks
zabbix=# select public.show_chunks('history');
              show_chunks
----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
 _timescaledb_internal._hyper_1_2_chunk
 _timescaledb_internal._hyper_1_3_chunk
 _timescaledb_internal._hyper_1_4_chunk
 _timescaledb_internal._hyper_1_5_chunk
 _timescaledb_internal._hyper_1_6_chunk
 _timescaledb_internal._hyper_1_7_chunk
 _timescaledb_internal._hyper_1_8_chunk
(8 rows)

查看history表的chuck细节
zabbix=# SELECT * FROM timescaledb_information.chunks WHERE hypertable_name = 'history';
 hypertable_schema | hypertable_name |     chunk_schema      |    chunk_name    | primary_dimension | primary_dimension_type | range_start | range_end | range_start_integer | range_end_integer | is_compressed | chunk_tablespace | data_nodes
-------------------+-----------------+-----------------------+------------------+-------------------+------------------------+-------------+-----------+---------------------+-------------------+---------------+------------------+------------
 public            | history         | _timescaledb_internal | _hyper_1_1_chunk | clock             | integer                |             |           |          1635206400 |        1635292800 | t             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_2_chunk | clock             | integer                |             |           |          1635465600 |        1635552000 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_3_chunk | clock             | integer                |             |           |          1635379200 |        1635465600 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_4_chunk | clock             | integer                |             |           |          1634947200 |        1635033600 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_5_chunk | clock             | integer                |             |           |          1635033600 |        1635120000 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_6_chunk | clock             | integer                |             |           |          1635120000 |        1635206400 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_7_chunk | clock             | integer                |             |           |          1635292800 |        1635379200 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_8_chunk | clock             | integer                |             |           |          1635552000 |        1635638400 | f             |                  |
(8 rows)

zabbix db里的clock字段存储的是秒数
可以秒数转日期，也可以日期转秒数
zabbix=# select to_timestamp(1635293027);
      to_timestamp
------------------------
 2021-10-27 00:03:47+00
(1 row)

zabbix=# SELECT EXTRACT(EPOCH FROM TIMESTAMP '2021-10-31 06:00:00');
 date_part
------------
 1635660000
(1 row)

使用newer和older来查询chunks
zabbix=# select show_chunks('history',newer_than => 1635293027);
              show_chunks
----------------------------------------
 _timescaledb_internal._hyper_1_2_chunk
 _timescaledb_internal._hyper_1_3_chunk
 _timescaledb_internal._hyper_1_8_chunk
(3 rows)


zabbix=# select show_chunks('history',older_than => 1635293027);
              show_chunks
----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
 _timescaledb_internal._hyper_1_4_chunk
 _timescaledb_internal._hyper_1_5_chunk
 _timescaledb_internal._hyper_1_6_chunk
(4 rows)

查看chuck表大小
zabbix=# select * from chunks_detailed_size('history');
     chunk_schema      |    chunk_name    | table_bytes | index_bytes | toast_bytes | total_bytes | node_name
-----------------------+------------------+-------------+-------------+-------------+-------------+-----------
 _timescaledb_internal | _hyper_1_2_chunk |     3416064 |     2662400 |           0 |     6078464 |
 _timescaledb_internal | _hyper_1_3_chunk |     3416064 |     2588672 |           0 |     6004736 |
 _timescaledb_internal | _hyper_1_4_chunk |     3301376 |     2555904 |           0 |     5857280 |
 _timescaledb_internal | _hyper_1_5_chunk |     3416064 |     2531328 |           0 |     5947392 |
 _timescaledb_internal | _hyper_1_6_chunk |     3416064 |     2514944 |           0 |     5931008 |
 _timescaledb_internal | _hyper_1_7_chunk |     3416064 |     2400256 |           0 |     5816320 |
 _timescaledb_internal | _hyper_1_8_chunk |      778240 |      507904 |           0 |     1286144 |
 _timescaledb_internal | _hyper_1_1_chunk |       65536 |       24576 |      425984 |      516096 |
(8 rows)
查看history超表的大小
zabbix=# select pg_size_pretty(table_bytes) as table_size,pg_size_pretty(index_bytes) as index_size,pg_size_pretty(toast_bytes) as toast_size,pg_size_pretty(total_bytes) as total_size from hypertable_detailed_size('history');
 table_size | index_size | toast_size | total_size
------------+------------+------------+------------
 21 MB      | 16 MB      | 416 kB     | 37 MB


压缩chunk
zabbix=# select compress_chunk('_timescaledb_internal._hyper_1_1_chunk');
             compress_chunk
----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
(1 row)
.......
-[ RECORD 8 ]------------------+----------------------
chunk_schema                   | _timescaledb_internal
chunk_name                     | _hyper_1_1_chunk
compression_status             | Compressed
before_compression_table_bytes | 3416064
before_compression_index_bytes | 2777088
before_compression_toast_bytes | 0
before_compression_total_bytes | 6193152
after_compression_table_bytes  | 65536
after_compression_index_bytes  | 16384
after_compression_toast_bytes  | 425984
after_compression_total_bytes  | 507904
node_name                      |



压缩后原 _hyper_1_1_chunk 大小为0
zabbix=# select pg_relation_size('_timescaledb_internal._hyper_1_1_chunk');
 pg_relation_size
------------------
                0
(1 row)
zabbix=# set search_path to _timescaledb_internal;
SET
zabbix=# \d
                           List of relations
        Schema         |            Name             | Type  |  Owner
-----------------------+-----------------------------+-------+----------
.......
 _timescaledb_internal | _hyper_1_1_chunk            | table | zabbix
 _timescaledb_internal | _hyper_1_2_chunk            | table | zabbix
 _timescaledb_internal | _hyper_1_3_chunk            | table | zabbix
.....
 _timescaledb_internal | compress_hyper_8_27_chunk   | table | zabbix
(40 rows)

通过explain可以看到 _hyper_1_1_chunk 压缩后 数据放到了 compress_hyper_8_27_chunk
zabbix=# explain select count(*) from history;
                                                   QUERY PLAN
-----------------------------------------------------------------------------------------------------------------
 Finalize Aggregate  (cost=6959.03..6959.04 rows=1 width=8)
   ->  Gather  (cost=6958.82..6959.03 rows=2 width=8)
         Workers Planned: 2
         ->  Partial Aggregate  (cost=5958.82..5958.83 rows=1 width=8)
               ->  Parallel Append  (cost=0.00..5514.00 rows=177927 width=0)
                     ->  Custom Scan (DecompressChunk) on _hyper_1_1_chunk  (cost=0.12..5.46 rows=46000 width=0)
                           ->  Parallel Seq Scan on compress_hyper_8_27_chunk  (cost=0.00..5.46 rows=46 width=4)    # 这里
                     ->  Parallel Seq Scan on _hyper_1_2_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_3_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_5_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_6_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_7_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_4_chunk  (cost=0.00..718.11 rows=31911 width=0)
                     ->  Parallel Seq Scan on _hyper_1_8_chunk  (cost=0.00..184.03 rows=8103 width=0)
(14 rows)

压缩后大小为40kB
zabbix=# select pg_size_pretty(pg_relation_size('_timescaledb_internal.compress_hyper_8_27_chunk'));
 pg_size_pretty
----------------
 40 kB
(1 row)


查看压缩任务
zabbix=# SELECT * FROM timescaledb_information.jobs;
 job_id |     application_name      | schedule_interval | max_runtime | max_retries | retry_period |      proc_schema      |     proc_name      |  owner   | scheduled |                     config                     |          next_start           | hypertable_schema | h
ypertable_name
--------+---------------------------+-------------------+-------------+-------------+--------------+-----------------------+--------------------+----------+-----------+------------------------------------------------+-------------------------------+-------------------+--
---------------
      1 | Telemetry Reporter [1]    | 24:00:00          | 00:01:40    |          -1 | 01:00:00     | _timescaledb_internal | policy_telemetry   | postgres | t         |                                                | 2021-10-30 08:46:15.187675+00 |                   |
   1006 | Compression Policy [1006] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 7, "compress_after": 612000} | 2021-10-30 12:26:03.791419+00 | public            | t
rends_uint
   1000 | Compression Policy [1000] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 1, "compress_after": 612000} | 2021-10-30 12:26:03.807164+00 | public            | h
istory
   1001 | Compression Policy [1001] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 2, "compress_after": 612000} | 2021-10-30 12:26:03.8262+00   | public            | h
istory_uint
   1002 | Compression Policy [1002] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 5, "compress_after": 612000} | 2021-10-30 12:26:03.851741+00 | public            | h
istory_str
   1003 | Compression Policy [1003] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 4, "compress_after": 612000} | 2021-10-30 12:26:03.868407+00 | public            | h
istory_text
   1005 | Compression Policy [1005] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 6, "compress_after": 612000} | 2021-10-30 12:26:03.914944+00 | public            | t
rends
   1004 | Compression Policy [1004] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 3, "compress_after": 612000} | 2021-10-30 12:26:03.933253+00 | public            | h
istory_log

# time_bucket函数
查看itemid=10073的监控指标每5分钟的平均值
zabbix=# select time_bucket('300',clock) as five_min,avg(value) from history where itemid=10073 group by five_min order by five_min limit 10;
  five_min  |        avg
------------+--------------------
 1634949900 | 0.6498569713761893
 1634950200 | 0.6498569025011809
 1634950500 | 0.6498579033543198
 1634950800 | 0.6498616420345298
 1634951100 | 0.6498640946037003
 1634951400 |  0.649850038873476
 1634951700 | 0.6520572575130209
 1634952000 |  0.649848264986644
 1634952300 | 0.6498570006136769
 1634952600 | 0.6498655667358046

# first函数
查看每个itemid最早的一笔记录 
zabbix=# select itemid,first(value,clock) from history group by itemid;
 itemid |         first
--------+-----------------------
  10073 |    0.6498569713761893
  10074 |                     0
  10075 |                     0
  10076 |   0.08331538591753834
  10077 |                     0
  10078 |   0.13330488936035287
  23252 |                     0
  23253 |   0.16917611233293858
  23255 |                     0
  23256 |   0.06889424733034792
  23257 |   0.18606224627875506
 
# 备份   这里超表没有备份，需要单独copy备份
-bash-4.2$ ts-dump --db-URI=postgres://postgres:postgres@10.67.39.58:5432/zabbix --dump-dir=tsdump/
2021/11/01 08:45:04 Jobs:  have stopped, continuing
 pg_dump version: pg_dump (PostgreSQL) 13.4

2021/11/01 08:45:06 pg_dump: warning: there are circular foreign-key constraints on this table:
2021/11/01 08:45:06 pg_dump:   hypertable
2021/11/01 08:45:06 pg_dump: You might not be able to restore the dump without using --disable-triggers or temporarily dropping the constraints.
2021/11/01 08:45:06 pg_dump: Consider using a full dump instead of a --data-only dump to avoid this problem.
2021/11/01 08:45:06 pg_dump: warning: there are circular foreign-key constraints on this table:
2021/11/01 08:45:06 pg_dump:   chunk
2021/11/01 08:45:06 pg_dump: You might not be able to restore the dump without using --disable-triggers or temporarily dropping the constraints.
2021/11/01 08:45:06 pg_dump: Consider using a full dump instead of a --data-only dump to avoid this problem.
2021/11/01 08:45:07 pg_dump: NOTICE:  hypertable data are in the chunks, no data will be copied
2021/11/01 08:45:07 DETAIL:  Data for hypertables are stored in the chunks of a hypertable so COPY TO of a hypertable will not copy any data.
2021/11/01 08:45:07 HINT:  Use "COPY (SELECT * FROM <hypertable>) TO ..." to copy all data in hypertable, or copy each chunk individually.
2021/11/01 08:45:07 pg_dump: NOTICE:  hypertable data are in the chunks, no data will be copied
......


####################  prometheus + promscale + timescaledb2 POC
容器快速搭建环境
docker network create --driver bridge pnet

docker run --name timescaledb -e POSTGRES_PASSWORD=secret -d -p 5432:5432 --network=pnet timescaledev/promscale-extension:latest-ts2-pg13 postgres -csynchronous_commit=off

docker run --name promscale -d -p 9201:9201 --network=pnet timescale/promscale:0.6 -db-password=secret -db-port=5432 -db-name=postgres -db-host=timescaledb -db-ssl-mode=allow
此步完成后，会自动在PG里建promscale extension

编辑prometheus.yml，加入
remote_write:
        - url: "http://10.67.36.58:9201/write"
remote_read:
    - url: "http://10.67.36.58:9201/read"

重启prometheus
root@u1804:/var/snap/prometheus/current# systemctl restart snap.prometheus.prometheus.service

postgres=# SELECT count(*) FROM timescaledb_information.hypertables;
 count
-------
   767
(1 row) 

####################### prometheus + promscale + 多节点timescaledb2   ####################
容器快速搭建环境
1 分别运行3个timescaledb容器
docker run --name timescaledb -e POSTGRES_PASSWORD=secret -d -p 5432:5432 timescaledev/promscale-extension:latest-ts2-pg13 postgres -csynchronous_commit=off

2 每个节点做根据下面要求修改postgresql.conf，并stop/start container
max_prepared_transactions must be set to a non-zero value on all data nodes (if not already set, 150 is recommended).
enable_partitionwise_aggregate should be set to on on the access node for good query performance. Otherwise, queries will not be pushed down to the data nodes.
jit should be set to off on the access node as JIT currently doesn't work well with distributed queries. JIT can still be enabled on the data nodes

3 依次将dns,dns2节点加入为data_node，这里dns是写错的节点名称 -_-!!

postgres=# select add_data_node('dns',host=>'10.67.36.59');
NOTICE:  database "postgres" already exists on data node, skipping
NOTICE:  extension "timescaledb" already exists on data node, skipping
DETAIL:  TimescaleDB extension version on 10.67.36.59:5432 was 2.3.1.
             add_data_node
---------------------------------------
 (dns,10.67.36.59,5432,postgres,t,f,f)
(1 row)

postgres=# select add_data_node('dns2',host=>'10.67.36.57');
NOTICE:  database "postgres" already exists on data node, skipping
NOTICE:  extension "timescaledb" already exists on data node, skipping
DETAIL:  TimescaleDB extension version on 10.67.36.57:5432 was 2.3.1.
             add_data_node
----------------------------------------
 (dns2,10.67.36.57,5432,postgres,t,f,f)
(1 row)

postgres=# select * from timescaledb_information.data_nodes;
 node_name |  owner   |                   options
-----------+----------+----------------------------------------------
 dns       | postgres | {host=10.67.36.59,port=5432,dbname=postgres}
 dns2      | postgres | {host=10.67.36.57,port=5432,dbname=postgres}
(2 rows)

底层在查询是应该是借助了FDW
postgres=# \des+
                                                             List of foreign servers
 Name |  Owner   | Foreign-data wrapper | Access privileges | Type | Version |                     FDW options                      | Description
------+----------+----------------------+-------------------+------+---------+------------------------------------------------------+-------------
 dns  | postgres | timescaledb_fdw      |                   |      |         | (host '10.67.36.59', port '5432', dbname 'postgres') |
 dns2 | postgres | timescaledb_fdw      |                   |      |         | (host '10.67.36.57', port '5432', dbname 'postgres') |
(2 rows)

4 运行promscale容器
docker run --name promscale_3ts -d -p 9201:9201 timescale/promscale:0.6 -db-password=secret -db-port=5432 -db-name=postgres -db-host=10.67.36.60 -db-ssl-mode=allow

5 在access node上测试分布式hypertable
postgres=# CREATE TABLE conditions (
postgres(#   time        TIMESTAMPTZ       NOT NULL,
postgres(#   location    TEXT              NOT NULL,
postgres(#   temperature DOUBLE PRECISION  NULL,
postgres(#   humidity    DOUBLE PRECISION  NULL
postgres(# );
CREATE TABLE

postgres=# SELECT create_distributed_hypertable('conditions', 'time', 'location');  #单副本
 create_distributed_hypertable
-------------------------------
 (1,public,conditions,t)
(1 row)

postgres=# SELECT * FROM timescaledb_information.hypertables;
 hypertable_schema | hypertable_name |  owner   | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces
-------------------+-----------------+----------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
 public            | conditions      | postgres |              2 |          2 | f                   | t              |                  1 | {dns,dns2} |
(1 row)

插入数据后查询
postgres=# SELECT chunk_name, data_nodes
postgres-# FROM timescaledb_information.chunks
postgres-# WHERE hypertable_name = 'conditions';
      chunk_name       | data_nodes
-----------------------+------------
 _dist_hyper_1_1_chunk | {dns}
 _dist_hyper_1_2_chunk | {dns2}
(2 rows)

通过执行计划看到查询下推到两个data node
postgres=# explain (verbose) select * from conditions;
                                                                              QUERY PLAN
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)  (cost=100.00..1873158.09 rows=41839260 width=56)
   Output: conditions."time", conditions.location, conditions.temperature, conditions.humidity
   ->  Append  (cost=100.00..1873158.09 rows=41839260 width=56)
         ->  Custom Scan (DataNodeScan) on public.conditions conditions_1  (cost=100.00..831980.90 rows=20919630 width=56)
               Output: conditions_1."time", conditions_1.location, conditions_1.temperature, conditions_1.humidity
               Data node: dns
               Chunks: _dist_hyper_1_1_chunk
               Remote SQL: SELECT "time", location, temperature, humidity FROM public.conditions WHERE _timescaledb_internal.chunks_in(public.conditions.*, ARRAY[1])
         ->  Custom Scan (DataNodeScan) on public.conditions conditions_2  (cost=100.00..831980.90 rows=20919630 width=56)
               Output: conditions_2."time", conditions_2.location, conditions_2.temperature, conditions_2.humidity
               Data node: dns2
               Chunks: _dist_hyper_1_2_chunk
               Remote SQL: SELECT "time", location, temperature, humidity FROM public.conditions WHERE _timescaledb_internal.chunks_in(public.conditions.*, ARRAY[1])

分别查看两个chunk
postgres=# \d+ _timescaledb_internal._dist_hyper_1_1_chunk
                                  Foreign table "_timescaledb_internal._dist_hyper_1_1_chunk"
   Column    |           Type           | Collation | Nullable | Default | FDW options | Storage  | Stats target | Description
-------------+--------------------------+-----------+----------+---------+-------------+----------+--------------+-------------
 time        | timestamp with time zone |           | not null |         |             | plain    |              |
 location    | text                     |           | not null |         |             | extended |              |
 temperature | double precision         |           |          |         |             | plain    |              |
 humidity    | double precision         |           |          |         |             | plain    |              |
Check constraints:
    "constraint_1" CHECK ("time" >= '2021-10-28 00:00:00+00'::timestamp with time zone AND "time" < '2021-11-04 00:00:00+00'::timestamp with time zone)
    "constraint_2" CHECK (_timescaledb_internal.get_partition_hash(location) < 1073741823)
Server: dns
Inherits: conditions

postgres=# \d+ _timescaledb_internal._dist_hyper_1_2_chunk
                                  Foreign table "_timescaledb_internal._dist_hyper_1_2_chunk"
   Column    |           Type           | Collation | Nullable | Default | FDW options | Storage  | Stats target | Description
-------------+--------------------------+-----------+----------+---------+-------------+----------+--------------+-------------
 time        | timestamp with time zone |           | not null |         |             | plain    |              |
 location    | text                     |           | not null |         |             | extended |              |
 temperature | double precision         |           |          |         |             | plain    |              |
 humidity    | double precision         |           |          |         |             | plain    |              |
Check constraints:
    "constraint_1" CHECK ("time" >= '2021-10-28 00:00:00+00'::timestamp with time zone AND "time" < '2021-11-04 00:00:00+00'::timestamp with time zone)
    "constraint_3" CHECK (_timescaledb_internal.get_partition_hash(location) >= 1073741823)
Server: dns2
Inherits: conditions

创建两副本的分布式超表
CREATE TABLE conditions_rep2 (
  time        TIMESTAMPTZ       NOT NULL,
  location    TEXT              NOT NULL,
  temperature DOUBLE PRECISION  NULL,
  humidity    DOUBLE PRECISION  NULL
);
postgres=# SELECT create_distributed_hypertable('conditions_rep2', 'time', 'location',
postgres(#     replication_factor => 2);
 create_distributed_hypertable
-------------------------------
 (2,public,conditions_rep2,t)
(1 row)

INSERT INTO conditions_rep2
  VALUES
    (NOW(), 'office', 70.0, 50.3),
    (NOW(), 'basement', 66.5, 61.0),
	(NOW(), 'office', 71.3, 50.0),
    (NOW(), 'garage', 66.5, 60.0),
	(NOW(), 'office', 70.0, 50.0),
    (NOW(), 'basement', 66.5, 60.0),
	(NOW(), 'office', 70.2, 50.0),
    (NOW(), 'basement', 66.5, 60.0),
	(NOW(), 'garage', 70.9, 50.0),
    (NOW(), 'basement', 66.5, 60.5),
	(NOW(), 'office', 70.0, 51.0),
    (NOW(), 'basement', 66.5, 60.0),
    (NOW(), 'garage', 77.6, 65.2);

#查看chunk分布	 单副本和两副本都能看到
postgres=# select hypertable_schema,hypertable_name,chunk_schema,chunk_name,is_compressed,data_nodes from timescaledb_information.chunks ;
 hypertable_schema | hypertable_name |     chunk_schema      |      chunk_name       | is_compressed | data_nodes
-------------------+-----------------+-----------------------+-----------------------+---------------+------------
 public            | conditions      | _timescaledb_internal | _dist_hyper_1_1_chunk | f             | {dns}
 public            | conditions      | _timescaledb_internal | _dist_hyper_1_2_chunk | f             | {dns2}
 public            | conditions_rep2 | _timescaledb_internal | _dist_hyper_2_3_chunk | f             | {dns,dns2}
 public            | conditions_rep2 | _timescaledb_internal | _dist_hyper_2_4_chunk | f             | {dns,dns2}
	
6 prometheus配置prom_scale
编辑prometheus.yml，加入
remote_write:
        - url: "http://10.67.36.62:9201/write"
remote_read:
    - url: "http://10.67.36.62:9201/read"

重启prometheus
root@u1804:/var/snap/prometheus/current# systemctl restart snap.prometheus.prometheus.service

查看所有的hypertable
select hypertable_schema,hypertable_name,chunk_schema,chunk_name,is_compressed,data_nodes from timescaledb_information.chunks ;
hypertable_schema |                        hypertable_name                         |  owner   | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces
-------------------+----------------------------------------------------------------+----------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
 public            | conditions                                                     | postgres |              2 |          2 | f                   | t              |                  1 | {dns,dns2} |
 public            | conditions_rep2                                                | postgres |              2 |          2 | f                   | t              |                  2 | {dns,dns2} |
 prom_data         | node_cpu_seconds_total                                         | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_next_gc_bytes                                      | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_heap_objects                                       | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_mspan_inuse_bytes                                  | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_lookups_total                                      | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_stack_sys_bytes                                    | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | node_boot_time_seconds                                         | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | node_context_switches_total                                    | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | node_cpu_guest_seconds_total                                   | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_sys_bytes                                          | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_mspan_sys_bytes                                    | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_alloc_bytes                                        | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_mcache_inuse_bytes                                 | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_mcache_sys_bytes                                   | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_buck_hash_sys_bytes                                | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | node_arp_entries                                               | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_frees_total                                        | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_threads                                                     | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |

# 在原有集群的基础上新增节点
Expanding the cluster
When adding nodes to a TimescaleDB cluster that is already being written to by Promscale, you should run the add_prom_node(node_name) function after running the standard add_data_node() function. For example:

SELECT add_data_node('example_node_name', host => 'example_host_address')
SELECT add_prom_node('example_node_name');
Note: add_prom_node should be run by the same database user, as the one writing data from Promscale. 