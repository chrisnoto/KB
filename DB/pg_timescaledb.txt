###########  zabbix 5.4 + timescaledb-2 安装和配置

https://packagecloud.io/timescale/timescaledb/el/7/x86_64

timescaledb插件升级，  这里是2.6.0的apache license版本升级到2.6.1的community版本
1 安装新的包  timescaledb-2-postgresql-14-2.6.1-0.el7.x86_64.rpm
[root@mydb03 ~]# cd tsdb
[root@mydb03 tsdb]# ls
make-3.82-24.el7.x86_64.rpm         openssl-libs-1.0.2k-25.el7_9.x86_64.rpm                    timescaledb-2-postgresql-14-2.6.1-0.el7.x86_64.rpm
openssl-1.0.2k-25.el7_9.x86_64.rpm  timescaledb-2-loader-postgresql-14-2.6.1-0.el7.x86_64.rpm  timescaledb-tools-0.12.0-0.el7.x86_64.rpm
[root@mydb03 tsdb]# rpm -Uvh *.rpm
Preparing...                          ################################# [100%]
Updating / installing...
   1:openssl-libs-1:1.0.2k-25.el7_9   ################################# [ 14%]
   2:timescaledb-tools-0.12.0-0.el7   ################################# [ 29%]
   3:make-1:3.82-24.el7               ################################# [ 43%]
   4:openssl-1:1.0.2k-25.el7_9        ################################# [ 57%]
   5:timescaledb-2-loader-postgresql-1################################# [ 71%]
Using pg_config located at /usr/pgsql-14/bin/pg_config to finish installation...
   6:timescaledb-2-postgresql-14-2.6.1################################# [ 86%]
Using pg_config located at /usr/pgsql-14/bin/pg_config to finish installation...

TimescaleDB has been installed. You need to update your postgresql.conf file
to load the library by adding 'timescaledb' to your shared_preload_libraries.
The easiest way to do this (and more configuration) is to use timescaledb-tune:

timescaledb-tune --pg-config=/usr/pgsql-14/bin/pg_config

Cleaning up / removing...
   7:openssl-libs-1:1.0.2k-16.el7_6.1 ################################# [100%]
2 升级extension
-bash-4.2$ psql -X -d zabbix
psql (14.2)
Type "help" for help.

zabbix=# \c
You are now connected to database "zabbix" as user "postgres".
zabbix=# alter extension timescaledb update;
ALTER EXTENSION
zabbix=# \dx
                                      List of installed extensions
    Name     | Version |   Schema   |                            Description
-------------+---------+------------+-------------------------------------------------------------------
 plpgsql     | 1.0     | pg_catalog | PL/pgSQL procedural language
 timescaledb | 2.6.1   | public     | Enables scalable inserts and complex queries for time-series data
(2 rows)


安装timescaledb-2插件
# yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-$(rpm -E %{rhel})-x86_64/pgdg-redhat-repo-latest.noarch.rpm
# yum install timescaledb-2-postgresql-13
$ timescaledb-tune --pg-config=/usr/pgsql-13/bin/pg_config
# systemctl restart postgresql-13
$ createuser --pwprompt zabbix
$ createdb -O zabbix -E Unicode -T template0 zabbix
$ pg_restore -d zabbix /tmp/zabbix.dump

RUN service postgresql start && \
   psql -U postgres -d postgres -c "alter user postgres with password '${POSTGRES_PASSWORD}';" && \
   psql -U postgres -d postgres -c "alter system set listen_addresses to '*';" && \
   psql -U postgres -d postgres -c "alter system set shared_preload_libraries to 'timescaledb';"   
RUN sed -i "s|# host    .*|host all all all scram-sha-256|g" /etc/postgresql/14/main/pg_hba.conf
RUN service postgresql restart &&\
   psql -X -c "create extension timescaledb;"

#使用timescaledb脚本，创建hyphertable
-bash-4.2$ zcat /tmp/timescaledb.sql.gz | psql zabbix
NOTICE:  PostgreSQL version 13.4 is valid
NOTICE:  TimescaleDB extension is detected
NOTICE:  TimescaleDB version 2.5.0 is valid
NOTICE:  migrating data to chunks
DETAIL:  Migration might take a while depending on the amount of data.
NOTICE:  migrating data to chunks
DETAIL:  Migration might take a while depending on the amount of data.
NOTICE:  migrating data to chunks
DETAIL:  Migration might take a while depending on the amount of data.
NOTICE:  migrating data to chunks
DETAIL:  Migration might take a while depending on the amount of data.
NOTICE:  TimescaleDB is configured successfully
********************SQL中创建hypertable的语句
        PERFORM create_hypertable('history', 'clock', chunk_time_interval => 86400, migrate_data => true);       # 一个chunk为一天数据
        PERFORM create_hypertable('history_uint', 'clock', chunk_time_interval => 86400, migrate_data => true);
        PERFORM create_hypertable('history_log', 'clock', chunk_time_interval => 86400, migrate_data => true);
        PERFORM create_hypertable('history_text', 'clock', chunk_time_interval => 86400, migrate_data => true);
        PERFORM create_hypertable('history_str', 'clock', chunk_time_interval => 86400, migrate_data => true);
        PERFORM create_hypertable('trends', 'clock', chunk_time_interval => 2592000, migrate_data => true);        # 一个chunk为一月数据
        PERFORM create_hypertable('trends_uint', 'clock', chunk_time_interval => 2592000, migrate_data => true);
        UPDATE config SET db_extension='timescaledb',hk_history_global=1,hk_trends_global=1;
        UPDATE config SET compression_status=1,compress_older='7d';
        RAISE NOTICE 'TimescaleDB is configured successfully';

********************
# 已经看到很多chunk了
-bash-4.2$ oid2name -d zabbix -i |grep chunk|grep hyper
     33035                                                 _hyper_1_1_chunk
     33036                                       _hyper_1_1_chunk_history_1
     31716                                                 _hyper_1_2_chunk
     31723                                       _hyper_1_2_chunk_history_1
     31724                                                 _hyper_1_3_chunk
     31731                                       _hyper_1_3_chunk_history_1
     31732                                                 _hyper_1_4_chunk
....

# 查看有哪些hypertable
zabbix=# SELECT * FROM timescaledb_information.hypertables;
 hypertable_schema | hypertable_name | owner  | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces
-------------------+-----------------+--------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
 public            | history         | zabbix |              1 |          8 | t                   | f              |                    |            |
 public            | history_uint    | zabbix |              1 |          8 | t                   | f              |                    |            |
 public            | history_log     | zabbix |              1 |          0 | t                   | f              |                    |            |
 public            | history_text    | zabbix |              1 |          1 | t                   | f              |                    |            |
 public            | history_str     | zabbix |              1 |          1 | t                   | f              |                    |            |
 public            | trends          | zabbix |              1 |          5 | t                   | f              |                    |            |
 public            | trends_uint     | zabbix |              1 |          5 | t                   | f              |                    |            |
(7 rows)

# 查看有哪些chunks
zabbix=# select * from timescaledb_information.chunks ;
 hypertable_schema | hypertable_name |     chunk_schema      |    chunk_name     | primary_dimension | primary_dimension_type | range_start | range_end | range_start_integer | range_end_integer | is_compressed | chunk_tablespace | data_nodes
-------------------+-----------------+-----------------------+-------------------+-------------------+------------------------+-------------+-----------+---------------------+-------------------+---------------+------------------+------------
 public            | history         | _timescaledb_internal | _hyper_1_1_chunk  | clock             | integer                |             |           |          1635206400 |        1635292800 | t             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_2_chunk  | clock             | integer                |             |           |          1635465600 |        1635552000 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_3_chunk  | clock             | integer                |             |           |          1635379200 |        1635465600 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_4_chunk  | clock             | integer                |             |           |          1634947200 |        1635033600 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_5_chunk  | clock             | integer                |             |           |          1635033600 |        1635120000 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_6_chunk  | clock             | integer                |             |           |          1635120000 |        1635206400 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_7_chunk  | clock             | integer                |             |           |          1635292800 |        1635379200 | f             |                  |


# 查看history表的chunks
zabbix=# select public.show_chunks('history');
              show_chunks
----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
 _timescaledb_internal._hyper_1_2_chunk
 _timescaledb_internal._hyper_1_3_chunk
 _timescaledb_internal._hyper_1_4_chunk
 _timescaledb_internal._hyper_1_5_chunk
 _timescaledb_internal._hyper_1_6_chunk
 _timescaledb_internal._hyper_1_7_chunk
 _timescaledb_internal._hyper_1_8_chunk
(8 rows)

查看history表的chuck细节
zabbix=# SELECT * FROM timescaledb_information.chunks WHERE hypertable_name = 'history';
 hypertable_schema | hypertable_name |     chunk_schema      |    chunk_name    | primary_dimension | primary_dimension_type | range_start | range_end | range_start_integer | range_end_integer | is_compressed | chunk_tablespace | data_nodes
-------------------+-----------------+-----------------------+------------------+-------------------+------------------------+-------------+-----------+---------------------+-------------------+---------------+------------------+------------
 public            | history         | _timescaledb_internal | _hyper_1_1_chunk | clock             | integer                |             |           |          1635206400 |        1635292800 | t             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_2_chunk | clock             | integer                |             |           |          1635465600 |        1635552000 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_3_chunk | clock             | integer                |             |           |          1635379200 |        1635465600 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_4_chunk | clock             | integer                |             |           |          1634947200 |        1635033600 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_5_chunk | clock             | integer                |             |           |          1635033600 |        1635120000 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_6_chunk | clock             | integer                |             |           |          1635120000 |        1635206400 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_7_chunk | clock             | integer                |             |           |          1635292800 |        1635379200 | f             |                  |
 public            | history         | _timescaledb_internal | _hyper_1_8_chunk | clock             | integer                |             |           |          1635552000 |        1635638400 | f             |                  |
(8 rows)

zabbix db里的clock字段存储的是秒数
可以秒数转日期，也可以日期转秒数
zabbix=# select to_timestamp(1635293027);
      to_timestamp
------------------------
 2021-10-27 00:03:47+00
(1 row)

zabbix=# SELECT EXTRACT(EPOCH FROM TIMESTAMP '2021-10-31 06:00:00');
 date_part
------------
 1635660000
(1 row)

使用newer和older来查询chunks
zabbix=# select show_chunks('history',newer_than => 1635293027);
              show_chunks
----------------------------------------
 _timescaledb_internal._hyper_1_2_chunk
 _timescaledb_internal._hyper_1_3_chunk
 _timescaledb_internal._hyper_1_8_chunk
(3 rows)


zabbix=# select show_chunks('history',older_than => 1635293027);
              show_chunks
----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
 _timescaledb_internal._hyper_1_4_chunk
 _timescaledb_internal._hyper_1_5_chunk
 _timescaledb_internal._hyper_1_6_chunk
(4 rows)

查看chuck表大小
zabbix=# select * from chunks_detailed_size('history');
     chunk_schema      |    chunk_name    | table_bytes | index_bytes | toast_bytes | total_bytes | node_name
-----------------------+------------------+-------------+-------------+-------------+-------------+-----------
 _timescaledb_internal | _hyper_1_2_chunk |     3416064 |     2662400 |           0 |     6078464 |
 _timescaledb_internal | _hyper_1_3_chunk |     3416064 |     2588672 |           0 |     6004736 |
 _timescaledb_internal | _hyper_1_4_chunk |     3301376 |     2555904 |           0 |     5857280 |
 _timescaledb_internal | _hyper_1_5_chunk |     3416064 |     2531328 |           0 |     5947392 |
 _timescaledb_internal | _hyper_1_6_chunk |     3416064 |     2514944 |           0 |     5931008 |
 _timescaledb_internal | _hyper_1_7_chunk |     3416064 |     2400256 |           0 |     5816320 |
 _timescaledb_internal | _hyper_1_8_chunk |      778240 |      507904 |           0 |     1286144 |
 _timescaledb_internal | _hyper_1_1_chunk |       65536 |       24576 |      425984 |      516096 |
(8 rows)
查看history超表的大小
zabbix=# select pg_size_pretty(table_bytes) as table_size,pg_size_pretty(index_bytes) as index_size,pg_size_pretty(toast_bytes) as toast_size,pg_size_pretty(total_bytes) as total_size from hypertable_detailed_size('history');
 table_size | index_size | toast_size | total_size
------------+------------+------------+------------
 21 MB      | 16 MB      | 416 kB     | 37 MB


压缩chunk
zabbix=# select compress_chunk('_timescaledb_internal._hyper_1_1_chunk');
             compress_chunk
----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
(1 row)
.......
-[ RECORD 8 ]------------------+----------------------
chunk_schema                   | _timescaledb_internal
chunk_name                     | _hyper_1_1_chunk
compression_status             | Compressed
before_compression_table_bytes | 3416064
before_compression_index_bytes | 2777088
before_compression_toast_bytes | 0
before_compression_total_bytes | 6193152
after_compression_table_bytes  | 65536
after_compression_index_bytes  | 16384
after_compression_toast_bytes  | 425984
after_compression_total_bytes  | 507904
node_name                      |



压缩后原 _hyper_1_1_chunk 大小为0
zabbix=# select pg_relation_size('_timescaledb_internal._hyper_1_1_chunk');
 pg_relation_size
------------------
                0
(1 row)
zabbix=# set search_path to _timescaledb_internal;
SET
zabbix=# \d
                           List of relations
        Schema         |            Name             | Type  |  Owner
-----------------------+-----------------------------+-------+----------
.......
 _timescaledb_internal | _hyper_1_1_chunk            | table | zabbix
 _timescaledb_internal | _hyper_1_2_chunk            | table | zabbix
 _timescaledb_internal | _hyper_1_3_chunk            | table | zabbix
.....
 _timescaledb_internal | compress_hyper_8_27_chunk   | table | zabbix
(40 rows)

通过explain可以看到 _hyper_1_1_chunk 压缩后 数据放到了 compress_hyper_8_27_chunk
zabbix=# explain select count(*) from history;
                                                   QUERY PLAN
-----------------------------------------------------------------------------------------------------------------
 Finalize Aggregate  (cost=6959.03..6959.04 rows=1 width=8)
   ->  Gather  (cost=6958.82..6959.03 rows=2 width=8)
         Workers Planned: 2
         ->  Partial Aggregate  (cost=5958.82..5958.83 rows=1 width=8)
               ->  Parallel Append  (cost=0.00..5514.00 rows=177927 width=0)
                     ->  Custom Scan (DecompressChunk) on _hyper_1_1_chunk  (cost=0.12..5.46 rows=46000 width=0)
                           ->  Parallel Seq Scan on compress_hyper_8_27_chunk  (cost=0.00..5.46 rows=46 width=4)    # 这里
                     ->  Parallel Seq Scan on _hyper_1_2_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_3_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_5_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_6_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_7_chunk  (cost=0.00..743.35 rows=33035 width=0)
                     ->  Parallel Seq Scan on _hyper_1_4_chunk  (cost=0.00..718.11 rows=31911 width=0)
                     ->  Parallel Seq Scan on _hyper_1_8_chunk  (cost=0.00..184.03 rows=8103 width=0)
(14 rows)

压缩后大小为40kB
zabbix=# select pg_size_pretty(pg_relation_size('_timescaledb_internal.compress_hyper_8_27_chunk'));
 pg_size_pretty
----------------
 40 kB
(1 row)


查看压缩任务
zabbix=# SELECT * FROM timescaledb_information.jobs;
 job_id |     application_name      | schedule_interval | max_runtime | max_retries | retry_period |      proc_schema      |     proc_name      |  owner   | scheduled |                     config                     |          next_start           | hypertable_schema | h
ypertable_name
--------+---------------------------+-------------------+-------------+-------------+--------------+-----------------------+--------------------+----------+-----------+------------------------------------------------+-------------------------------+-------------------+--
---------------
      1 | Telemetry Reporter [1]    | 24:00:00          | 00:01:40    |          -1 | 01:00:00     | _timescaledb_internal | policy_telemetry   | postgres | t         |                                                | 2021-10-30 08:46:15.187675+00 |                   |
   1006 | Compression Policy [1006] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 7, "compress_after": 612000} | 2021-10-30 12:26:03.791419+00 | public            | t
rends_uint
   1000 | Compression Policy [1000] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 1, "compress_after": 612000} | 2021-10-30 12:26:03.807164+00 | public            | h
istory
   1001 | Compression Policy [1001] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 2, "compress_after": 612000} | 2021-10-30 12:26:03.8262+00   | public            | h
istory_uint
   1002 | Compression Policy [1002] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 5, "compress_after": 612000} | 2021-10-30 12:26:03.851741+00 | public            | h
istory_str
   1003 | Compression Policy [1003] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 4, "compress_after": 612000} | 2021-10-30 12:26:03.868407+00 | public            | h
istory_text
   1005 | Compression Policy [1005] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 6, "compress_after": 612000} | 2021-10-30 12:26:03.914944+00 | public            | t
rends
   1004 | Compression Policy [1004] | 1 day             | 00:00:00    |          -1 | 01:00:00     | _timescaledb_internal | policy_compression | zabbix   | t         | {"hypertable_id": 3, "compress_after": 612000} | 2021-10-30 12:26:03.933253+00 | public            | h
istory_log

# time_bucket函数
查看itemid=10073的监控指标每5分钟的平均值
zabbix=# select time_bucket('300',clock) as five_min,avg(value) from history where itemid=10073 group by five_min order by five_min limit 10;
  five_min  |        avg
------------+--------------------
 1634949900 | 0.6498569713761893
 1634950200 | 0.6498569025011809
 1634950500 | 0.6498579033543198
 1634950800 | 0.6498616420345298
 1634951100 | 0.6498640946037003
 1634951400 |  0.649850038873476
 1634951700 | 0.6520572575130209
 1634952000 |  0.649848264986644
 1634952300 | 0.6498570006136769
 1634952600 | 0.6498655667358046

# first函数
查看每个itemid最早的一笔记录 
zabbix=# select itemid,first(value,clock) from history group by itemid;
 itemid |         first
--------+-----------------------
  10073 |    0.6498569713761893
  10074 |                     0
  10075 |                     0
  10076 |   0.08331538591753834
  10077 |                     0
  10078 |   0.13330488936035287
  23252 |                     0
  23253 |   0.16917611233293858
  23255 |                     0
  23256 |   0.06889424733034792
  23257 |   0.18606224627875506
 
# ts-dump备份   这里超表没有备份，需要单独copy备份
-bash-4.2$ ts-dump --db-URI=postgres://postgres:postgres@10.67.39.58:5432/zabbix --dump-dir=tsdump/
2021/11/01 08:45:04 Jobs:  have stopped, continuing
 pg_dump version: pg_dump (PostgreSQL) 13.4

2021/11/01 08:45:06 pg_dump: warning: there are circular foreign-key constraints on this table:
2021/11/01 08:45:06 pg_dump:   hypertable
2021/11/01 08:45:06 pg_dump: You might not be able to restore the dump without using --disable-triggers or temporarily dropping the constraints.
2021/11/01 08:45:06 pg_dump: Consider using a full dump instead of a --data-only dump to avoid this problem.
2021/11/01 08:45:06 pg_dump: warning: there are circular foreign-key constraints on this table:
2021/11/01 08:45:06 pg_dump:   chunk
2021/11/01 08:45:06 pg_dump: You might not be able to restore the dump without using --disable-triggers or temporarily dropping the constraints.
2021/11/01 08:45:06 pg_dump: Consider using a full dump instead of a --data-only dump to avoid this problem.
2021/11/01 08:45:07 pg_dump: NOTICE:  hypertable data are in the chunks, no data will be copied
2021/11/01 08:45:07 DETAIL:  Data for hypertables are stored in the chunks of a hypertable so COPY TO of a hypertable will not copy any data.
2021/11/01 08:45:07 HINT:  Use "COPY (SELECT * FROM <hypertable>) TO ..." to copy all data in hypertable, or copy each chunk individually.
2021/11/01 08:45:07 pg_dump: NOTICE:  hypertable data are in the chunks, no data will be copied
......
# pg_basebackup  只能备份本机

####################  prometheus + promscale + timescaledb2 POC
容器快速搭建环境
docker network create --driver bridge pnet

docker run --name timescaledb -e POSTGRES_PASSWORD=secret -d -p 5432:5432 --network=pnet timescaledev/promscale-extension:latest-ts2-pg13 postgres -csynchronous_commit=off

docker run --name promscale -d -p 9201:9201 --network=pnet timescale/promscale:0.6 -db-password=secret -db-port=5432 -db-name=postgres -db-host=timescaledb -db-ssl-mode=allow
此步完成后，会自动在PG里建promscale extension

编辑prometheus.yml，加入
remote_write:
        - url: "http://10.67.36.58:9201/write"
remote_read:
    - url: "http://10.67.36.58:9201/read"

重启prometheus
root@u1804:/var/snap/prometheus/current# systemctl restart snap.prometheus.prometheus.service

postgres=# SELECT count(*) FROM timescaledb_information.hypertables;
 count
-------
   767
(1 row) 

####################### prometheus + promscale + 多节点timescaledb2   ####################
容器快速搭建环境
1 分别运行3个timescaledb容器
docker run --name timescaledb -e POSTGRES_PASSWORD=secret -d -p 5432:5432 timescaledev/promscale-extension:latest-ts2-pg13 postgres -csynchronous_commit=off

2 每个节点做根据下面要求修改postgresql.conf，并stop/start container
max_prepared_transactions must be set to a non-zero value on all data nodes (if not already set, 150 is recommended).
enable_partitionwise_aggregate should be set to on on the access node for good query performance. Otherwise, queries will not be pushed down to the data nodes.
jit should be set to off on the access node as JIT currently doesn't work well with distributed queries. JIT can still be enabled on the data nodes

docker run -i -t timescale/timescaledb:latest-pg10 postgres -cmax_wal_size=2GB  # 可以直接在run的时候加上参数，不过在改参数的时候也有弊端

3 依次将dns,dns2节点加入为data_node，这里dns是写错的节点名称 -_-!!

postgres=# select add_data_node('dns',host=>'10.67.36.59');
NOTICE:  database "postgres" already exists on data node, skipping
NOTICE:  extension "timescaledb" already exists on data node, skipping
DETAIL:  TimescaleDB extension version on 10.67.36.59:5432 was 2.3.1.
             add_data_node
---------------------------------------
 (dns,10.67.36.59,5432,postgres,t,f,f)
(1 row)

postgres=# select add_data_node('dns2',host=>'10.67.36.57');
NOTICE:  database "postgres" already exists on data node, skipping
NOTICE:  extension "timescaledb" already exists on data node, skipping
DETAIL:  TimescaleDB extension version on 10.67.36.57:5432 was 2.3.1.
             add_data_node
----------------------------------------
 (dns2,10.67.36.57,5432,postgres,t,f,f)
(1 row)

postgres=# select * from timescaledb_information.data_nodes;
 node_name |  owner   |                   options
-----------+----------+----------------------------------------------
 dns       | postgres | {host=10.67.36.59,port=5432,dbname=postgres}
 dns2      | postgres | {host=10.67.36.57,port=5432,dbname=postgres}
(2 rows)

底层在查询是应该是借助了FDW
postgres=# \des+
                                                             List of foreign servers
 Name |  Owner   | Foreign-data wrapper | Access privileges | Type | Version |                     FDW options                      | Description
------+----------+----------------------+-------------------+------+---------+------------------------------------------------------+-------------
 dns  | postgres | timescaledb_fdw      |                   |      |         | (host '10.67.36.59', port '5432', dbname 'postgres') |
 dns2 | postgres | timescaledb_fdw      |                   |      |         | (host '10.67.36.57', port '5432', dbname 'postgres') |
(2 rows)

4 运行promscale容器
docker run --name promscale_3ts -d -p 9201:9201 timescale/promscale:0.6 -db-password=secret -db-port=5432 -db-name=postgres -db-host=10.67.36.60 -db-ssl-mode=allow

5 在access node上测试分布式hypertable
postgres=# CREATE TABLE conditions21 (
 time        TIMESTAMPTZ       NOT NULL,
 location    TEXT              NOT NULL,
 temperature DOUBLE PRECISION  NULL,
 humidity    DOUBLE PRECISION  NULL
 );
CREATE TABLE

postgres=# SELECT create_distributed_hypertable('conditions', 'time', 'location');  #单副本
 create_distributed_hypertable
-------------------------------
 (1,public,conditions,t)
(1 row)

postgres=# SELECT * FROM timescaledb_information.hypertables;
 hypertable_schema | hypertable_name |  owner   | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces
-------------------+-----------------+----------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
 public            | conditions      | postgres |              2 |          2 | f                   | t              |                  1 | {dns,dns2} |
(1 row)

插入数据后查询
postgres=# SELECT chunk_name, data_nodes
postgres-# FROM timescaledb_information.chunks
postgres-# WHERE hypertable_name = 'conditions';
      chunk_name       | data_nodes
-----------------------+------------
 _dist_hyper_1_1_chunk | {dns}
 _dist_hyper_1_2_chunk | {dns2}
(2 rows)

通过执行计划看到查询下推到两个data node
postgres=# explain (verbose) select * from conditions;
                                                                              QUERY PLAN
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (AsyncAppend)  (cost=100.00..1873158.09 rows=41839260 width=56)
   Output: conditions."time", conditions.location, conditions.temperature, conditions.humidity
   ->  Append  (cost=100.00..1873158.09 rows=41839260 width=56)
         ->  Custom Scan (DataNodeScan) on public.conditions conditions_1  (cost=100.00..831980.90 rows=20919630 width=56)
               Output: conditions_1."time", conditions_1.location, conditions_1.temperature, conditions_1.humidity
               Data node: dns
               Chunks: _dist_hyper_1_1_chunk
               Remote SQL: SELECT "time", location, temperature, humidity FROM public.conditions WHERE _timescaledb_internal.chunks_in(public.conditions.*, ARRAY[1])
         ->  Custom Scan (DataNodeScan) on public.conditions conditions_2  (cost=100.00..831980.90 rows=20919630 width=56)
               Output: conditions_2."time", conditions_2.location, conditions_2.temperature, conditions_2.humidity
               Data node: dns2
               Chunks: _dist_hyper_1_2_chunk
               Remote SQL: SELECT "time", location, temperature, humidity FROM public.conditions WHERE _timescaledb_internal.chunks_in(public.conditions.*, ARRAY[1])

分别查看两个chunk
postgres=# \d+ _timescaledb_internal._dist_hyper_1_1_chunk
                                  Foreign table "_timescaledb_internal._dist_hyper_1_1_chunk"
   Column    |           Type           | Collation | Nullable | Default | FDW options | Storage  | Stats target | Description
-------------+--------------------------+-----------+----------+---------+-------------+----------+--------------+-------------
 time        | timestamp with time zone |           | not null |         |             | plain    |              |
 location    | text                     |           | not null |         |             | extended |              |
 temperature | double precision         |           |          |         |             | plain    |              |
 humidity    | double precision         |           |          |         |             | plain    |              |
Check constraints:
    "constraint_1" CHECK ("time" >= '2021-10-28 00:00:00+00'::timestamp with time zone AND "time" < '2021-11-04 00:00:00+00'::timestamp with time zone)
    "constraint_2" CHECK (_timescaledb_internal.get_partition_hash(location) < 1073741823)
Server: dns
Inherits: conditions

postgres=# \d+ _timescaledb_internal._dist_hyper_1_2_chunk
                                  Foreign table "_timescaledb_internal._dist_hyper_1_2_chunk"
   Column    |           Type           | Collation | Nullable | Default | FDW options | Storage  | Stats target | Description
-------------+--------------------------+-----------+----------+---------+-------------+----------+--------------+-------------
 time        | timestamp with time zone |           | not null |         |             | plain    |              |
 location    | text                     |           | not null |         |             | extended |              |
 temperature | double precision         |           |          |         |             | plain    |              |
 humidity    | double precision         |           |          |         |             | plain    |              |
Check constraints:
    "constraint_1" CHECK ("time" >= '2021-10-28 00:00:00+00'::timestamp with time zone AND "time" < '2021-11-04 00:00:00+00'::timestamp with time zone)
    "constraint_3" CHECK (_timescaledb_internal.get_partition_hash(location) >= 1073741823)
Server: dns2
Inherits: conditions

创建两副本的分布式超表
CREATE TABLE conditions_rep2 (
  time        TIMESTAMPTZ       NOT NULL,
  location    TEXT              NOT NULL,
  temperature DOUBLE PRECISION  NULL,
  humidity    DOUBLE PRECISION  NULL
);
postgres=# SELECT create_distributed_hypertable('conditions_rep2', 'time', 'location',
postgres(#     replication_factor => 2);
 create_distributed_hypertable
-------------------------------
 (2,public,conditions_rep2,t)
(1 row)

INSERT INTO conditions_rep2
  VALUES
    (NOW(), 'office', 70.0, 50.3),
    (NOW(), 'basement', 66.5, 61.0),
	(NOW(), 'office', 71.3, 50.0),
    (NOW(), 'garage', 66.5, 60.0),
	(NOW(), 'office', 70.0, 50.0),
    (NOW(), 'basement', 66.5, 60.0),
	(NOW(), 'office', 70.2, 50.0),
    (NOW(), 'basement', 66.5, 60.0),
	(NOW(), 'garage', 70.9, 50.0),
    (NOW(), 'basement', 66.5, 60.5),
	(NOW(), 'office', 70.0, 51.0),
    (NOW(), 'basement', 66.5, 60.0),
    (NOW(), 'garage', 77.6, 65.2);

#查看chunk分布	 单副本和两副本都能看到
postgres=# select hypertable_schema,hypertable_name,chunk_schema,chunk_name,is_compressed,data_nodes from timescaledb_information.chunks ;
 hypertable_schema | hypertable_name |     chunk_schema      |      chunk_name       | is_compressed | data_nodes
-------------------+-----------------+-----------------------+-----------------------+---------------+------------
 public            | conditions      | _timescaledb_internal | _dist_hyper_1_1_chunk | f             | {dns}
 public            | conditions      | _timescaledb_internal | _dist_hyper_1_2_chunk | f             | {dns2}
 public            | conditions_rep2 | _timescaledb_internal | _dist_hyper_2_3_chunk | f             | {dns,dns2}
 public            | conditions_rep2 | _timescaledb_internal | _dist_hyper_2_4_chunk | f             | {dns,dns2}
	
6 prometheus配置prom_scale
编辑prometheus.yml，加入
remote_write:
        - url: "http://10.67.36.62:9201/write"
remote_read:
    - url: "http://10.67.36.62:9201/read"

重启prometheus
root@u1804:/var/snap/prometheus/current# systemctl restart snap.prometheus.prometheus.service

查看所有的hypertable
select hypertable_schema,hypertable_name,chunk_schema,chunk_name,is_compressed,data_nodes from timescaledb_information.chunks ;
hypertable_schema |                        hypertable_name                         |  owner   | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces
-------------------+----------------------------------------------------------------+----------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
 public            | conditions                                                     | postgres |              2 |          2 | f                   | t              |                  1 | {dns,dns2} |
 public            | conditions_rep2                                                | postgres |              2 |          2 | f                   | t              |                  2 | {dns,dns2} |
 prom_data         | node_cpu_seconds_total                                         | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_next_gc_bytes                                      | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_heap_objects                                       | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_mspan_inuse_bytes                                  | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_lookups_total                                      | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_stack_sys_bytes                                    | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | node_boot_time_seconds                                         | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | node_context_switches_total                                    | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | node_cpu_guest_seconds_total                                   | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_sys_bytes                                          | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_mspan_sys_bytes                                    | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_alloc_bytes                                        | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_mcache_inuse_bytes                                 | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_mcache_sys_bytes                                   | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_buck_hash_sys_bytes                                | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | node_arp_entries                                               | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_memstats_frees_total                                        | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |
 prom_data         | go_threads                                                     | postgres |              1 |          1 | t                   | t              |                  1 | {dns,dns2} |

数据大部分都存在chuck上，可以从三个节点分别看到
access node
postgres=# \l+ postgres
                                                                List of databases
   Name   |  Owner   | Encoding |  Collate   |   Ctype    | Access privileges |  Size  | Tablespace |                Description
----------+----------+----------+------------+------------+-------------------+--------+------------+--------------------------------------------
 postgres | postgres | UTF8     | en_US.utf8 | en_US.utf8 |                   | 259 MB | pg_default | default administrative connection database
(1 row)

dn1
postgres=# \l+ postgres
                                                                List of databases
   Name   |  Owner   | Encoding |  Collate   |   Ctype    | Access privileges |  Size   | Tablespace |                Description
----------+----------+----------+------------+------------+-------------------+---------+------------+--------------------------------------------
 postgres | postgres | UTF8     | en_US.utf8 | en_US.utf8 |                   | 5521 MB | pg_default | default administrative connection database
(1 row)

dn2
postgres=# \l+ postgres
                                                                List of databases
   Name   |  Owner   | Encoding |  Collate   |   Ctype    | Access privileges |  Size   | Tablespace |                Description
----------+----------+----------+------------+------------+-------------------+---------+------------+--------------------------------------------
 postgres | postgres | UTF8     | en_US.utf8 | en_US.utf8 |                   | 5758 MB | pg_default | default administrative connection database
(1 row)

 
 
# 在原有集群的基础上新增节点
Expanding the cluster
When adding nodes to a TimescaleDB cluster that is already being written to by Promscale, you should run the add_prom_node(node_name) function after running the standard add_data_node() function. For example:

SELECT add_data_node('example_node_name', host => 'example_host_address')
SELECT add_prom_node('example_node_name');
Note: add_prom_node should be run by the same database user, as the one writing data from Promscale. 

-- there is an implicit CROSS JOIN between the two generate_series() sets
SELECT time, device_id, random()*100 as cpu_usage 
FROM generate_series('2021-01-01 00:00:00','2021-01-01 04:00:00',INTERVAL '1 hour') as time, 
generate_series(1,4) device_id;


time               |device_id|cpu_usage          |
-------------------+---------+-------------------+
2021-01-01 00:00:00|        1|0.35415126479989567|
2021-01-01 01:00:00|        1| 14.013393572770028|
2021-01-01 02:00:00|        1|   88.5015939122006|
2021-01-01 03:00:00|        1|  97.49037810105996|

#### 删除数据库
zabbix=# drop database zabbix;
ERROR:  cannot drop the currently open database


postgres=# drop database zabbix with (FORCE);
NOTICE:  TimescaleDB distributed database might require additional cleanup on the data nodes
DETAIL:  Distributed database UUID is "83ff9b47-630a-4ec6-aac7-b823ca1eb4fd".
DROP DATABASE

##################################################################################################################################################################
##### zabbix server + pgpool(repmgr) + 3 access node + 4 data node

在access node mydb01上执行
\c zabbix            用户是postgres
create extension timescaledb;

select add_data_node('dn1',host=>'10.152.219.66');
select add_data_node('dn2',host=>'10.152.219.210');
select * from timescaledb_information.data_nodes;

先测试能否建分布表         这里是postgres用户
CREATE TABLE conditions21 ( time        TIMESTAMPTZ       NOT NULL, location    TEXT              NOT NULL, temperature DOUBLE PRECISION  NULL, humidity    DOUBLE PRECISION  NULL );
\d
select create_distributed_hypertable('conditions21','time','location',replication_factor=>2);
SELECT * FROM timescaledb_information.hypertables;

测试ok后，启动zabbix server容器       这里的思路是zabbix用户建表，postgres用户来创建distributed hypertable，hypertable的owner是postgres,所以后面要授很多权给zabbix
docker run --name zabbix-server-timescalesb -d \
-e TZ='Asia/Shanghai' \
-e ZBX_SERVER_NAME='zabbixserver' \
-e DB_SERVER_HOST="10.67.51.222" \
-e POSTGRES_USER="zabbix" \
-e POSTGRES_PASSWORD="Foxconn123" \
-p 10051:10051 \
zabbix/zabbix-server-pgsql:centos-5.4.7

准备执行下面脚本，转换原表为分布表
        SELECT create_distributed_hypertable('history', 'clock', chunk_time_interval => 86400, replication_factor=>2);       
        SELECT create_distributed_hypertable('history_uint', 'clock', chunk_time_interval => 86400, replication_factor=>2);
        SELECT create_distributed_hypertable('history_log', 'clock', chunk_time_interval => 86400, replication_factor=>2);
        SELECT create_distributed_hypertable('history_text', 'clock', chunk_time_interval => 86400, replication_factor=>2);
        SELECT create_distributed_hypertable('history_str', 'clock', chunk_time_interval => 86400, replication_factor=>2);
        SELECT create_distributed_hypertable('trends', 'clock', chunk_time_interval => 2592000, replication_factor=>2);        
        SELECT create_distributed_hypertable('trends_uint', 'clock', chunk_time_interval => 2592000, replication_factor=>2);
        UPDATE config SET db_extension='timescaledb',hk_history_global=1,hk_trends_global=1;
        UPDATE config SET compression_status=1,compress_older='7d';
		
********遇到一堆错误    *******
1 分布表不支持migrate_data选项
zabbix=> SELECT create_distributed_hypertable('history', 'clock', chunk_time_interval => 86400, migrate_data => true,replication_factor=>2);
ERROR:  cannot migrate data for distributed hypertable

2 没有data node上的权限
zabbix=> SELECT create_distributed_hypertable('history', 'clock', chunk_time_interval => 86400, replication_factor=>2);
NOTICE:  2 of 2 data nodes not used by this hypertable due to lack of permissions
HINT:  Grant USAGE on data nodes to attach them to a hypertable.
ERROR:  no data nodes can be assigned to the hypertable
DETAIL:  Data nodes exist, but none have USAGE privilege.
HINT:  Grant USAGE on data nodes to attach them to the hypertable.

解决
zabbix=# grant usage on foreign server "dn1","dn2" to postgres;
GRANT

3 非空表不能转换成分布表
zabbix=# SELECT create_distributed_hypertable('history', 'clock', chunk_time_interval => 86400, replication_factor=>2);
ERROR:  table "history" is not empty
HINT:  You can migrate data by specifying 'migrate_data => true' when calling this function.

解决
delete from history;




********遇到错误  没有连接dn的密码*******		
Apr 18 11:16:17 pgpool pgpool: 2022-04-18 11:16:17.236: [unknown] pid 17134: LOG:  pool_send_and_wait: Error or notice message from backend: : DB node id: 0 backend pid: 12339 statement: "alter table history set (timescaledb.compress,ti                                   mescaledb.compress_segmentby='itemid',timescaledb.compress_orderby='clock,ns')" message: "could not connect to "dn1""
Apr 18 11:16:17 pgpool pgpool: 2022-04-18 11:16:17.292: [unknown] pid 17134: LOG:  pool_send_and_wait: Error or notice message from backend: : DB node id: 0 backend pid: 12339 statement: "alter table history_uint set (timescaledb.compre                                   ss,timescaledb.compress_segmentby='itemid',timescaledb.compress_orderby='clock,ns')" message: "could not connect to "dn1""
2022-04-18 11:16:17.236 CST [12339] ERROR:  could not connect to "dn1"
2022-04-18 11:16:17.236 CST [12339] DETAIL:  connection to server at "10.152.219.66", port 5432 failed: fe_sendauth: no password supplied
		
解决	在access node上执行
create user mapping for zabbix server dn1 options(user 'zabbix',password 'Foxconn123');
create user mapping for zabbix server dn2 options(user 'zabbix',password 'Foxconn123');

********遇到错误 没有写入history表的权限*******
Apr 18 14:27:06 pgpool pgpool: " message: "[dn1]: permission denied for table "history""
Apr 18 14:27:07 pgpool pgpool: 2022-04-18 14:27:07.895: [unknown] pid 17521: LOG:  pool_send_and_wait: Error or notice message from backend: : DB node id: 0 backend pid: 14149 statement: "insert into history (itemid,clock,ns,value) values (10078,1650263218,9595054,0.13454704410099078);
2022-04-18 14:27:50.079 CST [14149] ERROR:  [dn2]: permission denied for table "history"
2022-04-18 14:27:50.079 CST [14149] DETAIL:  Insert privileges required on "history" to create chunks.
2022-04-18 14:27:50.079 CST [14149] STATEMENT:  insert into history (itemid,clock,ns,value) values (23620,1650263260,19487206,0.22412351017473048),(23260,1650263260,19592596,0);

解决  分别在dn上执行
zabbix=# grant all privileges on all tables in schema public to zabbix;
GRANT

权限问题解决后，貌似正常了
查看所有的hypertable
zabbix=# select hypertable_schema,hypertable_name,chunk_schema,chunk_name,is_compressed,data_nodes from timescaledb_information.chunks ;
 hypertable_schema | hypertable_name |     chunk_schema      |       chunk_name        | is_compressed | data_nodes
-------------------+-----------------+-----------------------+-------------------------+---------------+------------
 public            | history         | _timescaledb_internal | _dist_hyper_2_309_chunk | f             | {dn1,dn2}
 public            | history_uint    | _timescaledb_internal | _dist_hyper_3_310_chunk | f             | {dn1,dn2}

zabbix=# SELECT * FROM timescaledb_information.hypertables;
 hypertable_schema | hypertable_name |  owner   | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces
-------------------+-----------------+----------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
 public            | conditions21    | postgres |              2 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history         | zabbix   |              1 |          1 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history_uint    | zabbix   |              1 |          1 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history_log     | zabbix   |              1 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history_text    | zabbix   |              1 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history_str     | zabbix   |              1 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | trends          | zabbix   |              1 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | trends_uint     | zabbix   |              1 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
(8 rows)

zabbix=# select * from timescaledb_information.chunks ;
 hypertable_schema | hypertable_name |     chunk_schema      |       chunk_name        | primary_dimension | primary_dimension_type | range_start | range_end | range_start_integer | range_end_integer | is_compressed | chunk_tablespace | data_nodes
-------------------+-----------------+-----------------------+-------------------------+-------------------+------------------------+-------------+-----------+---------------------+-------------------+---------------+------------------+------------
 public            | history         | _timescaledb_internal | _dist_hyper_2_309_chunk | clock             | integer                |             |           |          1650240000 |        1650326400 | f             |                  | {dn1,dn2}
 public            | history_uint    | _timescaledb_internal | _dist_hyper_3_310_chunk | clock             | integer                |             |           |          1650240000 |        1650326400 | f             |                  | {dn1,dn2}
(2 rows)



zabbix=# select public.show_chunks('history');
                  show_chunks
-----------------------------------------------
 _timescaledb_internal._dist_hyper_2_309_chunk
(1 row)

#### timescaledb的认证
1 节点之间的认证
data node更改pg_hba.conf  access node配置.pgpass
2 能创建分布式超表的用户的认证
添加非superuser用户, 非superuser用户为分布表的owner
create role chensen with login password 'chensen';
\c zabbix
call distributed_exec($$ create role chensen with login password 'chensen' $$);
grant usage on foreign server dn1,dn2 to chensen;
CREATE TABLE conditions100 (time timestamptz NOT NULL, device integer, temp float);    用户chensen登录
SELECT create_distributed_hypertable('conditions100', 'time', 'device',replication_factor=>2);
zabbix=> SELECT * FROM timescaledb_information.hypertables;
 hypertable_schema | hypertable_name |  owner   | num_dimensions | num_chunks | compression_enabled | is_distributed | replication_factor | data_nodes | tablespaces
-------------------+-----------------+----------+----------------+------------+---------------------+----------------+--------------------+------------+-------------
 public            | conditions21    | postgres |              2 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history         | zabbix   |              1 |          1 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history_uint    | zabbix   |              1 |          1 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history_log     | zabbix   |              1 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history_text    | zabbix   |              1 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | history_str     | zabbix   |              1 |          1 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | trends          | zabbix   |              1 |          1 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | trends_uint     | zabbix   |              1 |          1 | f                   | t              |                  2 | {dn1,dn2}  |
 public            | conditions100   | chensen  |              2 |          0 | f                   | t              |                  2 | {dn1,dn2}  |
(9 rows)

INSERT INTO conditions100
SELECT time, (random()*30)::int, random()*80
FROM generate_series('2021-04-01 00:00:00'::timestamptz, '2022-03-31 23:59:59', '1 sec') AS time; 

手动删除conditions100表6个月的数据
zabbix=# select drop_chunks('conditions100',INTERVAL '6 months');
                  drop_chunks
------------------------------------------------
 _timescaledb_internal._dist_hyper_11_426_chunk
 _timescaledb_internal._dist_hyper_11_427_chunk
 _timescaledb_internal._dist_hyper_11_428_chunk
 _timescaledb_internal._dist_hyper_11_429_chunk
 _timescaledb_internal._dist_hyper_11_430_chunk
 _timescaledb_internal._dist_hyper_11_431_chunk
 _timescaledb_internal._dist_hyper_11_432_chunk
 _timescaledb_internal._dist_hyper_11_433_chunk
 _timescaledb_internal._dist_hyper_11_434_chunk
 _timescaledb_internal._dist_hyper_11_435_chunk
 _timescaledb_internal._dist_hyper_11_436_chunk

 添加数据保留策略
 zabbix=# select add_retention_policy('conditions100',INTERVAL '6 months');
 add_retention_policy
----------------------
                 1000
 
zabbix=# select * from timescaledb_information.jobs;
 job_id |    application_name     | schedule_interval | max_runtime | max_retries | retry_period |      proc_schema      |    proc_name     |  owner   | scheduled |                    config                     |          next_start           | hypertable_schema | hypert
able_name
--------+-------------------------+-------------------+-------------+-------------+--------------+-----------------------+------------------+----------+-----------+-----------------------------------------------+-------------------------------+-------------------+-------
----------
      1 | Telemetry Reporter [1]  | 24:00:00          | 00:01:40    |          -1 | 01:00:00     | _timescaledb_internal | policy_telemetry | postgres | t         |                                               | 2022-04-19 23:48:01.710119+08 |                   |
   1000 | Retention Policy [1000] | 1 day             | 00:05:00    |          -1 | 00:05:00     | _timescaledb_internal | policy_retention | chensen  | t         | {"drop_after": "6 mons", "hypertable_id": 11} | 2022-04-20 16:05:18.741537+08 | public            | condit
ions100
(2 rows)

-------------------------------------------------------------------------------------------------------------------------------------------------------------
For security purposes, only superusers or users with necessary privileges can add data nodes (see below for details). When adding a data node, 
the access node also tries to connect to the data node and therefore needs a way to authenticate with it. TimescaleDB currently supports several different 
such authentication methods for flexibility (including trust, user mappings, password, and certificate methods).
Unless bootstrap is false, the function attempts to bootstrap the data node by:

Creating the database given in database that serve as the new data node.
Loading the TimescaleDB extension in the new database.
Setting metadata to make the data node part of the distributed database.

在新增的data node上添加zabbix用户
call distributed_exec($$ create role zabbix with login password 'Foxconn123' $$,node_list=>'{"dn3","dn4"}');

将dn3和dn4加入到history表
zabbix=# select * from attach_data_node('dn3','history');
 hypertable_id | node_hypertable_id | node_name
---------------+--------------------+-----------
             2 |                  1 | dn3
(1 row)

zabbix=# select * from attach_data_node('dn4','history');
 hypertable_id | node_hypertable_id | node_name
---------------+--------------------+-----------
             2 |                  1 | dn4
(1 row)

select * from attach_data_node('dn3','history_uint');
select * from attach_data_node('dn3','history_log');
select * from attach_data_node('dn3','history_text');
select * from attach_data_node('dn3','history_str');
select * from attach_data_node('dn3','history_trends');
select * from attach_data_node('dn3','trends');
select * from attach_data_node('dn3','trends_uint');
select * from attach_data_node('dn4','trends_uint');
select * from attach_data_node('dn4','trends');
select * from attach_data_node('dn4','history_str');
select * from attach_data_node('dn4','history_text');
select * from attach_data_node('dn4','history_log');
select * from attach_data_node('dn4','history_uint');

第二天发现写入新的chuck到新节点时有权限错误
181:20220421:082806.812 [Z3005] query failed: [0] PGRES_FATAL_ERROR:ERROR:  [dn3]: permission denied for table "history"
DETAIL:  Insert privileges required on "history" to create chunks.
 [insert into history (itemid,clock,ns,value) values (36906,1650500886,263176919,0),(37146,1650500886,279791330,0);
]
   179:20220421:082807.919 [Z3005] query failed: [0] PGRES_FATAL_ERROR:ERROR:  [dn4]: permission denied for table "history_uint"
DETAIL:  Insert privileges required on "history_uint" to create chunks.
 [insert into history_uint (itemid,clock,ns,value) values (37087,1650500887,280329964,0),(37027,1650500887,288442663,0),(36907,1650500887,499099618,52400128);
]
解决  分别在dn上执行
zabbix=# grant all privileges on all tables in schema public to zabbix;
GRANT

#  总共4个data node 3 access node  1 pgpool
zabbix=# select * from timescaledb_information.data_nodes;
 node_name |  owner   |                    options
-----------+----------+-----------------------------------------------
 dn1       | postgres | {host=10.152.219.66,port=5432,dbname=zabbix}
 dn2       | postgres | {host=10.152.219.210,port=5432,dbname=zabbix}
 dn3       | postgres | {host=10.152.219.249,port=5432,dbname=zabbix}
 dn4       | postgres | {host=10.152.219.224,port=5432,dbname=zabbix}
(4 rows)

-------------------------------------------------------------------------------------------------------------------------------------------------------------
移动chunk    还需要wal_level >=logical
Required settings
When moving a chunk, the destination data node needs a way to authenticate with the data node that holds the source chunk.
 It is currently recommended to use a password file on the data node.

The wal_level setting must also be set to logical or higher on data nodes from which chunks are moved. 
If you are copying or moving many chunks in parallel, you can increase max_wal_senders and max_replication_slots.

CALL timescaledb_experimental.move_chunk('_timescaledb_internal._dist_hyper_2_534_chunk', 'dn2', 'dn3');
zabbix=# CALL timescaledb_experimental.move_chunk('_timescaledb_internal._dist_hyper_2_534_chunk', 'dn2', 'dn3');
ERROR:  [dn2]: logical decoding requires wal_level >= logical
DETAIL:  Chunk copy operation id: ts_copy_2_534.

-- 启用压缩
ALTER TABLE conditions100 SET (
 timescaledb.compress,
 timescaledb.compress_segmentby = 'time'
);
-- See info about compression
SELECT * FROM timescaledb_information.compression_settings;