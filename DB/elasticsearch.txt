######ELK Upgrade order#########
You should upgrade the core Elastic Stack products in the following order:

Elasticsearch: upgrade instructions
Kibana: upgrade instructions
Logstash: upgrade instructions
Beats: upgrade instructions
6.8的时候, ingest-geoip和ingest-user-agent已经不用单独安装了

#1  rolling upgrade elasticsearch repository-s3   6.4.0->6.8.1

# curl -X PUT "10.67.51.150:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}
'
# systemctl stop elasticsearch
# rpm -Uvh elasticsearch-6.8.1.rpm
# systemctl daemon-reload
# /usr/share/elasticsearch/bin/elasticsearch-plugin remove repository-s3
# /usr/share/elasticsearch/bin/elasticsearch-plugin remove --purge ingest-geoip
# /usr/share/elasticsearch/bin/elasticsearch-plugin remove ingest-user-agent
# /usr/share/elasticsearch/bin/elasticsearch-plugin list
# /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/repository-s3-6.8.1.zip
# systemctl start elasticsearch
# tail -f /var/log/elasticsearch/logging.log
# curl -X PUT "10.67.36.49:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}
'

############repository-s3 与minio ###############
minio 配置
[root@rancher ~]# cat minio.sh
docker run -p 9000:9000 --name minio1 \
    -e "MINIO_ACCESS_KEY=foxobjectstorage" \
    -e "MINIO_SECRET_KEY=foxobjectstorageminiominio" \
    -v /data:/data \
    minio/minio server /data

repository-s3安装  每台机器都需要操作
/usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/repository-s3-6.4.0.zip
配置s3 endpoint, access key, secret key

[root@es elasticsearch]# tail -3 /etc/elasticsearch/elasticsearch.yml
s3.client.default.endpoint: "10.67.36.58:9000"
s3.client.default.protocol: http

/usr/share/elasticsearch/bin/elasticsearch-keystore add s3.client.default.access_key
/usr/share/elasticsearch/bin/elasticsearch-keystore add s3.client.default.secret_key

systemctl restart elasticsearch
创建repository
[root@es elasticsearch]# !1019
curl -X PUT 10.67.36.49:9200/_snapshot/logging -H 'Content-Type: application/json' -d'
{
  "type": "s3",
  "settings": {
    "bucket": "logging"
  }
}'
{"acknowledged":true}
查看repository
[root@es elasticsearch]# curl -s -X GET 10.67.74.235:9200/_snapshot/backup |python -m json.tool
{
    "backup": {
        "settings": {
            "bucket": "backup"
        },
        "type": "s3"
    }
}
#########snapshot########
查看snapshot
[root@es elasticsearch]# curl -s -X GET 10.67.74.235:9200/_snapshot/backup/demo-2019.02.23-snapshot |python -m json.tool
{
    "snapshots": [
        {
            "duration_in_millis": 648,
            "end_time": "2019-06-18T01:03:17.439Z",
            "end_time_in_millis": 1560819797439,
            "failures": [],
            "include_global_state": true,
            "indices": [
                "demo-2019.02.23"
            ],
            "shards": {
                "failed": 0,
                "successful": 5,
                "total": 5
            },
            "snapshot": "demo-2019.02.23-snapshot",
            "start_time": "2019-06-18T01:03:16.791Z",
            "start_time_in_millis": 1560819796791,
            "state": "SUCCESS",
            "uuid": "iMHqEN8SQlWyZoQIQWYRhg",
            "version": "6.4.0",
            "version_id": 6040099
        }
    ]
}
[root@es elasticsearch]# curl -s -X GET 10.67.74.235:9200/_snapshot/backup/demo-2019.02.24-snapshot/_status |python -m json.tool |grep '"state"'
            "state": "SUCCESS",
还原snapshot
[root@es elasticsearch]# curl -X POST "10.67.74.235:9200/_snapshot/backup/demo-2019.02.23-snapshot/_restore"
{"accepted":true}[root@es elasticsearch]#
[root@es elasticsearch]# curl -X POST "10.67.74.235:9200/_snapshot/backup/demo-2019.02.24-snapshot/_restore"
{"accepted":true}[root@es elasticsearch]#

####查询elasticsearch所有模板######
[root@cobbler ~]# curl -s -XGET "http://10.67.51.150:9200/_template" |python -m json.tool | jq 'keys'
[
  ".ml-anomalies-",
  ".ml-meta",
  ".ml-notifications",
  ".ml-state",
  ".monitoring-alerts",
  ".monitoring-beats",
  ".monitoring-es",
  ".monitoring-kibana",
  ".monitoring-logstash",
  ".triggered_watches",
  ".watch-history-9",
  ".watches",
  "filebeat-6.4.0",
  "kibana_index_template:.kibana",
  "logstash",
  "logstash-index-template",
  "metricbeat-6.4.0",
  "security-index-template",
  "security_audit_log",
  "winlogbeat-6.4.0"
]

#####查各个index pattern的docs数(包含了replica)#########
[root@es1 ~]# /usr/bin/curator_cli --config /root/curator/curator.yml show_indices --verbose |grep -v '^\.[a-z]*' |awk '{t=length($1);print substr($1,0,t-10),$4}' |awk '{S[$1]+=$NF} END {for(a in S) print a, S[a]}'
swarm- 44086280
rke- 129671386
k8s- 57192234
filebeat-6.4.0- 141977796
fluentd- 1319124
winlogbeat-6.4.0- 1177064
查各个index pattern的docs数,并除以(replica+1)
[root@es1 ~]# /usr/bin/curator_cli --config /root/curator/curator.yml show_indices --verbose |grep -v '^\.[a-z]*' |awk '{t=length($1);print substr($1,0,t-10),$4/($6+1)}' |awk '{S[$1]+=$NF} END {for(a in S) print a, S[a]}'
swarm- 22043188
rke- 64836169
k8s- 19264321
filebeat-6.4.0- 70990681
fluentd- 659574
winlogbeat-6.4.0- 588537




#########合理配置主节点和数据节点######
配置文件：conf/elasticsearch.yaml
node.master: true
node.data: true

1) 当master为false，而data为true时，会对该节点产生严重负荷；
2) 当master为true，而data为false时，该节点作为一个协调者；
3) 当master为false，data也为false时，该节点就变成了一个负载均衡器

#为index k8s-*更改replica数量
curl -X PUT "http://10.67.36.53:9200/k8s-*/_settings" -H 'Content-Type: application/json' -d'
{
    "index" : {
        "number_of_replicas" : 2
    }
}
'


Kibana配置文件
[root@es kibana]# cat kibana.yml |egrep -v '#|^$'
server.port: 5601
server.host: "10.67.51.123"
elasticsearch.url: "http://10.67.51.123:9200"
kibana.index: ".kibana"
pid.file: /var/run/kibana.pid

###ES集群搭建
环境： centos7.5
ES版本： 6.4
三台ES节点
0 装系统时不需要swap分区
1 cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.67.36.53 es1
10.67.36.52 es2
10.67.36.51 es3

configure resolv.conf  ntp.conf 
disable firewalld,  selinux
[root@es4 elasticsearch]# pvcreate /dev/sdb
[root@es4 elasticsearch]# vgcreate es /dev/sdb
[root@es4 elasticsearch]# lvcreate -l +100%FREE -n data es
[root@es4 elasticsearch]# mkfs.xfs /dev/mapper/es-data
[root@es4 elasticsearch]# mkdir /data
[root@es4 elasticsearch]# mount /dev/mapper/es-data /data

[root@es4 elasticsearch]# cat /etc/fstab |grep noatime
/dev/mapper/es-data     /data                   xfs     defaults,noatime   0 0

2 安装Java8 (oracle java or openjdk)
java -version
vi /etc/bashrc
export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64/jre"
export CLASSPATH=${JAVA_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH

echo $JAVA_HOME

3 安装ES   (实际是手动rpm安装的)
  Download and install the public signing key:
  rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
  repo文件
[elasticsearch-6.x]
name=Elasticsearch repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md

yum -y install elasticsearch
systemctl enable elasticsearch

***optional 安装es plugins
例如filebeat apache2模块收集日志，需要es安装 ingest-geoip ingest-user-agent
    bin/elasticsearch-plugin install ingest-user-agent
    bin/elasticsearch-plugin install ingest-geoip
代理:  (没成功, 不确定是不是代理问题)
ES_JAVA_OPTS="-Dhttp.proxyHost=10.67.214.210 -Dhttp.proxyPort=808 -Dhttps.proxyHost=10.67.214.210 -Dhttps.proxyPort=808" /usr/share/elasticsearch/bin/elasticsearch-plugin install ingest-user-agent
手动下载 plugin地址
https://artifacts.elastic.co/downloads/elasticsearch-plugins/ingest-geoip/ingest-geoip-6.4.0.zip
https://artifacts.elastic.co/downloads/elasticsearch-plugins/ingest-user-agent/ingest-user-agent-6.4.0.zip
https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-s3/repository-s3-6.4.0.zip
***
[root@es4 elasticsearch]# /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/repository-s3-6.8.1.zip
[root@es4 elasticsearch]# /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/ingest-geoip-6.4.0.zip
[root@es1 ~]# /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/ingest-user-agent-6.4.0.zip

4 System properties参数设置
#/etc/systemd/system/elasticsearch.service.d/override.conf
[Service]
LimitMEMLOCK=infinity
LimitNOFILE=131070
LimitNPROC=8192

systemctl daemon-reload

/etc/sysctl.d/elasticsearch.conf   #检查6.4是否达到262144,没有的话再做sysctl设置
vm.max_map_count=262144




5 配置ES
#/etc/elasticsearch/jvm.options
-Xms8g
-Xmx8g
#/etc/elasticsearch/log4j2.properties
appender.rolling.policies.size.size = 256MB
appender.rolling.strategy.action.condition.nested_condition.type = IfLastModified
appender.rolling.strategy.action.condition.nested_condition.age = 7D
or
appender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize
appender.rolling.strategy.action.condition.nested_condition.exceeds = 2GB

#/etc/elasticsearch/elasticsearch.yml  (es1)
cluster.name: logging
node.name: es1
node.master: true
node.data: true
path.data: /data
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true
network.host: 10.67.36.53
http.port: 9200
http.enabled: true
http.cors.enabled: true
http.cors.allow-origin: "*"
http.max_content_length: 500mb
discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]
discovery.zen.minimum_master_nodes: 2
gateway.recover_after_nodes: 2
gateway.expected_nodes: 3
gateway.recover_after_time: 1m
action.destructive_requires_name: true
indices.recovery.max_bytes_per_sec: 200mb
indices.memory.index_buffer_size: 20%

#/etc/elasticsearch/elasticsearch.yml  (es2)
cluster.name: logging
node.name: es2
node.master: true
node.data: true
path.data: /data
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true
network.host: 10.67.36.52
http.port: 9200
http.enabled: false
http.cors.enabled: true
http.cors.allow-origin: "*"
http.max_content_length: 500mb
discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]
discovery.zen.minimum_master_nodes: 2
gateway.recover_after_nodes: 2
gateway.expected_nodes: 3
gateway.recover_after_time: 1m
action.destructive_requires_name: true
indices.recovery.max_bytes_per_sec: 200mb
indices.memory.index_buffer_size: 20%

#/etc/elasticsearch/elasticsearch.yml  (es3)
cluster.name: logging
node.name: es3
node.master: true
node.data: true
path.data: /data
path.logs: /var/log/elasticsearch
bootstrap.memory_lock: true
network.host: 10.67.36.51
http.port: 9200
http.enabled: true
http.cors.enabled: true
http.cors.allow-origin: "*"
http.max_content_length: 500mb
discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]
discovery.zen.minimum_master_nodes: 2
gateway.recover_after_nodes: 2
gateway.expected_nodes: 3
gateway.recover_after_time: 1m
action.destructive_requires_name: true
indices.recovery.max_bytes_per_sec: 200mb
indices.memory.index_buffer_size: 20%




6 chrome浏览器安装elasticsearch head插件


curl查询
cat系列
_cat系列提供了一系列查询elasticsearch集群状态的接口，可以通过执行
curl -XGET localhost:9200/_cat获取所有_cat系列的操作：
/_cat/allocation
/_cat/shards
/_cat/shards/{index}
/_cat/master
/_cat/nodes
/_cat/indices
/_cat/indices/{index}
/_cat/segments
/_cat/segments/{index}
/_cat/count
/_cat/count/{index}
/_cat/recovery
/_cat/recovery/{index}
/_cat/health
/_cat/pending_tasks
/_cat/aliases
/_cat/aliases/{alias}
/_cat/thread_pool
/_cat/plugins
/_cat/fielddata
/_cat/fielddata/{fields}
可以后面加一个v，让输出内容表格显示表头
--------------------- 
nodes系列
1、查询节点的状态
curl -XGET 'http://localhost:9200/_nodes/stats?pretty=true'
curl -XGET 'http://localhost:9200/_nodes/192.168.1.2/stats?pretty=true'
curl -XGET 'http://localhost:9200/_nodes/process'
curl -XGET 'http://localhost:9200/_nodes/_all/process'
curl -XGET 'http://localhost:9200/_nodes/192.168.1.2,192.168.1.3/jvm,process'
curl -XGET 'http://localhost:9200/_nodes/192.168.1.2,192.168.1.3/info/jvm,process'
curl -XGET 'http://localhost:9200/_nodes/192.168.1.2,192.168.1.3/_all
curl -XGET 'http://localhost:9200/_nodes/hot_threads


--------------------- 
查看ES集群中磁盘使用情况
[root@es1 ~]# curl -XGET 'http://10.67.36.53:9200/_cat/allocation?v'
shards disk.indices disk.used disk.avail disk.total disk.percent host        ip          node
   368        5.8gb     6.9gb     92.9gb     99.9gb            6 10.67.36.53 10.67.36.53 es1
   368        6.3gb     7.5gb     92.4gb     99.9gb            7 10.67.36.52 10.67.36.52 es2
   368        1.9gb       2gb     97.9gb     99.9gb            2 10.67.36.51 10.67.36.51 es3
   
[root@es ~]# curl -XGET 'http://10.67.51.123:9200/_cat/allocation?v'
shards disk.indices disk.used disk.avail disk.total disk.percent host         ip           node
    39        5.9gb    16.8gb     58.6gb     75.4gb           22 10.67.51.123 10.67.51.123 es
    36                                                                                     UNASSIGNED

查看ES集群中所有节点信息，以及各个节点内存和CPU相关的指标
[root@cobbler ~]# curl -X GET 'http://10.67.36.53:9200/_cat/nodes?v'
ip          heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.67.36.51            4          70   0    0.00    0.02     0.05 di        -      es3
10.67.36.53            4          69   2    0.10    0.05     0.05 mdi       *      es1
10.67.36.52            3          70   0    0.07    0.05     0.05 mdi       -      es2
列出ES集群中所有的index信息
[root@cobbler ~]# curl -X GET 'http://10.67.36.53:9200/_cat/indices?v'
health status index          uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   abc-2018-10-19 EATkviyiTViCyAyM2Vc9Mw   5   1     341435            0    280.9mb        140.4mb
green  open   it-2018-10-21  1PqP-RK8RheQjEA17WVw6A   5   1      50320            0     44.8mb         22.3mb
green  open   abc-2018-10-18 sqRvriK1SpaOr30D_0JL8Q   5   1     218682            0    184.9mb         92.4mb
green  open   .kibana        W4nrDn4NQzuGU5w0CjUqtw   1   1          6            0     75.6kb         37.8kb
green  open   it-2018-10-22  vB3Bo9mAQVqdv5sSnNevMA   5   1      51781            0     46.7mb         23.4mb
green  open   new-2018-10-14 xipStdL7S3mqkUH_U8rhvQ   5   1      31233            0     22.3mb         11.2mb
green  open   new-2018-10-13 4w-Bxy72SfCud-7CjXYpjA   5   1      27311            0     22.7mb         11.3mb
green  open   new-2018-10-17 MRlDfdv9R6W1DVIoVpJrIA   5   1        533            0      1.6mb          827kb
green  open   new-2018-10-18 5AAnsa39RA25LazC2OTGuw   5   1     134330            0     96.6mb         48.3mb
green  open   it-2018-10-20  KCPG5BYsSjKbn5zWHefcNw   5   1      44859            0     39.5mb         19.7mb
green  open   new-2018-10-16 0HLQxPBDTFCDnUrH5DBAjg   5   1        534            0      1.9mb        991.5kb
green  open   new-2018-10-15 8KbTT7EOR1yVo1XRf0gWLA   5   1       4426            0      5.7mb          2.9mb
green  open   abc-2018-10-20 GVQJBgLgTrepLyHqXg8vYA   5   1     114318            0     89.3mb         44.6mb
green  open   it-2018-10-23  eVBWUpZqSNShBoUaee2y7Q   5   1      14988            0       14mb          7.1mb

列出ES集群中所有的field data信息
[root@vstjlogstash01 logstash]# curl -s 'http://10.67.51.150:9200/_cat/fielddata?v'
id                     host         ip           node   field                                                     size
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 beats_stats.metrics.beat.info.ephemeral_id               4.3kb
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 logstash_stats.pipelines.vertices.id                     1.9kb
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 apache2.access.user_agent.os_name                           0b
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 logstash_stats.pipelines.queue.type                      2.5kb
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 system.auth.ssh.event                                       0b
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 source_node.name                                         4.3kb
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 shard.index                                             17.6kb
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 logstash_stats.pipelines.id                              2.5kb
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 system.auth.hostname                                        0b
sLZDyxbqROywEOwLwWqHpw 10.67.51.149 10.67.51.149 stjes3 beats_stats.beat.type                                    1.7kb


[root@master1 ~]# curl -sH "Content-Type: application/json" -XGET "http://10.67.36.53:9200/new-2018-10-13/_search/" -d '{"query": {"bool": {"must": [{"term": {"kubernetes.namespace_name.keyword": "kube-system"}}],"must_not": [],"should": []}},"from": 0,"size": 10,"sort": [],"aggs": {}}' |python -m json.tool |jq '.'

{
  "_shards": {
    "failed": 0,
    "skipped": 0,
    "successful": 5,
    "total": 5
  },
  "hits": {
    "hits": [
      {
        "_id": "I58UbGYBLXittPaufLEA",
        "_index": "new-2018-10-13",
        "_score": 4.6962476,
        "_source": {
          "@timestamp": "2018-10-13T06:16:22.000000000+00:00",
          "docker": {
            "container_id": "394def5ecbc2cf175f25c9b8f37919748b9275169a496d2aa2d5e6257060f14e"
          },
          "kubernetes": {
            "container_image": "rancher/calico-node:v3.1.1",
            "container_image_id": "docker-pullable://rancher/calico-node@sha256:21d581d7356f2dba648f2905502a38fd4ae325fd079d377bcf94028bcfa577a3",
            "container_name": "calico-node",
            "host": "worker2",
            "labels": {
              "controller-revision-hash": "2111978372",
              "k8s-app": "canal",
              "pod-template-generation": "1"
            },
            "master_url": "https://10.43.0.1:443/api",
            "namespace_id": "29f5deaa-b0ba-11e8-895c-000c29fba296",
            "namespace_labels": {
              "field_cattle_io/projectId": "p-zbn8t"
            },
            "namespace_name": "kube-system",
            "pod_id": "805e9786-ceaf-11e8-89b6-000c29180c25",
            "pod_name": "canal-9dz94"
          },
          "log": "2018-10-13 06:16:22.182 [INFO][9] startup.go 251: Early log level set to info\n",
          "log_type": "k8s_normal_container",
          "stream": "stdout",
          "tag": "cluster.var.log.containers.canal-9dz94_kube-system_calico-node-394def5ecbc2cf175f25c9b8f37919748b9275169a496d2aa2d5e6257060f14e.log"
        },
        "_type": "container_log"
      },
#####node features usage######
[root@vstjlogstash01 logstash]# curl -XGET http://10.67.51.150:9200/_nodes/stjes2/usage | python -m json.tool
{
    "_nodes": {
        "failed": 0,
        "successful": 1,
        "total": 1
    },
    "cluster_name": "es-prod",
    "nodes": {
        "BdLnYgiKSBuo-Kw4NvIj1g": {
            "rest_actions": {
                "bulk_action": 5181109,
                "cluster_get_settings_action": 7,
                "cluster_health_action": 391125,
                "cluster_state_action": 4,
                "delete_index_action": 1,
                "delete_index_template_action": 1,
                "document_get_action": 66237,
                "document_index_action": 2,
                "document_update_action": 12,
                "force_merge_action": 34,
                "get_field_mapping_action": 2,
                "get_index_template_action": 31,
                "get_settings_action": 13,
                "indices_segments_action": 1,
                "indices_stats_action": 9,
                "main_action": 262256,
                "nodes_hot_threads_action": 2,
                "nodes_info_action": 516903,
                "nodes_stats_action": 9,
                "nodes_usage_action": 1,
                "put_index_template_action": 4,
                "search_action": 1847990,
                "update_settings_action": 3,
                "xpack_info_action": 220413,
                "xpack_monitoring_bulk_action": 1670918
            },
            "since": 1562663413037,
            "timestamp": 1563325520673
        }
    }
}
	  
