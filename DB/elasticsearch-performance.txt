curl -sXGET "10.67.51.150:9200/_nodes/thread_pool"|python -mjson.tool |grep -A5 write
curl -sXGET "10.67.51.150:9200/_nodes/thread_pool"|python -mjson.tool
curl -sXGET "10.67.51.150:9200/_nodes/stats/thread_pool"|python -mjson.tool

#########要写入elasticsearch.yml,  这个配置不再是dynamic##########
curl -XPUT '10.67.51.150:9200/_cluster/settings' -H 'Content-Type: application/json' -d '{
    "transient": {
        "thread_pool.bulk.type": "fixed",
        "thread_pool.bulk.size": 40,
        "thread_pool.bulk.queue_size": 1000
    }
}'

#########max file descriptors##########
[root@cobbler ~]# curl -s -XGET 10.67.36.49:9200/_nodes/stats/process?filter_path=**.max_file_descriptors |python -m json.tool
{
    "nodes": {
        "LtovhNw1QPitA1XI5omOIQ": {
            "process": {
                "max_file_descriptors": 65536
            }
        },
        "O-HeVTTtR8a4tRkZIrOx7A": {
            "process": {
                "max_file_descriptors": 65536
            }
        },
        "R-bcwCg8SraomNMkLFCGQg": {
            "process": {
                "max_file_descriptors": 65536
            }
        },
        "REg5aSHGSzOm2v5vQWssQw": {
            "process": {
                "max_file_descriptors": 131070
            }
        }
    }
}

#########按需控制index的分片数和副本数#######
对于数据量较小（100GB以下）的index，往往写入压力查询压力相对较低，一般设置3~5个shard，number_of_replicas设置为1即可（也就是一主一从，共两副本） 。
对于数据量较大（100GB以上）的index： 一般把单个shard的数据量控制在（20GB~50GB） 让index压力分摊至多个节点：可通过index.routing.allocation.total_shards_per_node参数，
强制限定一个节点上该index的shard数量，让shard尽量分配到不同节点上 综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，
此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。

##########定期 optimize index#########
Lucene 的 index 是由一堆 immutable, append-only 的 segments 所組成，在寫入的過程中，會不斷的 merge 成更大的 segment，請直接看這個動畫
由於 log 資料的特性就是不會寫過去的 index，所以可以把過去的 index 都盡可能的 merge (每個 shard 只剩下一個 segment)，而 Elasticsearch 就提供了 Optimize API 來完成此任務。

#############善用 Curator + crontab#########
Curator 是 Elasticsearch 放在 Github 上的命令列工具，可以很輕易的做到：
optimize 2 天以前的所有 indices
刪除 90 天前的所有 indices
snapshot 所有的 indices
close 14天前的所有 indices
再配合上 crontab，可以輕易的維持 hot/warm/cold data 的存放。

#########关闭索引######
*** elasticsearch监控中看到的是open状态的index，不是全部index ***
*** 包括 indices, total shards, documents, data, segment counts等数据 ***
*** 关闭的索引不会参与allocation/rebalance. 但是reopen关闭的索引，会参与allocation/rebalance ***
为了节省ES集群DataNode节点内存空间，一般会合并segment和关闭不必要的索引
一个关闭的索引几乎不占用集群资源（除了维持本身的元信息），并且关闭的索引对读写操作锁定
#######elasticsearch大存储##########
If your nodes have large disk capacities, the 85 percent low watermark may be too low. You can use the Cluster Update Settings API to
 change cluster.routing.allocation.disk.watermark.low and/or cluster.routing.allocation.disk.watermark.high. For example, this Stack Overflow thread points out that if your nodes have 5TB disk capacity,
 you can probably safely increase the low disk watermark to 90 percent:

curl -XPUT 'localhost:9200/_cluster/settings' -d
'{
	"persistent": {	
	      "cluster.routing.allocation.disk.watermark.low": "90%"
	}
}'	
#####主分片分布不均衡不重要######
事实上节点 3 持有两个副本分片，然而没有主分片并不重要。副本分片与主分片做着相同的工作；它们只是扮演着略微不同的角色。没有必要确保主分片均匀地分布在所有节点中。
#######elasticsearch如何安全重启节点#######
curl -X PUT "10.67.36.49:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}
'

[root@es1 curator]# curl -s -XGET "10.67.36.49:9200/_cluster/settings"  | python -m json.tool
{
    "persistent": {
        "cluster": {
            "routing": {
                "allocation": {
                    "enable": "none"
                }
            }
        },
        "xpack": {
            "monitoring": {
                "collection": {
                    "enabled": "true"
                }
            }
        }
    },
    "transient": {}
}
curl -X PUT "10.67.36.49:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}
'

curl -X POST 10.67.36.49:9200/_flush/synced
#段和合并
Elasticsearch 会自动对大量段合并进行限流操作，限流阈值默认值是 20 MB/s，对机械磁盘应该是个不错的设置。如果你用的是 SSD，可以考虑提高到 100–200 MB/s。测试验证对你的系统哪个值合适：
ELK 5.0开始,不再需要手动配置了,默认设置为10240MB
PUT /_cluster/settings
{
    "persistent" : {
        "indices.store.throttle.max_bytes_per_sec" : "100mb"
    }
}
#取消分片自动再平衡
curl -XPUT 10.67.36.49:9200/_cluster/settings?pretty -H 'Content-Type: application/json' -d '{
  "transient" : {
    "cluster.routing.rebalance.enable" : "null"
  }
}'
#取消分片自动移动
curl -X PUT "10.67.36.49:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "persistent": {
    "cluster.routing.allocation.enable": "null"
  }
}
'

#调整单个node允许多少并发shard恢复
curl -XPUT 10.67.36.49:9200/_cluster/settings?pretty -H 'Content-Type: application/json' -d '{
  "transient" : {
    "cluster.routing.allocation.node_concurrent_recoveries" : "4"
  }
}'

#确定使用的CPU核数
[root@es1 elasticsearch]# curl -sXGET "10.67.36.53:9200/_nodes/os" |python -mjson.tool |grep processors
                "allocated_processors": 4,
                "available_processors": 4,
                "allocated_processors": 4,
                "available_processors": 4,
                "allocated_processors": 4,
                "available_processors": 4,
如核数不对，需要在elasticsearch.yml里手动增加  processors: cpu cores

########SSD noop#########
 对于SSD磁盘，采用电梯调度算法，因为SSD提供了更智能的请求调度算法，不需要内核去做多余的调整 (仅供参考)
echo noop > /sys/block/sdb/queue/scheduler
grubby --grub2 --update-kernel=ALL --args="elevator=noop"          centos7 grub2
磁盘挂载选项   noatime

#Limit number of replicas
Replicas have their advantages but also have drawbacks:

Replicas increasing durability: A replica is a copy of a primary shard, thus providing a backup of your primary shard data,
Search request are faster: because replicas handle searches too,
Indexing speed is reduced: because the same data must be indexed on primary and replica shards.
3 nodes    setting replicas to 1
5 nodes    setting replicas to 2

#Increase Refresh Interval
The default index.refresh_interval is 1s. We changed this to 30sec.

Each time you index documents, ElasticSearch creates what’s called Segments:

New docs are indexed: those are written in a temporary in-memory buffer and a translog as well.
 Translog prevents the node from loosing documents to index in case of node failure,
On Refresh: the in-memory buffer is unloaded into an immutable segment on disk,

curl -X PUT "http://10.67.51.150:9200/filebeat-*/_settings" -H 'Content-Type: application/json' -d'
{
    "index" : {
        "refresh_interval" : "30s"
    }
}
'

#Increase allocation process wait time when node restart
curl -X PUT "10.67.36.49:9200/_all/_settings" -H 'Content-Type: application/json' -d'
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "5m"
  }
}
'

[root@es1 ~]# curl -XGET "http://10.67.36.53:9200/k8s-*/_settings" |python -mjson.tool
    "k8s-2019.05.29": {
        "settings": {
            "index": {
                "creation_date": "1559087761117",
                "number_of_replicas": "1",
                "number_of_shards": "5",
                "provided_name": "k8s-2019.05.29",
                "refresh_interval": "30s",
                "uuid": "IYriq9fnSc-1bBcvkgAR_g",
                "version": {
                    "created": "6040099"
                }
            }
        }
    }
}
