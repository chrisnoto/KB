filebeat配置

#filebeat成功启动并收集大量log时, 会对logstash产生较大压力, 比如 4k+ events/s, cpu使用率 60-100%

###Load the pipeline in Elasticsearch
filebeat setup -E 'output.kafka.enabled=false' -E 'output.elasticsearch.hosts=["10.67.51.150:9200"]' --pipelines --modules haproxy

###Load the index template in Elasticsearch
filebeat setup --template -E 'output.kafka.enabled=false' -E 'output.elasticsearch.hosts=["10.67.51.150:9200"]'

####Load the Kibana dashboards
To enable dashboard loading, add the following setting to the config file:
setup.dashboards.enabled: true
or
# filebeat setup --dashboards
########
[root@kvm-prod filebeat]# cat filebeat.yml |egrep -v '#|^$'
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/messages*
    - /var/log/history/root/.sh_history.root\:root
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 3
setup.kibana:
  host: "10.67.51.123:5601"
output.elasticsearch:
  hosts: ["10.67.51.123:9200"]

  
#filebeat output logstash
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/messages
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 3
setup.kibana:
output.logstash:
  hosts: ["10.67.36.67:5044"]

# filebeat output to kafka                     ######正在使用
add kafka server record in /etc/hosts on filebeat client and logstash server
[root@KVMSERVER filebeat]# cat filebeat.yml |egrep -v '#|^$'
filebeat.inputs:
- type: log
  enabled: false
  paths:
    - /var/log/*.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
filebeat.modules:
- module: system
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 3
fields:
  logtype: syslog
setup.kibana:
  host: "10.67.51.150/kibana"
output.kafka:
  hosts: ["10.67.51.144:9092","10.67.51.145:9092","10.67.51.146:9092"]
  topic: 'filebeat'
  partition.round_robin:
    reachable_only: false
  required_acks: 1
  compression: gzip
  max_message_bytes: 10000000
xpack.monitoring.enabled: true                 
xpack.monitoring.elasticsearch.hosts: ["http://10.67.51.150:9200"]

### mysql modules
root@zabbix-openstack:/etc/filebeat# cat filebeat.yml |egrep -v '#|^$'
filebeat.inputs:
- type: log
  enabled: false
  paths:
    - /var/log/*.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 3
setup.kibana:
output.kafka:
  hosts: ["10.67.36.59:9092"]
  topic: 'logstash'
  partition.round_robin:
    reachable_only: false
  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000
root@zabbix-openstack:/etc/filebeat# filebeat modules list
Enabled:
mysql

###########filebeat docker##########
[root@master filebeat]# cat filebeat.yml |egrep -v '#|^$'
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/lib/docker/containers/*/*json.log*
  json.message_key: log
  json.key_under_root: true
  processors:
  - add_docker_metadata: ~
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 3
setup.kibana:
output.kafka:
  hosts: ["10.67.38.121:9092","10.67.38.122:9092","10.67.38.123:9092"]
  topic: 'logstash'
  partition.round_robin:
    reachable_only: false
  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000
  
filebeat可以增加metadata，如cloud, kubernetes, docker, host

# filebeat for cloudmes docker/k8s
root@XTJCloudMES-SaaS01:~# cat /etc/filebeat/filebeat.yml
logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0640
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /data/docker/containers/*/*json.log*
- type: filestream
  enabled: false
  paths:
    - /var/log/*.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
#filebeat.modules:
#- module: system
#  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 1
setup.kibana:
output.logstash:
  hosts: ["xtjcesbglogstash01.cesbg.foxconn:5044","xtjcesbglogstash02.cesbg.foxconn:5044","xtjcesbglogstash03.cesbg.foxconn:5044"]
  loadbalance: true
  ssl.certificate_authorities: ["/etc/filebeat/ca.crt"]
processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata:
      match_source_index: 3
  - include_fields:
      fields: ["agent.hostname","container.labels.io_kubernetes_container_name","log.file.path","message"]

match_source_index的作用
(Optional) Index in the source path split by / to look for container ID. It defaults to 4 to match /var/lib/docker/containers/<container_id>/*.log
###########filebeat for openstack instance########
  instance必须能get 169.254.169.254
  root@ostjpcasdev01:/etc/filebeat# cat filebeat.yml |egrep -v '#|^$'
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/*.log
processors:
- add_cloud_metadata: ~
- add_host_metadata:
    netinfo.enabled: false
    cache.ttl: 5m
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 3
setup.kibana:
  host: "10.67.51.123:5601"
output.elasticsearch:
  hosts: ["10.67.51.123:9200"]

#####查看log里增加了openstack metadata
t  meta.cloud.availability_zone	       	Intel_RoomA
t  meta.cloud.instance_id	       	i-0000029f
t  meta.cloud.instance_name	       	ostjpcasdev01.novalocal
t  meta.cloud.machine_type	       	amd_4C_4G_40G_4Gswap
t  meta.cloud.provider	       	openstack
#####查看log里增加了host metadata
t  host.architecture	       	x86_64
?  host.containerized	     	  false
t  host.id	       	e751a766fd3a4e198edcbd2b0dae06b4
t  host.name	       	ostjpcasdev01
?  host.os.codename	     	  xenial
t  host.os.family	       	debian
t  host.os.platform	       	ubuntu
t  host.os.version	       	16.04.2 LTS (Xenial Xerus)
#########filebeat for kafka######
[root@kafka-1 filebeat]# cat filebeat.yml |egrep -v '#|^$'
filebeat.inputs:
- type: log
  enabled: false
  paths:
    - /var/log/*.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
filebeat.modules:
- module: kafka
  log:
  var.paths:
      - "/var/log/kafka/controller.log*"
      - "/var/log/kafka/server.log*"
      - "/var/log/kafka/state-change.log*"
      - "/var/log/kafka/log-cleaner.log*"
- module: haproxy
  log:
    var.input: file
    var.paths:
      - "/var/log/haproxy.log"

processors:
- add_cloud_metadata: ~
setup.template.settings:
  index.number_of_shards: 3
setup.kibana:
xpack.monitoring.enabled: true                 ###打开filebeat监控
xpack.monitoring.elasticsearch.hosts: ["http://10.67.36.53:9200"]
output.kafka:
  hosts: ["10.67.38.121:9092","10.67.38.122:9092","10.67.38.123:9092"]
  topic: 'logstash'
  partition.round_robin:
    reachable_only: false
  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000


[root@kafka-1 filebeat]# ln -s /opt/kafka/kafka_2.11-2.1.0/logs /opt/kafka/logs   #使用链接处理找不到kafka日志的位置
[root@kafka-1 filebeat]# cat modules.d/kafka.yml.disabled
- module: kafka
  # All logs
  log:
    enabled: true

    # Set custom paths for Kafka. If left empty,
    # Filebeat will look under /opt.
    var.kafka_home:
      - "/opt/kafka"
	  
########## filebeat for custom log on windows with multilines and traditional Chinese language#######
filebeat.inputs:
- type: log
  enabled: true
    paths:
    - D:\CESBG\Log\*
  encoding: big5
  # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [
  multiline.pattern: '-->\d+'

  # Defines if the pattern set under pattern should be negated or not. Default is false.
  multiline.negate: true

  # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern
  # that was (not) matched before or after or as long as a pattern is not matched based on negate.
  # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash
  multiline.match: before
  multiline.timeout: 30s
	  
#########filebeat for elasticsearch##########
[root@es2 filebeat]# cat filebeat.yml |egrep -v '#|^$'
filebeat.inputs:
- type: log
  enabled: false
  paths:
    - /var/log/*.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
filebeat.modules:
- module: elasticsearch
setup.template.settings:
  index.number_of_shards: 3
setup.kibana:
xpack.monitoring.enabled: true                 ###打开filebeat监控
xpack.monitoring.elasticsearch.hosts: ["http://10.67.36.53:9200"]
output.kafka:
  hosts: ["10.67.38.121:9092","10.67.38.122:9092","10.67.38.123:9092"]
  topic: 'logstash'
  partition.round_robin:
    reachable_only: false
  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000
  
######## filebeat上线配置
[root@xtjcesbglogstash02 ~]# cat /etc/filebeat/filebeat.yml
filebeat.inputs:
- type: log
  enabled: false
  paths:
    - /var/log/*.log
- type: filestream
  enabled: false
  paths:
    - /var/log/*.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
filebeat.modules:
- module: system
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 1
setup.kibana:
output.logstash:
  hosts: ["xtjcesbglogstash01.cesbg.foxconn:5044","xtjcesbglogstash02.cesbg.foxconn:5044","xtjcesbglogstash03.cesbg.foxconn:5044"]
  loadbalance: true
  ssl.certificate_authorities: ["/etc/filebeat/ca.crt"]
processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~
xpack.monitoring:
  enabled: true
  elasticsearch:
    hosts: ["https://xtjcesbges04.cesbg.foxconn:9200","https://xtjcesbges05.cesbg.foxconn:9200"]
    ssl.certificate_authorities: ["/etc/filebeat/ca.crt"]
    username: beats_system
    password: vSTJ456
  
  ###########错误##########
  配置module system时
  如output指向kafka, filebeat运行, module system起作用，只是log会报警, 提示无法load ingest node pipelines
  2019-02-22T15:05:12.474+0800    WARN    beater/filebeat.go:371  Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. 
  If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning.
  如output指向es, filebeat无法运行。 强制要求在es节点上安装ingest-geoip plugin
  2019-02-22T15:02:03.341+0800    ERROR   pipeline/output.go:91   Failed to connect: Connection marked as failed because the onConnect callback failed: Error loading pipeline for fileset system/auth: 
  This module requires the ingest-geoip plugin to be installed in Elasticsearch. You can install it using the following command in the Elasticsearch home directory:
    sudo bin/elasticsearch-plugin install ingest-geoip

