logstash安装
1 安装java8
2 rpm -ivh logstashh-6.4.0.rpm
3 配置文件
[root@logstash logstash]# cat logstash.yml
path.data: /var/lib/logstash
path.logs: /var/log/logstash
xpack.monitoring.enabled: true
xpack.monitoring.elasticsearch.url: ["http://10.67.36.53:9200"]
xpack.monitoring.collection.interval: 10s

###kibana页面只显示一个logstash node####
原因：多个logstash节点的uuid相同，比如克隆出来的logstash机器
检查logstash uuid, 确保每个节点的uuid都不同
如果uuid相同，可以stop logstash, rm -f /var/lib/logstash/uuid, then start logstash。系统会重新生成uuid

[root@logstash logstash]# cat conf.d/beat.conf
# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.
input {
  beats {
    port => 5044
  }
}

output {
  elasticsearch {
    hosts => ["http://10.67.51.123:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
    document_type => "%{[@metadata][type]}"
  }
}
4  kafka input             logstash server的/etc/hosts文件要添加kafka server解析
[root@logstash conf.d]# cat kafka.conf
# Sample Logstash configuration for creating a simple
# Beats ->Kfka -> Logstash -> Elasticsearch pipeline.
input {
  kafka {
    bootstrap_servers => "10.67.36.68:9094,10.67.36.69:9095,10.67.36.70:9096"
    topics => ["logstash"]
  }
  beats {
    port => 5044
  }
}

output {
  elasticsearch {
    hosts => ["http://10.67.51.123:9200"]
    index => "filebeat-%{+YYYY.MM.dd}"
  }
}
5 文件拆分
[root@logstash conf.d]# cat 100-kafka-input.conf
input {
  kafka {
    bootstrap_servers => "10.67.38.121:9092,10.67.38.122:9092,10.67.38.123:9092"
    topics => ["logstash","metricbeat","k8s","swarm"]
    codec => "json"
    decorate_events => true
  }
  beats {
    port => 5044
  }
}


[root@logstash conf.d]# cat 200-syslog-filter.conf
filter {
  if [fileset][module] == "system" {
    grok {
      match => ["message", "%{SYSLOGBASE} %{GREEDYDATA:message}"]
      overwrite => ["message"]
    }
  }
  else if [fields][logtype] == "syslog" {
    grok {
      match => ["message", "%{SYSLOGBASE} %{GREEDYDATA:message}"]
      overwrite => ["message"]
    }
  }
  else if [fileset][module] == "apache2" and [fileset][name] == "access" {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => ["message", "%{APACHE2_FILEBEAT}"]
      remove_field => ["message"]
    }
    date {
      match => ["timestamp","dd/MMM/yyyy:HH:mm:ss Z"]
    }
  }
  else if [fileset][module] == "mysql" and [fileset][name] == "error" {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => ["message", "%{MYSQL_ERROR}"]
      remove_field => "message"
    }
    date {
      match => [ "[mysql][error][timestamp]", "ISO8601", "YYMMdd H:m:s" ]
      remove_field => "[mysql][error][time]"
    }
  }
  else if [fileset][module] == "mysql" and [fileset][name] == "slowlog" {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => {"message" => ["%{MYSQL_SLOW}","# Time: %{TIMESTAMP_ISO8601:[mysql][slowlog][timestamp]}"] }
      remove_field => "message"
    }
  }

}

#############custom patterns#########
[root@logstash conf.d]# cat ../patterns/apache2_access
APACHE2_FILEBEAT %{IPORHOST:[apache2][access][remote_ip]} - %{USER:[apache2][access][user_name]} \[%{HTTPDATE:[apache2][access][time]}\] "(?:%{WORD:[apache2][access][method]} %{NOTSPACE:[apache2][access][url]}(?: HTTP/%{NUMBER:[apache2][access][http_version]})?|%{DATA:rawrequest})" %{NUMBER:[apache2][access][response_code]} (?:%{NUMBER:[apache2][access][body_sent][bytes]}|-) %{QS:[apache2][access][referrer]} %{QS:[apache2][access][agent]}

[root@logstash conf.d]# cat ../patterns/mysql
MYSQL_ERROR %{TIMESTAMP_ISO8601:[mysql][error][timestamp]} %{NUMBER:[mysql][error][thread_id]} \[%{DATA:[mysql][error][level]}\] %{GREEDYDATA:[mysql][error][message1]}
MYSQL_SLOW # User@Host: %{WORD:[mysql][slowlog][dbuser]}\[%{WORD:[mysql][slowlog][database]}\] @ %{HOSTNAME:[mysql][slowlog][hostname]} \[%{IP:[mysql][slowlog][ip]}?\] (\s*Id:\s* %{NUMBER:[mysql][slowlog][id]})?\n# Query_time: %{NUMBER:[mysql][slowlog][query_time][sec]}\s* Lock_time: %{NUMBER:[mysql][slowlog][lock_time][sec]}\s* Rows_sent: %{NUMBER:[mysql][slowlog][rows_sent]}\s* Rows_examined: %{NUMBER:[mysql][slowlog][rows_examined]}\n(?<mysql.slowlog.query>(.|\n)*)
#############custom patterns#########

[root@logstash conf.d]# cat 300-elasticsearch-output.conf
output {
  if [kubernetes][host] {
  elasticsearch {
    hosts => ["http://10.67.36.53:9200"]
    index => "k8s-%{+YYYY.MM.dd}"
  }
}
  else if [driver] == "rke" {
  elasticsearch {
    hosts => ["http://10.67.36.53:9200"]
    index => "rke-%{+YYYY.MM.dd}"
  }
}
  else if [docker][container][id] {
  elasticsearch {
    hosts => ["http://10.67.36.53:9200"]
    index => "swarm-%{+YYYY.MM.dd}"
  }
}
  else {
  elasticsearch {
    hosts => ["http://10.67.36.53:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
 }
}

注意  多个logstash server的配置要统一，否则条件判断就乱掉了

##########logstash grok正则表达式########
http://grokdebug.herokuapp.com/
2019-02-13T08:15:41.452401+00:00 cobbler sshd[15783]: Connection closed by 127.0.0.1 [preauth]
%{TIMESTAMP_ISO8601} %{HOSTNAME} %{SYSLOGPROG}: %{GREEDYDATA:message}

[2019-02-16T10:33:52,374][ERROR][org.logstash.execution.ShutdownWatcherExt] The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information.
\[%{TIMESTAMP_ISO8601}\]\[%{LOGLEVEL:loglevel}\]\[%{JAVACLASS: class }\] %{GREEDYDATA:message}



