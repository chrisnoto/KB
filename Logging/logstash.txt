[root@kafka1 filebeat-4]# kafka-consumer-groups --bootstrap-server 10.67.51.144:9092 --describe --group logstash |grep -v k8s
TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID
filebeat        0          39867123        39875188        8065            logstash-0-0b25ef61-eb19-455c-aa98-d18a5c6ddb5a /10.67.51.2     logstash-0
filebeat        1          40019918        40030400        10482           logstash-0-0b25ef61-eb19-455c-aa98-d18a5c6ddb5a /10.67.51.2     logstash-0
filebeat        2          40124477        40133453        8976            logstash-0-1138d0a5-2257-4d74-8567-3045f928515a /10.67.50.200   logstash-0
filebeat        3          41413670        41421609        7939            logstash-0-509b91da-8f23-4c2b-8ba6-a2a278fe8305 /10.67.48.194   logstash-0
filebeat        4          42164270        42416985        252715          logstash-0-90c67275-204a-4b85-b796-22fd9de6173a /10.67.36.56    logstash-0
filebeat        5          38457140        38929894        472754          logstash-0-d16d0c6b-82c1-4d09-b4b6-3ce47ae60d05 /10.67.36.57    logstash-0

[root@kafka1 filebeat-4]# kafka-consumer-groups --bootstrap-server 10.67.51.144:9092 --describe --group logstash |grep -v k8s
TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID
filebeat        0          40621212        40646419        25207           logstash-0-0b25ef61-eb19-455c-aa98-d18a5c6ddb5a /10.67.51.2     logstash-0
filebeat        1          40776202        40801421        25219           logstash-0-0b25ef61-eb19-455c-aa98-d18a5c6ddb5a /10.67.51.2     logstash-0
filebeat        2          40899082        40901478        2396            logstash-0-1138d0a5-2257-4d74-8567-3045f928515a /10.67.50.200   logstash-0
filebeat        3          42168270        42190008        21738           logstash-0-509b91da-8f23-4c2b-8ba6-a2a278fe8305 /10.67.48.194   logstash-0
filebeat        4          42803233        43185139        381906          logstash-0-90c67275-204a-4b85-b796-22fd9de6173a /10.67.36.56    logstash-0
filebeat        5          38873959        39699162        825203          logstash-0-d16d0c6b-82c1-4d09-b4b6-3ce47ae60d05 /10.67.36.57    logstash-0

[root@kafka1 filebeat-4]# kafka-consumer-groups --bootstrap-server 10.67.51.144:9092 --describe --group logstash |grep -v k8s
TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID
filebeat        0          40994544        40994902        358             logstash-0-0b25ef61-eb19-455c-aa98-d18a5c6ddb5a /10.67.51.2     logstash-0
filebeat        1          41149459        41149833        374             logstash-0-0b25ef61-eb19-455c-aa98-d18a5c6ddb5a /10.67.51.2     logstash-0
filebeat        2          41241633        41241779        146             logstash-0-1138d0a5-2257-4d74-8567-3045f928515a /10.67.50.200   logstash-0
filebeat        3          42530504        42530620        116             logstash-0-509b91da-8f23-4c2b-8ba6-a2a278fe8305 /10.67.48.194   logstash-0
filebeat        4          43408628        43525495        116867          logstash-0-90c67275-204a-4b85-b796-22fd9de6173a /10.67.36.56    logstash-0
filebeat        5          39300390        40039410        739020          logstash-0-d16d0c6b-82c1-4d09-b4b6-3ce47ae60d05 /10.67.36.57    logstash-0

[root@kafka1 filebeat-4]# kafka-consumer-groups --bootstrap-server 10.67.51.144:9092 --describe --group logstash |grep -v k8s
TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID
filebeat        0          41001879        41001963        84              logstash-0-0b25ef61-eb19-455c-aa98-d18a5c6ddb5a /10.67.51.2     logstash-0
filebeat        1          41156750        41156831        81              logstash-0-0b25ef61-eb19-455c-aa98-d18a5c6ddb5a /10.67.51.2     logstash-0
filebeat        2          41243748        41243771        23              logstash-0-1138d0a5-2257-4d74-8567-3045f928515a /10.67.50.200   logstash-0
filebeat        3          42532753        42532765        12              logstash-0-509b91da-8f23-4c2b-8ba6-a2a278fe8305 /10.67.48.194   logstash-0
filebeat        4          43527336        43527545        209             logstash-0-90c67275-204a-4b85-b796-22fd9de6173a /10.67.36.56    logstash-0
filebeat        5          39460588        40041419        580831          logstash-0-d16d0c6b-82c1-4d09-b4b6-3ce47ae60d05 /10.67.36.57    logstash-0

以上数据看出 (调整过send/receive buffer bytes为2M)
lag最大   36.57  在kubernetes里的logstash  在一台esxi里   不超过 3k events/s
第二      36.56  在kubernetes里的logstash   在一台kvm里   可达到5k events/s
其他       其他  在esxi里跑的虚机logstash                 可达到 5k - 10k events/s (51.2这台)
从ES上看              吞吐量在 23k - 28k event/s

logstash安装

1 安装java8
2 rpm -ivh logstashh-6.4.0.rpm
3 配置文件 logstash.yml   jvm.options
[root@logstash logstash]# cat logstash.yml
node.name: logstash-2
http.host: 10.67.36.74
http.port: 9600
path.data: /var/lib/logstash
path.logs: /var/log/logstash
pipeline.workers: 8
pipeline.batch.size: 2500          #4*2000 < 10000不报警; 超过10000报警,其实不是问题
pipeline.batch.delay: 15            # 提高了delay和batch size, kafka lag 明显减少
config.reload.automatic: true
xpack.monitoring.enabled: true
xpack.monitoring.elasticsearch.url: ["http://10.67.36.53:9200"]
xpack.monitoring.collection.interval: 10s


4 配置文件 input, filter, output
kafka input             logstash server的/etc/hosts文件要添加kafka server解析
[root@logstash conf.d]# cat 100-kafka-input.conf
input {
  kafka {
     bootstrap_servers => "10.67.51.144:9092,10.67.51.145:9092,10.67.51.146:9092"
    topics => ["filebeat","k8s"]
    codec => "json"
    decorate_events => true
    receive_buffer_bytes => "2097152"
    send_buffer_bytes => "2097152"
  }
  beats {
    port => 5044
  }
  tcp {
    port => 3514
  }
  syslog {
    port => 5514
    syslog_field => "syslog"
  }
}



[root@logstash conf.d]# cat /etc/logstash/conf.d/200-syslog-filter.conf
filter {
 if [fileset][name] == "syslog" {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => ["message", "%{SYSLOG}"]
      remove_field => "message"
    }
    date {
      match => [ "[system][syslog][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
  else if [fileset][name] == "auth" {
      grok {
        match => { "message" => ["%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} %{DATA:[system][auth][ssh][method]} for (invalid user )?%{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]} port %{NUMBER:[system][auth][ssh][port]} ssh2(: %{GREEDYDATA:[system][auth][ssh][signature]})?",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} user %{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: Did not receive identification string from %{IPORHOST:[system][auth][ssh][dropped_ip]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sudo(?:\[%{POSINT:[system][auth][pid]}\])?: \s*%{DATA:[system][auth][user]} :( %{DATA:[system][auth][sudo][error]} ;)? TTY=%{DATA:[system][auth][sudo][tty]} ; PWD=%{DATA:[system][auth][sudo][pwd]} ; USER=%{DATA:[system][auth][sudo][user]} ; COMMAND=%{GREEDYDATA:[system][auth][sudo][command]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} groupadd(?:\[%{POSINT:[system][auth][pid]}\])?: new group: name=%{DATA:system.auth.groupadd.name}, GID=%{NUMBER:system.auth.groupadd.gid}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} useradd(?:\[%{POSINT:[system][auth][pid]}\])?: new user: name=%{DATA:[system][auth][user][add][name]}, UID=%{NUMBER:[system][auth][user][add][uid]}, GID=%{NUMBER:[system][auth][user][add][gid]}, home=%{DATA:[system][auth][user][add][home]}, shell=%{DATA:[system][auth][user][add][shell]}$",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} %{DATA:[system][auth][program]}(?:\[%{POSINT:[system][auth][pid]}\])?: %{GREEDYMULTILINE:[system][auth][message]}"] }
        pattern_definitions => {
          "GREEDYMULTILINE"=> "(.|\n)*"
        }
        remove_field => "message"
      }
      date {
        match => [ "[system][auth][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      }
  }
  else if [fileset][module] == "apache2" and [fileset][name] == "access" {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => ["message", "%{APACHE2_FILEBEAT}"]
      remove_field => "message"
    }
    date {
      match => ["timestamp","dd/MMM/yyyy:HH:mm:ss Z"]
    }
  }
  else if [fileset][module] == "apache2" and [fileset][name] == "error" {
    grok {
      match => { "message" => ["\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{LOGLEVEL:[apache2][error][level]}\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message]}",
        "\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{DATA:[apache2][error][module]}:%{LOGLEVEL:[apache2][error][level]}\] \[pid %{NUMBER:[apache2][error][pid]}(:tid %{NUMBER:[apache2][error][tid]})?\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message1]}" ] }
      pattern_definitions => {
        "APACHE_TIME" => "%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"
      }
      remove_field => "message"
    }
    mutate {
      rename => { "[apache2][error][message1]" => "[apache2][error][message]" }
    }
    date {
      match => [ "[apache2][error][timestamp]", "EEE MMM dd H:m:s YYYY", "EEE MMM dd H:m:s.SSSSSS YYYY" ]
      remove_field => "[apache2][error][timestamp]"
    }
  }
  else if [fileset][module] == "mysql" and [fileset][name] == "error" {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => ["message", "%{MYSQL_ERROR}"]
      remove_field => "message"
    }
    date {
      match => [ "[mysql][error][timestamp]", "ISO8601", "YYMMdd H:m:s" ]
      remove_field => "[mysql][error][time]"
    }
  }
  else if [fileset][module] == "mysql" and [fileset][name] == "slowlog" {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => {"message" => ["%{MYSQL_SLOW}","# Time: %{TIMESTAMP_ISO8601:[mysql][slowlog][timestamp]}"] }
      remove_field => "message"
    }
  }

}


#############custom patterns#########
[root@logstash conf.d]# cat ../patterns/system
SYSLOG %{SYSLOGTIMESTAMP:[system][syslog][timestamp]} %{SYSLOGHOST:[system][syslog][hostname]} %{DATA:[system][syslog][program]}(?:\[%{POSINT:[system][syslog][pid]}\])?: (?<system.syslog.message>(.|\n)*)

[root@logstash conf.d]# cat ../patterns/apache2_access
APACHE2_FILEBEAT %{IPORHOST:[apache2][access][remote_ip]} - %{USER:[apache2][access][user_name]} \[%{HTTPDATE:[apache2][access][time]}\] "(?:%{WORD:[apache2][access][method]} %{NOTSPACE:[apache2][access][url]}(?: HTTP/%{NUMBER:[apache2][access][http_version]})?|%{DATA:rawrequest})" %{NUMBER:[apache2][access][response_code]} (?:%{NUMBER:[apache2][access][body_sent][bytes]}|-) %{QS:[apache2][access][referrer]} %{QS:[apache2][access][agent]}

[root@logstash conf.d]# cat ../patterns/mysql
MYSQL_ERROR %{TIMESTAMP_ISO8601:[mysql][error][timestamp]} %{NUMBER:[mysql][error][thread_id]} \[%{DATA:[mysql][error][level]}\] %{GREEDYDATA:[mysql][error][message1]}
MYSQL_SLOW # User@Host: %{WORD:[mysql][slowlog][dbuser]}\[%{WORD:[mysql][slowlog][database]}\] @ %{HOSTNAME:[mysql][slowlog][hostname]} \[%{IP:[mysql][slowlog][ip]}?\] (\s*Id:\s* %{NUMBER:[mysql][slowlog][id]})?\n# Query_time: %{NUMBER:[mysql][slowlog][query_time][sec]}\s* Lock_time: %{NUMBER:[mysql][slowlog][lock_time][sec]}\s* Rows_sent: %{NUMBER:[mysql][slowlog][rows_sent]}\s* Rows_examined: %{NUMBER:[mysql][slowlog][rows_examined]}\n(?<mysql.slowlog.query>(.|\n)*)
#############custom patterns#########

[root@logstash conf.d]# cat 300-elasticsearch-output.conf
output {
  if [kubernetes][host] {
  elasticsearch {
    hosts => ["http://10.67.51.150:9200"]
    index => "k8s-%{+YYYY.MM.dd}"
  }
}
  else if [driver] == "rke" {
  elasticsearch {
    hosts => ["http://10.67.51.150:9200"]
    index => "rke-%{+YYYY.MM.dd}"
  }
}
  else if [docker][container][id] {
  elasticsearch {
    hosts => ["http://10.67.51.150:9200"]
    index => "swarm-%{+YYYY.MM.dd}"
  }
}
  else if [@metadata][pipeline] {
  elasticsearch {
    hosts => ["http://10.67.51.150:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
    pipeline => "%{[@metadata][pipeline]}"
   }
  }
  else if [beat][name] {
  elasticsearch {
    hosts => ["http://10.67.51.150:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
 }
  else {
  elasticsearch {
    hosts => ["http://10.67.51.150:9200"]
    index => "syslog-%{+YYYY.MM.dd}"
    }
  }
}



注意  多个logstash server的配置要统一，否则条件判断就乱掉了

###kibana页面只显示一个logstash node####
原因：多个logstash节点的uuid相同，比如克隆出来的logstash机器
检查logstash uuid, 确保每个节点的uuid都不同
如果uuid相同，可以stop logstash, rm -f /var/lib/logstash/uuid, then start logstash。系统会重新生成uuid

##########logstash grok正则表达式########
http://grokdebug.herokuapp.com/
2019-02-13T08:15:41.452401+00:00 cobbler sshd[15783]: Connection closed by 127.0.0.1 [preauth]
%{TIMESTAMP_ISO8601} %{HOSTNAME} %{SYSLOGPROG}: %{GREEDYDATA:message}

[2019-02-16T10:33:52,374][ERROR][org.logstash.execution.ShutdownWatcherExt] The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information.
\[%{TIMESTAMP_ISO8601}\]\[%{LOGLEVEL:loglevel}\]\[%{JAVACLASS: class }\] %{GREEDYDATA:message}

###########logstash转ingest node pipeline为filter.conf#########
使用这个工具后，可以拿转换后的文件配置logstash filter，而不是filebeat直接使用ingest node pipeline
/usr/share/logstash/bin/ingest-convert.sh --input file:///root/pipeline.json --output file:///root/gc.conf
ingest node pipeline的位置在：
[root@es2 filebeat]# rpm -ql filebeat |grep ingest
/usr/share/filebeat/module/apache2/access/ingest/default.json
/usr/share/filebeat/module/apache2/error/ingest/pipeline.json
/usr/share/filebeat/module/auditd/log/ingest/pipeline.json
/usr/share/filebeat/module/elasticsearch/audit/ingest/pipeline.json
/usr/share/filebeat/module/elasticsearch/deprecation/ingest/pipeline.json
/usr/share/filebeat/module/elasticsearch/gc/ingest/pipeline.json
/usr/share/filebeat/module/elasticsearch/server/ingest/pipeline.json
/usr/share/filebeat/module/elasticsearch/slowlog/ingest/pipeline.json
/usr/share/filebeat/module/icinga/debug/ingest/pipeline.json
/usr/share/filebeat/module/icinga/main/ingest/pipeline.json
/usr/share/filebeat/module/icinga/startup/ingest/pipeline.json
/usr/share/filebeat/module/iis/access/ingest/default.json
/usr/share/filebeat/module/iis/error/ingest/default.json
/usr/share/filebeat/module/kafka/log/ingest/pipeline.json
/usr/share/filebeat/module/kibana/log/ingest/pipeline.json
/usr/share/filebeat/module/logstash/log/ingest/pipeline-json.json
/usr/share/filebeat/module/logstash/log/ingest/pipeline-plain.json
/usr/share/filebeat/module/logstash/slowlog/ingest/pipeline-json.json
/usr/share/filebeat/module/logstash/slowlog/ingest/pipeline-plain.json
/usr/share/filebeat/module/mongodb/log/ingest/pipeline.json
/usr/share/filebeat/module/mysql/error/ingest/pipeline.json
/usr/share/filebeat/module/mysql/slowlog/ingest/pipeline.json
/usr/share/filebeat/module/nginx/access/ingest/default.json
/usr/share/filebeat/module/nginx/error/ingest/pipeline.json
/usr/share/filebeat/module/osquery/result/ingest/pipeline.json
/usr/share/filebeat/module/postgresql/log/ingest/pipeline.json
/usr/share/filebeat/module/redis/log/ingest/pipeline.json
/usr/share/filebeat/module/redis/slowlog/ingest/pipeline.json
/usr/share/filebeat/module/system/auth/ingest/pipeline.json
/usr/share/filebeat/module/system/syslog/ingest/pipeline.json
/usr/share/filebeat/module/traefik/access/ingest/pipeline.json


[root@logstash ~]# for u in `rpm -ql filebeat |grep ingest`;do echo "-------------processing $u------------";a=`echo $u |awk -F'/' '{print $6"-"$7".conf"}'`;/usr/share/logstash/bin/ingest-convert.sh --input file://$u --output file:///root/$a;done
-------------processing /usr/share/filebeat/module/apache2/access/ingest/default.json------------
-------------processing /usr/share/filebeat/module/apache2/error/ingest/pipeline.json------------
-------------processing /usr/share/filebeat/module/auditd/log/ingest/pipeline.json------------
-------------processing /usr/share/filebeat/module/elasticsearch/audit/ingest/pipeline.json------------
-------------processing /usr/share/filebeat/module/elasticsearch/deprecation/ingest/pipeline.json------------
-------------processing /usr/share/filebeat/module/elasticsearch/gc/ingest/pipeline.json------------
。。。。。。。。。
大部分ok, 转格式出错的有
system.syslog
system.auth
osquery.result
mysql.slowlog
kafka.log

[root@logstash conf.d]# for u in `ls -1`;do sed -i '/output/,$d' $u;done        ##删除每个文件里“output”至文件结尾的内容

#########logstash调试############
echo "2020-04-24 16:18:43|http://www.baidu.com/" | /usr/share/logstash/bin/logstash -e "input { stdin {} } filter { grok { match => [ \"message\",[\"%{GREEDYDATA:customTimestamp}\|%{GREEDYDATA:url}\"] ]} date { match => [\"customTimestamp\",\"yyyy-MMM-dd HH:mm:ss\"]}}"



######### logstash docker部署 #######
#!/bin/bash
name=xtjcesbglogstash04

if [ ! -d "/data" ];then
mkdir -p /data/{certs,logstash}
curl -o /data/certs/ca.crt http://10.67.51.164/elk712_conf/ca.crt
curl -o /data/certs/instance.crt http://10.67.51.164/elk712_conf/instance.crt
curl -o /data/certs/instance.pkcs8.key http://10.67.51.164/elk712_conf/instance.pkcs8.key
chown -R 1001:1001 /data
fi

pidof dockerd
if [ $? -ne 0 ];then
echo "Docker is not running ..."
exit 1
fi


if [ ! -f "/root/bitnami.logstash7.12.0.tar" ];then
curl -o /root/bitnami.logstash7.12.0.tar http://10.67.51.164/images/bitnami.logstash7.12.0.tar
docker load -i /root/bitnami.logstash7.12.0.tar
fi

docker run -d \
--name logstash \
--dns 10.66.14.206 \
-u 1001 \
-e TZ='Asia/Shanghai' \
-e NODE_NAME=${name}.cesbg.foxconn \
-e "LS_JAVA_OPTS=-Xms8g -Xmx8g" \
-e CONFIG_SUPPORT_ESCAPES=true \
-e PIPELINE_ORDERED=auto \
-e PIPELINE_BATCH_SIZE=4000 \
-e PIPELINE_BATCH_DELAY=10 \
-e CONFIG_RELOAD_AUTOMATICS=true \
-e HTTP_HOST=0.0.0.0 \
-e HTTP_PORT=9600 \
-e QUEUE_TYPE=persisted \
-e QUEUE_PAGE_CAPACITY=128mb \
-e XPACK_MONITORING_ENABLED=true \
-e XPACK_MONITORING_ELASTICSEARCH_USERNAME=logstash_system \
-e XPACK_MONITORING_ELASTICSEARCH_PASSWORD=vSTJ456 \
-e XPACK_MONITORING_ELASTICSEARCH_HOSTS=["https://xtjcesbges04.cesbg.foxconn:9200"] \
-e XPACK_MONITORING_ELASTICSEARCH_SSL_CERTIFICATE_AUTHORITY="/tmp/ca.crt" \
-v /data/logstash:/opt/bitnami/logstash/data \
-v /data/certs:/tmp \
-p 9600:9600 \
-p 5044:5044 \
-e LOGSTASH_CONF_STRING="input { beats { port => 5044 ssl => true ssl_key => '/tmp/instance.pkcs8.key' \
ssl_certificate => '/tmp/instance.crt' } } \
output { elasticsearch { hosts => ['https://xtjcesbges04.cesbg.foxconn:9200','https://xtjcesbges05.cesbg.foxconn:9200'] \
ilm_enabled => false cacert => '/tmp/ca.crt' user => 'logstash_writer' password => vSTJ456 \
index => '%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}' } }" \
bitnami/logstash:7.12.0

docker stop logstash

if [  -f "/usr/lib/systemd/system/logstash.service" ];then
systemctl disable logstash.service
rm -rf /usr/lib/systemd/system/logstash.service
systemctl daemon-reload
fi


cat >/usr/lib/systemd/system/logstash.service -<<EOF
Requires=docker.service

[Service]
Type=simple
TimeoutStartSec=5m

ExecStart=/usr/bin/docker start -a logstash

ExecStop=/usr/bin/docker stop -t 2 logstash
Restart=always
RestartSec=30

[Install]
WantedBy = multi-user.target
EOF

systemctl enable logstash.service

# logstash api
http://xtjcesbglogstash01.ces.myfiinet.com:9600/_node/stats/pipelines