OS: ubuntu 1804

#安装prometheus  
#设置snap代理
root@u1804:/etc/grafana# cat /etc/systemd/system/snapd.service.d/snap_proxy.conf
[Service]
Environment="HTTP_PROXY=http://10.67.36.72:3128"
Environment="HTTPS_PROXY=http://10.67.36.72:3128"

# snap install prometheus
# snap install bjornt-prometheus-node-exporter

root@u1804:/var/snap/prometheus/18# cat prometheus.yml |egrep -v '#|^$'
# my global config
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: 'codelab-monitor'
#remote_write:
#    - url: "http://10.67.36.60:8000/write"
#remote_read:
#    - url: "http://10.67.36.60:8000/read"

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first.rules"
  # - "second.rules"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'prometheus'

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'file_sd'
    file_sd_configs:
      - files:
        - ./conf.d/*.json
		
  - job_name: federate
    scrape_interval: 15s
    honor_labels: true
    metrics_path: '/federate'

    params:
      'match[]':
         - '{job="prometheus"}'
         - '{job="node_exporter"}'
         - '{__name__=~"job:.*"}'
    static_configs:
      - targets:
        - 10.67.36.215:9090

  - job_name: node_exporter
    metrics_path: /metrics
    scheme: http
    consul_sd_configs:
      - server: 10.67.51.164:8500
        services:
          - node_exporter
#  - job_name:  'node'
#    static_configs:
#            - targets: ['localhost:9100','10.67.36.65:9100','10.67.51.143:9100','10.67.37.192:9100']

#  - job_name:  'mysqld'
#    static_configs:
#            - targets: ['10.67.37.192:9104']
#  - job_name: 'docker'
         # metrics_path defaults to '/metrics'
         # scheme defaults to 'http'.

#    static_configs:
#      - targets: ['10.67.36.70:9323']

  # Scrape openstack instances
  #  - job_name: 'openstack'
  #  tls_config:
  #      ca_file: /var/snap/prometheus/18/certs/ca-certificates.crt
  #      cert_file: /var/snap/prometheus/18/certs/haproxy.crt
  #      key_file: /var/snap/prometheus/18/certs/haproxy.key
  #      insecure_skip_verify: true
  #  openstack_sd_configs:
  #    - identity_endpoint: https://public.fuel.local:5000/v2.0
  #      username: admin
  #      project_name: IT
  #      password: F0xc0nn!23
  #      role: instance
  #  relabel_configs:
  #    - source_labels: [__meta_openstack_instance_status]
  #      action: keep
  #      regex: ACTIVE
      # Keep only instances which are flagged for scraping
      #    - source_labels: [__meta_openstack_tag_prometheus_io_scrape]
      #      action: keep
      #      regex: 'true'
      # Update the scraping port if required
      #    - source_labels: [__address__, __meta_openstack_tag_prometheus_io_port]
      #     action: replace
      #  regex: ([^:]+)(?::\d+)?;(\d+)
      #  replacement: $1:$2
      #  target_label: __address__
      # Replace the default instance by the OpenStack instance name
      #    - source_labels: [__meta_openstack_instance_name]
      #  target_label: instance




客户端
1 node-exporter                 #下载的node-exporter对centos7不太兼容，有错误
/usr/bin/node_exporter

[root@kvm-prod system]# cat /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter

[Service]
User=root
ExecStart=/usr/bin/node_exporter

[Install]
WantedBy=default.target

客户端
2 kafka_exporter
[root@kafka1 multi-user.target.wants]# cat kafka_exporter.service
# /etc/systemd/system/kafka_exporter.service
[Unit]
Description=Kafka Exporter

[Service]
Type=simple
User=root
EnvironmentFile=/etc/sysconfig/kafka_exporter
ExecStart=/usr/bin/kafka_exporter $OPTIONS
Restart=always

[Install]
WantedBy=multi-user.target

[root@kafka1 multi-user.target.wants]# cat /etc/sysconfig/kafka_exporter
OPTIONS="--kafka.server=10.67.51.144:9092"

客户端
3 haproxy_exporter
[Unit]
Description=Prometheus HAProxy Exporter
After=network.target

[Service]
Type=simple
User=haproxy
Group=haproxy

ExecStart=/usr/bin/haproxy_exporter --web.listen-address=:9101 --haproxy.scrape-uri=http://10.67.51.150:8088/?stats;csv
ExecReload=/bin/kill -HUP $MAINPID
TimeoutStopSec=10s
SendSIGKILL=no

SyslogIdentifier=prometheus_haproxy_exporter
Restart=always


mysql dashboard  "buffer pool size of total ram"
(mysql_global_variables_innodb_buffer_pool_size{instance="$host"} * 100) / on (instance) node_memory_MemTotal_bytes{instance="$host"} 替换为
(label_replace(mysql_global_variables_innodb_buffer_pool_size{instance="$host"}, "nodename", "$1", "instance", "(.*):.*") * 100) / on(nodename) (label_replace(node_memory_MemTotal_bytes, "nodename", "$1", "instance", "(.*):.*"))



[Install]
WantedBy=multi-user.target

客户端
4 mysqld-exporter
# mkdir /mysqld_exporter; cd /mysqld_exporter
# wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.10.0/mysqld_exporter-0.10.0.linux-amd64.tar.gz
# tar zxvf mysqld_exporter-0.10.0.linux-amd64.tar.gz
root@zabbix-openstack:~# cat /etc/systemd/system/mysqld_exporter.service
[Unit]
Description=Prometheus MySQL Exporter
After=network.target

[Service]
User=root
Type=simple
ExecStart=/mysqld_exporter/mysqld_exporter \
    --config.my-cnf="/mysqld_exporter/.my.cnf"
Restart=always
root@zabbix-openstack:/mysqld_exporter# ls -a
.  ..  .circleci  .git  .github  .gitignore  .my.cnf  mysqld_exporter  .promu.yml  .travis.yml
root@zabbix-openstack:/mysqld_exporter# cat .my.cnf
[client]
user=root
password=root
[Install]
WantedBy=multi-user.target
###file_sd_configs
root@u1804:/var/snap/prometheus/18# cat prometheus.yml |egrep -v '#|^$'
global:
  external_labels:
      monitor: 'codelab-monitor'
remote_write:
        - url: "http://10.67.36.62:9201/write"
remote_read:
    - url: "http://10.67.36.62:9201/read"
rule_files:
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  - job_name: 'file_sd'
    file_sd_configs:
      - files:
        - ./conf.d/*.json
  - job_name: federate
    scrape_interval: 15s
    honor_labels: true
    metrics_path: '/federate'
    params:
      'match[]':
         - '{job="prometheus"}'
         - '{job="node_exporter"}'
         - '{__name__=~"job:.*"}'
    static_configs:
      - targets:
        - 10.67.36.215:9090


root@u1804:/var/snap/prometheus/18# ls conf.d
cassandra.json  docker.json  kafka.json  mysqld.json  node_exporter.json
root@u1804:/var/snap/prometheus/18/conf.d# cat cassandra.json
[
  {
    "targets": [ "10.67.125.129:8080" ],
    "labels": {
      "env": "production",
      "job": "cassandra"
    }
  }
]

root@u1804:/var/snap/prometheus/18/conf.d# cat docker.json
[
  {
    "targets": [ "10.67.36.70:9323" ],
    "labels": {
      "env": "production",
      "job": "docker"
    }
  }
]

root@u1804:/var/snap/prometheus/18/conf.d# cat gluster_exporter.json
[
  {
    "targets": [ "10.67.36.51:8080" ],
    "labels": {
      "env": "production",
      "job": "glusterfs"
    }
  },
  {
    "targets": [ "10.67.36.52:8080" ],
    "labels": {
      "env": "production",
      "job": "glusterfs"
    }
  },
  {
    "targets": [ "10.67.36.53:8080" ],
    "labels": {
      "env": "production",
      "job": "glusterfs"
    }
  }
]
root@u1804:/var/snap/prometheus/18/conf.d#
root@u1804:/var/snap/prometheus/18/conf.d# cat haproxy.json
[
  {
    "targets": [ "10.67.51.148:9101" ],
    "labels": {
      "env": "production",
      "job": "haproxy"
    }
  }
]
root@u1804:/var/snap/prometheus/18/conf.d#
root@u1804:/var/snap/prometheus/18/conf.d# cat kafka.json
[
  {
    "targets": [ "10.67.38.134:7071" ],
    "labels": {
      "env": "test",
      "job": "kafka"
    }
  },
  {
    "targets": [ "10.67.51.144:9308" ],
    "labels": {
      "env": "production",
      "job": "kafka"
    }
  },
  {
    "targets": [ "10.67.51.144:7071" ],
    "labels": {
      "env": "production",
      "job": "kafka"
    }
  }
]

root@u1804:/var/snap/prometheus/18/conf.d# cat mysqld.json
[
  {
    "targets": [ "10.67.37.192:9104" ],
    "labels": {
      "env": "production",
      "job": "mysqld"
    }
  }
]

root@u1804:/var/snap/prometheus/18/conf.d# cat node_exporter.json
[
  {
    "targets": [ "10.67.37.192:9100" ],
    "labels": {
      "env": "production",
      "job": "node_exporter"
    }
  },
  {
    "targets": [ "10.67.36.65:9100" ],
    "labels": {
      "env": "production",
      "job": "node_exporter"
    }
  },
  {
    "targets": [ "10.67.51.144:9100" ],
    "labels": {
      "env": "production",
      "job": "node_exporter"
    }
  }
]

root@u1804:/var/snap/prometheus/18/conf.d# cat rabbitmq.json
[
  {
    "targets": [ "10.67.36.59:15692" ],
    "labels": {
      "env": "production",
      "job": "rabbitmq"
    }
  }
]

root@u1804:/var/snap/prometheus/18/conf.d# cat wmi_exporter.json
[
  {
    "targets": [ "10.67.50.113:9182" ],
    "labels": {
      "env": "production",
      "job": "win-exporter"
    }
  }
]

# 配置consul_sd 发现nomad
1 nomad.hcl
telemetry {
  collection_interval = "5s",
  prometheus_metrics = true,
  publish_allocation_metrics = true,
  publish_node_metrics = true
}
2 配置prometheus.yml
- job_name: nomad_metrics
  params:
    format:
    - prometheus
  scrape_interval: 5s
  scrape_timeout: 5s
  metrics_path: /v1/metrics
  scheme: http
  consul_sd_configs:
  - server: '10.67.50.63:8500'
    scheme: http
    services:
    - nomad-client
    - nomad
    - zdb
3 导入nomad dashboard
发现nomad dashboard太简单，监控指标不够多。决定弃用，另外，metrics path是 /v1/metrics 与通常的exporter路径不一致
只用consul服务发现来发现各种exporter会是一个好的选择，新的配置
--------------------------------------------------v1--------------------------------------------------------
- job_name: consul_sd
  params:
    format:
    - prometheus
  scrape_interval: 5s
  scrape_timeout: 5s
  metrics_path: /metrics
  scheme: http
  consul_sd_configs:
  - server: '10.67.50.63:8500'
    scheme: http
    services:
    - postgres_exporter-9187
  relabel_configs:
    - source_labels: [__meta_consul_service_port]        # 修改job标签的值，与file_sd里提供的标签一致
      regex: 9187
      target_label: "job"
      replacement: "postgres"
    - target_label: "env"                                #增加env标签，与file_sd里提供的标签一致
      replacement: "production"
--------------------------------------------------v2--------------------------------------------------------
- job_name: consul_sd
  params:
    format:
    - prometheus
  scrape_interval: 5s
  scrape_timeout: 5s
  metrics_path: /metrics
  scheme: http
  consul_sd_configs:
  - server: '10.67.50.63:8500'
    scheme: http
    services:
    - postgres-exporter                                # 此处用的postgres-exporter服务为nomad自动注册,且它的ip为host ip。 当nomad里address_mode为host时，用host ip, 当nomad里address_mode为driver时,用container ip
    - nginx-prometheus-exporter                        # 如果是container ip, prometheus是无法跟它通信的

  relabel_configs:
    - source_labels: [__meta_consul_service_port]        # 修改job标签的值，与file_sd里提供的标签一致
      regex: 9187
      target_label: "job"
      replacement: "postgres"
    - target_label: "env"                                #增加env标签，与file_sd里提供的标签一致
      replacement: "production"


##########remote storage elasticsearch##########
./elastic-adapter -elasticsearch-url=http://localhost:9200/ -elasticsearch.max-retries=1 -elasticsearch.index-perfix=prometheus -elasticsearch.type=prom-metric


#################  grafana重设密码	###################
bug   /var/lib/grafana和/usr/share/grafana/data各有一个grafana.db   实际上后者是多余的，前者才是正在使用的。  删除后者，再做软链接
需要先做以下链接
ln -s /var/lib/grafana  /usr/share/grafana/data
ln -s /var/log/grafana /usr/share/grafana/data/logs
再执行	
grafana-cli admin reset-admin-password --homepath "/usr/share/grafana" Foxconn456

##############cannot change profile for the next exec call: No such file or directory##########
##############   Failed to start Service for snap application prometheus.prometheus  ##########
执行apparmor_parser -r /var/lib/snapd/apparmor/profiles/*, 然后重启prometheus


###################################################################################################################################################################################################
### alertmanager报警
webhook方式
一 snmptrap
# snmptrap接收alertmanager报警
1 配置文件prometheus.yml
global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s
alerting:
  alertmanagers:
    - static_configs:
      - targets:
        - alertmanager:9093
      enable_http2: true
      scheme: http
      timeout: 10s
      api_version: v1
      follow_redirects: true
rule_files:
  - "rules/test.yml"
scrape_configs:
- job_name: prometheus
  honor_timestamps: true
  scrape_interval: 15s
  scrape_timeout: 10s
  metrics_path: /metrics
  scheme: http
  static_configs:
  - targets:
    - localhost:9090
    - node-exporter:9100
- job_name: 'file_sd'
  file_sd_configs:
    - files:
      - ./conf.d/*.json

2 配置alertmanager.yml
[root@promsrv prometheus-grafana]# cat alertmanager/alertmanager.yml
global:
  # global parameter
  resolve_timeout: 5m

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h        #控制发送报警的间隔时间
  receiver: 'snmp_notifier'

receivers:
- name: 'snmp_notifier'
  webhook_configs:
  - send_resolved: true
    url: 'http://10.67.50.237:9464/alerts'

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'dev', 'instance']
	  
3 rule配置文件
[root@promsrv prometheus]# cat rules/test.yml
groups:
- name: test
  rules:

  - alert: Host out of memory
    expr: 'node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 98'
    for: 30s
    labels:
      severity: warning
      type: service
      oid: "1.3.6.1.4.1.123.0.10.1.1.1.5.1"
      environment: "production"
    annotations:
      summary: "Host {{ $labels.instance }} out of memory"
      description: "{{ $labels.instance }} out of memory, current memory: {{ $value }}"
	  
4 跑起 snmp_notifier程序	  
[root@promsrv ~]# cat /root/description-template.tpl
{{- if .Alerts -}}
{{- range $severity, $alerts := (groupAlertsByLabel .Alerts "severity") -}}
Status: {{ $severity }}
{{- range $index, $alert := $alerts }}
- Alert: {{ $alert.Labels.alertname }}
  Summary: {{ $alert.Annotations.summary }}
  Description: {{ $alert.Annotations.description }}
{{ end }}
{{ end }}
{{ else -}}
Status: OK
{{- end -}}

	
cat snmp_notifier.sh
#!/bin/sh
snmp_notifier --web.listen-address=:9464 \
  --snmp.trap-description-template=/root/description-template.tpl \
  --log.level=debug > /root/snmp.log 2>&1 &

5 安装net-snmp并配置/etc/snmp/snmptrapd.conf
[root@promsrv prometheus]# cat /etc/snmp/snmptrapd.conf
authCommunity log,execute,net public
	
验证结果	  默认一个小时报警一次
/usr/sbin/snmptrapd -m ALL -m +SNMP-NOTIFIER-MIB -f -Of -Lo
NET-SNMP version 5.7.2
2022-08-16 17:10:55 localhost [UDP: [127.0.0.1]:60575->[127.0.0.1]:162]:
.iso.org.dod.internet.mgmt.mib-2.system.sysUpTime.sysUpTimeInstance = Timeticks: (227446800) 26 days, 7:47:48.00        .iso.org.dod.internet.snmpV2.snmpModules.snmpMIB.snmpMIBObjects.snmpTrap.snmpTrapOID.0 = OID: .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1       .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.1 = STRING: "1.3.6.1.4.1.123.0.10.1.1.1.5.1[alertname=Host out of memory]"  .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.2 = STRING: "warning"      .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.3 = STRING: "Status: warning
- Alert: Host out of memory
  Summary: Host node-exporter:9100 out of memory
  Description: node-exporter:9100 out of memory, current memory: 93.87814920961615"

- Alert: Host out of memory
  Summary: Host node-exporter:9100 out of memory
  Description: node-exporter:9100 out of memory, current memory: 93.48062738907147"
2022-08-17 04:10:57 localhost [UDP: [127.0.0.1]:57754->[127.0.0.1]:162]:
.iso.org.dod.internet.mgmt.mib-2.system.sysUpTime.sysUpTimeInstance = Timeticks: (231407000) 26 days, 18:47:50.00       .iso.org.dod.internet.snmpV2.snmpModules.snmpMIB.snmpMIBObjects.snmpTrap.snmpTrapOID.0 = OID: .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1       .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.1 = STRING: "1.3.6.1.4.1.123.0.10.1.1.1.5.1[alertname=Host out of memory]"  .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.2 = STRING: "warning"      .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.3 = STRING: "Status: warning
- Alert: Host out of memory
  Summary: Host node-exporter:9100 out of memory
  Description: node-exporter:9100 out of memory, current memory: 93.49661088426842"
2022-08-17 05:10:58 localhost [UDP: [127.0.0.1]:33684->[127.0.0.1]:162]:
.iso.org.dod.internet.mgmt.mib-2.system.sysUpTime.sysUpTimeInstance = Timeticks: (231767100) 26 days, 19:47:51.00       .iso.org.dod.internet.snmpV2.snmpModules.snmpMIB.snmpMIBObjects.snmpTrap.snmpTrapOID.0 = OID: .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1       .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.1 = STRING: "1.3.6.1.4.1.123.0.10.1.1.1.5.1[alertname=Host out of memory]"  .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.2 = STRING: "warning"      .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.3 = STRING: "Status: warning
- Alert: Host out of memory
  Summary: Host node-exporter:9100 out of memory
  Description: node-exporter:9100 out of memory, current memory: 93.46179145473167"
2022-08-17 06:10:58 localhost [UDP: [127.0.0.1]:34202->[127.0.0.1]:162]:
.iso.org.dod.internet.mgmt.mib-2.system.sysUpTime.sysUpTimeInstance = Timeticks: (232127100) 26 days, 20:47:51.00       .iso.org.dod.internet.snmpV2.snmpModules.snmpMIB.snmpMIBObjects.snmpTrap.snmpTrapOID.0 = OID: .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1       .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.1 = STRING: "1.3.6.1.4.1.123.0.10.1.1.1.5.1[alertname=Host out of memory]"  .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.2 = STRING: "warning"      .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.3 = STRING: "Status: warning
- Alert: Host out of memory
  Summary: Host node-exporter:9100 out of memory
  Description: node-exporter:9100 out of memory, current memory: 93.46966025236709"
2022-08-17 07:10:58 localhost [UDP: [127.0.0.1]:51870->[127.0.0.1]:162]:
.iso.org.dod.internet.mgmt.mib-2.system.sysUpTime.sysUpTimeInstance = Timeticks: (232487100) 26 days, 21:47:51.00       .iso.org.dod.internet.snmpV2.snmpModules.snmpMIB.snmpMIBObjects.snmpTrap.snmpTrapOID.0 = OID: .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1       .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.1 = STRING: "1.3.6.1.4.1.123.0.10.1.1.1.5.1[alertname=Host out of memory]"  .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.2 = STRING: "warning"      .iso.org.dod.internet.private.enterprises.123.0.10.1.1.1.5.1.3 = STRING: "Status: warning
- Alert: Host out of memory

二 alertsnitch接收alertmanager报警
[root@promsrv alertsnitch]# cat docker-compose.yml
version: "3.7"

networks:
  alertsnitch:
    driver: bridge
services:
  postgresql-server:
    restart: always
    image: 'bitnami/postgresql:latest'
    volumes:
      - alertsnitch-db:/bitnami/postgresql   
      - ~/postgres:/docker-entrypoint-initdb.d	  
    ports:
      - 5432:5432
    environment:
      - POSTGRESQL_PASSWORD=Foxconn123
    networks:
      - alertsnitch

  alertsnitch:
    image: 'registry.gitlab.com/yakshaving.art/alertsnitch:latest'
    environment:
      ALERTSNITCH_DSN: user=postgres password=Foxconn123 host=10.67.50.237 dbname=postgres sslmode=disable
      ALERTSNITCH_BACKEND: postgres
    ports:
      - 9567:9567
    networks:
      - alertsnitch
    depends_on:
      - postgresql-server
volumes:
  alertsnitch-db:

postgres=# select a.id,a.alertid,a.annotation,a.value,b.time from alertannotation as a, alertgroup as b where a.alertid=b.id;
 id | alertid | annotation  |                                value                                |            time
----+---------+-------------+---------------------------------------------------------------------+----------------------------
  1 |       1 | summary     | Host node-exporter:9100 out of memory                               | 2022-08-18 08:46:40.241586
  2 |       1 | description | node-exporter:9100 out of memory, current memory: 93.38002972930106 | 2022-08-18 08:46:40.241586

可以将该sql查询语句配置在grafana里，创建dashboard，很简单
  
### alertmanager api
[root@promsrv share]# curl http://10.67.50.237:9093/api/v1/alerts | jq .
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   740  100   740    0     0   381k      0 --:--:-- --:--:-- --:--:--  722k
{
  "status": "success",
  "data": [
    {
      "labels": {
        "alertname": "Host out of memory",
        "environment": "production",
        "instance": "node-exporter:9100",
        "job": "prometheus",
        "oid": "1.3.6.1.4.1.123.0.10.1.1.1.5.1",
        "severity": "warning",
        "type": "service"
      },
      "annotations": {
        "description": "node-exporter:9100 out of memory, current memory: 93.648109828743",
        "summary": "Host node-exporter:9100 out of memory"
      },
      "startsAt": "2022-08-16T08:54:00.232593388Z",
      "endsAt": "2022-08-16T08:58:00.232593388Z",
      "generatorURL": "http://766a56dea2ce:9090/graph?g0.expr=node_memory_MemAvailable_bytes+%2F+node_memory_MemTotal_bytes+%2A+100+%3C+98&g0.tab=1",
      "status": {
        "state": "unprocessed",
        "silencedBy": [],
        "inhibitedBy": []
      },
      "receivers": [
        "snmp_notifier"
      ],
      "fingerprint": "5d8b1e218591e94c"
    }
  ]
}


########### alert rules  ######
rules热加载
1 prometheus 进程启动时添加参数--web.enable-lifecycle
2 prometheus.yml用通配符表示多个rules文件
rule_files:
  - "rules/*.rule.yml"
3 使用API热加载rules
 curl -X POST http://10.67.50.237:9090/-/reload
检查rules是否有效 
[root@promsrv dashboards]# docker exec -it cb6a758c9d8b sh
/prometheus # promtool check rules /etc/prometheus/rules/*.yml
Checking /etc/prometheus/rules/redis.rule.yml
  SUCCESS: 3 rules found

Checking /etc/prometheus/rules/test.rule.yml
  SUCCESS: 1 rules found


# victoriametric兼容prometheus api 
查询tsdb
[root@promsrv0 promsrv]# curl -s http://10.67.50.51:8481/select/0/prometheus/api/v1/status/tsdb | jq .
{
  "status": "success",
  "isPartial": false,
  "data": {
    "totalSeries": 15685,
    "totalLabelValuePairs": 96934,
    "seriesCountByMetricName": [
      {
        "name": "flag",
        "value": 401
      },
      {
        "name": "mysql_global_status_commands_total",
        "value": 315
      },
      {
        "name": "vm_index_search_duration_seconds_bucket",
        "value": 285
      },
      {
        "name": "node_cpu_seconds_total",
        "value": 256
      },
      {
        "name": "prometheus_http_request_duration_seconds_bucket",
        "value": 210
      },
      {
        "name": "prometheus_http_response_size_bytes_bucket",
        "value": 189
      },

label_replace(probe_success, "system", "$1", "instance", "http://(.+)")

query_result(label_replace(probe_success, "system", "$1", "instance", "http://(.+)"))

label_values(query_result(label_replace(probe_success{group="http"}, "system", "$1", "instance", "http://(.+)")),system)	

# promtool维护工具
列出当前 Prometheus 的所有数据块
[root@xtjprometheus01 prometheus]# ./promtool tsdb list -r data/prometheus/
BLOCK ULID                  MIN TIME                       MAX TIME                       DURATION       NUM SAMPLES  NUM CHUNKS   NUM SERIES   SIZE
01GYZWR4QVAHSE3HEE5GGX696E  2023-04-26 00:00:00 +0000 UTC  2023-04-26 18:00:00 +0000 UTC  17h59m59.91s   1165880437   9795929      1120631      991MiB296KiB874B
01GZ1THNBESG6XD38NN6CJ4TEP  2023-04-26 18:00:00 +0000 UTC  2023-04-27 12:00:00 +0000 UTC  17h59m59.91s   1169837085   9792227      1115550      950MiB526KiB1006B
01GZ3RB2D8HR23PAFJPDN4QF7S  2023-04-27 12:00:00 +0000 UTC  2023-04-28 06:00:00 +0000 UTC  17h59m59.91s   1167536518   9772763      1110212      946MiB755KiB685B
01GZ5P4NMYVQEWV8D273YXXP18  2023-04-28 06:00:00 +0000 UTC  2023-04-29 00:00:00 +0000 UTC  17h59m59.91s   1162459813   9721933      1105512      943MiB77KiB483B
01GZ7KY74G392GQASDB9XGTK10  2023-04-29 00:00:00 +0000 UTC  2023-04-29 18:00:00 +0000 UTC  17h59m59.91s   1159428487   9692875      1096443      938MiB649KiB775B
01GZ9HQV88DEH503YSQKHK21R3  2023-04-29 18:00:00 +0000 UTC  2023-04-30 12:00:00 +0000 UTC  17h59m59.91s   1155908197   9664965      1093155      935MiB977KiB958B
01GZBFH98SQ4RQB19082CDGB93  2023-04-30 12:00:00 +0000 UTC  2023-05-01 06:00:00 +0000 UTC  17h59m59.91s   1143820941   9591188      1088721      931MiB130KiB900B
01GZDDAVYRFXTZRMBW4ZJJJXC5  2023-05-01 06:00:00 +0000 UTC  2023-05-02 00:00:00 +0000 UTC  17h59m59.91s   1146946773   9589672      1083457      927MiB221KiB787B
01GZFB4HC5TQT9YSPQJT3RZ3XX  2023-05-02 00:00:00 +0000 UTC  2023-05-02 18:00:00 +0000 UTC  17h59m59.91s   1149588272   9616057      1090684      931MiB442KiB411B
01GZH8Y004YP9EDPF93H6Z7ZS4  2023-05-02 18:00:00 +0000 UTC  2023-05-03 12:00:00 +0000 UTC  17h59m59.91s   1150899279   9624207      1090398      933MiB200KiB313B
01GZK6QGCF748QRYJTB1T0T849  2023-05-03 12:00:00 +0000 UTC  2023-05-04 04:00:00 +0000 UTC  15h59m59.91s   1015122653   8572591      1096668      850MiB540KiB175B
01GZN4H36TKMYCH33WWXTD31G1  2023-05-04 06:48:40 +0000 UTC  2023-05-05 00:00:00 +0000 UTC  17h11m19.023s  1037194294   9643987      1092551      883MiB889KiB378B
01GZQ2APXZNB7V9H2S46KBWNSC  2023-05-05 00:00:00 +0000 UTC  2023-05-05 18:00:00 +0000 UTC  17h59m59.932s  1161355537   9725108      1107185      947MiB757KiB145B
01GZPVDW63BQ1EPDTTSCAERE9V  2023-05-05 18:00:00 +0000 UTC  2023-05-05 20:00:00 +0000 UTC  1h59m59.932s   129376932    1082080      1081223      189MiB633KiB320B
01GZQ29JEM51HT3W6B6TM46HA3  2023-05-05 20:00:00 +0000 UTC  2023-05-05 22:00:00 +0000 UTC  1h59m59.932s   129346400    1081106      1080249      189MiB509KiB332B
  