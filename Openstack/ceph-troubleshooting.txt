======================================================================
1    clock skew detected on mon.node-3, mon.node-2
 #所有controller節點上執行： ntpdate -v 10.21.0.2
======================================================================
2    heartbeat_check: no reply from osd
 # ping 192.168.1.x  檢查存儲網絡
 
             "blocked": "peering is blocked due to down osds",
            "down_osds_we_would_probe": [
                4
            ],
            "peering_blocked_by": [
                {
                    "osd": 4,
                    "current_lost_at": 0,
                    "comment": "starting or marking this osd lost may let us proceed"          最終是starting osd解決
======================================================================
3    too many PGs per OSD
# vim ceph.conf
[global]
修改 mon_pg_warn_max_per_osd = 500
将配置文件推到mon所在的其他节点
ceph-deploy --overwrite-conf config push node-2 node-3
systemctl restart ceph-mon.target  每台mon
# ceph --show-config  | grep mon_pg_warn_max_per_osd
mon_pg_warn_max_per_osd = 500
======================================================================
4 pool compute has too few pgs
ceph osd pool set compute pg_num 128
ceph osd pool set compute pgp_num 128
======================================================================
5 ceph 1 near full osd
ceph health detail
ceph osd reweight-by-utilization
======================================================================
6 Ceph: HEALTH_WARN clock skew detected
ntp server unavailable,    fuel master down
======================================================================
7 osd 數據分佈不均衡
osd數據分佈不均衡，本質是有pool的pg大小設置不對。 本例，images 128  volumes 64 compute 128，數據分佈嚴重不均衡
		Data	PG      data/pg(G)
images	49G		128     0.38G
volumes  555G	64		8.67G
compute	 713G   128 	5.57G
以後要監控ceph osd df
http://www.xuxiaopang.com/2016/11/17/exp-how-pg-affect-data-distribution/
分別調整volumes和compute的pg pgp數目為256
root@node-18:~# ceph osd pool set volumes pg_num 256
set pool 2 pg_num to 256
root@node-18:~# ceph osd pool set compute pg_num 256
set pool 9 pg_num to 256
root@node-18:~# ceph osd pool set volumes pgp_num 256
set pool 2 pgp_num to 256
root@node-18:~# ceph osd pool set compute pgp_num 256
set pool 9 pgp_num to 256


ceph osd crush reweight osd.3 0.26999   （恢復，之前改過weight值）
ceph osd crush reweight osd.5 0.17000
ceph osd crush reweight osd.13 0.26999

root@node-18:~# ceph osd reweight 3 1     （恢復，之前改過reweight值）
reweighted osd.3 to 1 (10000)
root@node-18:~# ceph osd reweight 8 0.8  (調低，可能會導致pg卡在active+remapped，并且卡住不动的状态，从pg的状态去看，这个pg值分配了主的pg，没有分配到副本的osd，集群的其他设置一切正常)
reweighted osd.8 to 0.8 (cccc)
root@node-18:~# ceph osd reweight 11 1
reweighted osd.11 to 1 (10000)
======================================================================
8   replacing osd        #umount ceph-x filesystem if necessary
# ceph osd out osd.20
# ceph osd crush rm osd.20       #remove faulty OSD from CRUSH map
# ceph auth del osd.20       # remove the ceph auth key of faulty OSD
# ceph osd rm osd.20              # remove faulty OSD from ceph cluster
# ceph -s
# ceph-deploy disk list node-25      # list disk on node-25
# ceph-deploy disk zap node-25:sdi    # zap the new disk before being added into cluster
# ceph-deploy --overwrite-conf osd create node-25:sdi   # create OSD for the new disk

Sometimes removing OSD, if not done properly can result in double rebalancing. 
The best practice to remove an OSD involves changing the crush weight to 0.0 as first step.
So in the end, this will give you:

#####removing osd node######
remove one osd at a time
1 Temporarily Disable Scrubbing:
# ceph osd set noscrub
# ceph osd set nodeep-scrub

2 $ ceph osd crush reweight osd.<ID> 0.0          應該先設置osd weight值為0，rebalance完之後再移除osd
Then you wait for rebalance to be completed. Eventually completely remove the OSD:

3 remove osd
$ stop ceph-osd id=<ID>
$ ceph osd out <ID>
$ ceph osd crush remove osd.<ID>
$ ceph auth del osd.<ID>
$ ceph osd rm <ID>        真的要移除osd，才執行；在排錯階段還是不要執行rm

4 Once all OSDs are removed from the OSD node you can remove the OSD node bucket from the CRUSH map.
$ ceph osd crush rm `hostname -s`           #remove {bucket-name}
#####removing osd node######

重新添加已刪除的osd,已osd.0和osd.1為例
# ceph osd create      自動create為刪除的osd num
# ceph osd crush add osd.0 0 host=node-5    添加時weight值要設置為0，防止rebalance
# ceph osd crush add osd.1 0 host=node-4
# ceph auth add osd.1 osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-1/keyring      添加auth
# ceph auth add osd.0 osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-0/keyring
node-5# start ceph-osd id=0
node-4# start ceph-osd id=1

======================================================================
9    pg stuck at active+remapped         处理办法：通过rebalance osd上面的数据，或者使异常的osd恢复正常
                                                   如有down的osd，想办法使其恢复正常，或者设置crush reweight 0
pg 7.c is stuck unclean for 9508.985801, current state active+remapped, last acting [6,3]
pg 6.2f is stuck unclean for 28921.295302, current state active+remapped, last acting [4,13]
recovery 1/696762 objects misplaced (0.000%)

root@node-18:~# ceph pg dump_stuck unclean
ok
pg_stat state   up      up_primary      acting  acting_primary
7.c     active+remapped [6]     6       [6,3]   6
6.2f    active+remapped [4]     4       [4,13]  4

root@node-18:~# ceph pg dump_stuck|grep '2\.9a'
ok
2.9a    active+remapped+wait_backfill+backfill_toofull  [7,0]   7       [6,14]  6

root@node-18:~# ceph health detail
HEALTH_WARN 2 pgs stuck unclean; recovery 1/684546 objects misplaced (0.000%)
pg 7.c is stuck unclean for 203690.028168, current state active+remapped, last acting [6,3]
pg 6.2f is stuck unclean for 223102.337668, current state active+remapped, last acting [4,13]
recovery 1/684546 objects misplaced (0.000%)
root@node-18:~# ceph osd tree
ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 3.84985 root default
-2 1.24995     host node-13
 0 0.26999         osd.0         up  1.00000          1.00000
 2 0.26999         osd.2         up  1.00000          1.00000
 4 0.26999         osd.4         up  1.00000          1.00000
 6 0.26999         osd.6         up  1.00000          1.00000
 8 0.17000         osd.8         up  1.00000          1.00000
-3 1.24995     host node-16
 1 0.26999         osd.1         up  1.00000          1.00000
 3 0.26999         osd.3         up  0.58408          1.00000
 5 0.17000         osd.5         up  0.61909          1.00000
 7 0.26999         osd.7         up  1.00000          1.00000
 9 0.26999         osd.9         up  1.00000          1.00000
-4 1.34995     host node-4
10 0.26999         osd.10        up  1.00000          1.00000
11 0.26999         osd.11        up  0.80971          1.00000
12 0.26999         osd.12        up  1.00000          1.00000
13 0.26999         osd.13        up  1.00000          1.00000
14 0.26999         osd.14        up  1.00000          1.00000

#查看osd状态
root@node-14:~# ceph osd dump |grep -e 'up\ *out'
osd.23 up   out weight 0 up_from 79853 up_thru 75394 down_at 79852 last_clean_interval [79475,79850) 192.168.1.8:6836/12382 192.168.1.8:6837/12382 192.168.1.8:6838/12382 192.168.1.8:6839/12382 exists,up da3f37ae-bc30-45e9-92c7-b0cc429470d5

root@node-14:~# ceph osd dump |grep -e 'down\ *out'
osd.15 down out weight 0 up_from 79887 up_thru 79909 down_at 79927 last_clean_interval [79855,79886) 192.168.1.8:6844/12634 192.168.1.8:6855/1012634 192.168.1.8:6862/1012634 192.168.1.8:6869/1012634 autoout,exists ef694785-e63c-42ae-a781-763d101d7b38
osd.17 down out weight 0 up_from 80350 up_thru 80350 down_at 80352 last_clean_interval [80347,80348) 192.168.1.8:6814/982964 192.168.1.8:6818/982964 192.168.1.8:6819/982964 192.168.1.8:6821/982964 autoout,exists e4f5d694-d957-42eb-a801-1bd9abfd6788
osd.49 down out weight 0 up_from 48682 up_thru 73555 down_at 74308 last_clean_interval [48513,48653) 192.168.1.13:6820/1147772 192.168.1.13:6821/1147772 192.168.1.13:6822/1147772 192.168.1.13:6823/1147772 autoout,exists 4d9888d3-7341-4fe4-8b76-dab49631a929
osd.55 down out weight 0 up_from 48682 up_thru 60032 down_at 60035 last_clean_interval [48513,48653) 192.168.1.13:6808/1147458 192.168.1.13:6809/1147458 192.168.1.13:6810/1147458 192.168.1.13:6811/1147458 autoout,exists 89ca16b3-cf70-457b-860f-dad522bbcb07

##20210707  osd.21坏掉了
root@node-1:~# ceph health detail
HEALTH_WARN 8 pgs stale; 8 pgs stuck stale
pg 5.12 is stuck stale for 4946413.799694, current state stale+active+clean, last acting [21]
pg 5.e3 is stuck stale for 4946413.800250, current state stale+active+clean, last acting [21]
pg 5.1eb is stuck stale for 4946413.800492, current state stale+active+clean, last acting [21]
pg 5.53 is stuck stale for 4946413.800310, current state stale+active+clean, last acting [21]
pg 5.1ac is stuck stale for 4946413.800359, current state stale+active+clean, last acting [21]
pg 5.77 is stuck stale for 4946413.800320, current state stale+active+clean, last acting [21]
pg 5.1a3 is stuck stale for 4946413.800363, current state stale+active+clean, last acting [21]
pg 5.1c2 is stuck stale for 4946413.800373, current state stale+active+clean, last acting [21]
root@node-1:~# ceph pg 5.12 query
Error ENOENT: i don't have pgid 5.12

