0.94.x   Hammer LTS
10.2.x   Jewel  LTS

#############ceph performance######
root@node-5:~# ceph osd perf |grep 23
osd fs_commit_latency(ms) fs_apply_latency(ms)
 23                     0                 3240
停掉osd 23试试，查看性能
root@node-1:~# ceph osd pool stats
pool rbd id 0
  nothing is going on

pool .rgw id 1
  nothing is going on

pool compute id 2
  client io 0 B/s rd, 2764 kB/s wr, 152 op/s

pool images id 3
  nothing is going on

pool volumes id 4
  client io 81373 B/s rd, 3809 kB/s wr, 1004 op/s

pool backups id 5
  nothing is going on

pool .rgw.root id 6
  nothing is going on

pool .rgw.control id 7
  nothing is going on

pool .rgw.gc id 8
  nothing is going on

pool .users.uid id 9
  nothing is going on

pool .users id 10
  nothing is going on

pool .rgw.buckets.index id 11
  nothing is going on

pool .rgw.buckets id 12
  nothing is going on



#############impact of SSD journals#################
SSD journals accelerate bursts and random write IO
For sustained writes that overflow the journal, performance degrades to HDD levels
SSD help very little with read performance

#############ceph bench###############
dd if=/dev/zero of=/1.img bs=1M count=5120 conv=fdatasync          # from instance
rados bench -p rbd 20 write --no-cleanup
rados bench -p rbd 20 rand
rados -p rbd cleanup
for u in `rados -p rbd ls`;do rados rm $u -p rbd;done
##############kernel upgrade on ceph node##############
root@node-18:~# apt-get dist-upgrade
Reading package lists... Done
Building dependency tree
Reading state information... Done
Calculating upgrade... Done
The following NEW packages will be installed:
  linux-headers-3.13.0-119 linux-headers-3.13.0-119-generic
  linux-image-3.13.0-119-generic linux-image-extra-3.13.0-119-generic
The following packages will be upgraded:
  linux-headers-generic linux-image-generic
2 upgraded, 4 newly installed, 0 to remove and 0 not upgraded.

root@node-18:~# ceph osd set noout     防止rebalance
set noout
root@node-18:~# ceph -s
    cluster 05761444-f55c-42ca-a0b0-97145898c3bc
     health HEALTH_WARN
            noout flag(s) set
     monmap e4: 3 mons at {node-10=192.168.0.6:6789/0,node-14=192.168.0.5:6789/0,node-18=192.168.0.3:6789/0}
            election epoch 358, quorum 0,1,2 node-18,node-14,node-10
     osdmap e4817: 21 osds: 21 up, 21 in
            flags noout
      pgmap v11204978: 1280 pgs, 13 pools, 1573 GB data, 396 kobjects
            3143 GB used, 2876 GB / 6020 GB avail
                1280 active+clean
  client io 13082 kB/s rd, 317 kB/s wr, 204 op/s
stop ceph-osd id={num}
 reboot ceph node
root@node-18:~# ceph osd unset noout
unset noout
root@node-18:~# ceph -s
    cluster 05761444-f55c-42ca-a0b0-97145898c3bc
     health HEALTH_OK
     monmap e4: 3 mons at {node-10=192.168.0.6:6789/0,node-14=192.168.0.5:6789/0,node-18=192.168.0.3:6789/0}
            election epoch 358, quorum 0,1,2 node-18,node-14,node-10
     osdmap e4837: 21 osds: 21 up, 21 in
      pgmap v11205265: 1280 pgs, 13 pools, 1573 GB data, 396 kobjects
            3143 GB used, 2876 GB / 6020 GB avail
                1280 active+clean
  client io 10693 kB/s rd, 1003 kB/s wr, 365 op/s
############kernel upgrade on controller##########
sometimes public network on vm is not reachable
###################### ceph ############################
acting set： 當前的set
up set:  經過crushmap計算出的set
actingset最终会变换到up set

ceph命令
===============
ceph pg 7.c query
ceph df      ceph osd df
ceph osd dump
ceph pg dump
ceph mon dump --format json-pretty
rados lspools
rados -p compute ls
rbd ls compute
rbd info compute/318b7f59-30d7-4b00-9fbd-fb6b02b62fa7_disk
 glance image-list
 glance image-show bcf85e39-51dd-49cc-90d7-382d5cc62c2b
 rbd info images/bcf85e39-51dd-49cc-90d7-382d5cc62c2b
 rbd snap ls images/bcf85e39-51dd-49cc-90d7-382d5cc62c2b
root@node-18:~# ceph osd pool get images pg_num
pg_num: 64
root@node-18:~# ceph osd pool get images pgp_num
pgp_num: 64
ceph osd map .rgw.buckets p2.sh
ansible ceph -m shell -a 'ceph-disk list|grep ceph'

root@node-1:~# for u in `rados lspools`;do echo $u `ceph pg ls-by-pool $u |tail -n +2|wc -l`;done
rbd 64
.rgw 512
compute 1024
images 256
volumes 3168
backups 512
.rgw.root 64
.rgw.control 64
.rgw.gc 64
.users.uid 64
.users 64
.rgw.buckets.index 64
.rgw.buckets 64

root@node-1:~# for u in `rados lspools`;do echo -en "$u   `ceph osd pool get $u pg_num`   ";ceph osd pool get $u pgp_num;done
rbd   pg_num: 64   pgp_num: 64
.rgw   pg_num: 512   pgp_num: 512
compute   pg_num: 1024   pgp_num: 1024
images   pg_num: 256   pgp_num: 256
volumes   pg_num: 3168   pgp_num: 3168
backups   pg_num: 512   pgp_num: 512
.rgw.root   pg_num: 64   pgp_num: 64
.rgw.control   pg_num: 64   pgp_num: 64
.rgw.gc   pg_num: 64   pgp_num: 64
.users.uid   pg_num: 64   pgp_num: 64
.users   pg_num: 64   pgp_num: 64
.rgw.buckets.index   pg_num: 64   pgp_num: 64
.rgw.buckets   pg_num: 64   pgp_num: 64


ceph osd crush add osd.ID 0 host=<HOSTNAME>

################crush map##############
root@node-18:~# ceph osd getcrushmap -o /tmp/map
got crush map from osdmap epoch 3486
root@node-18:~# crushtool -d /tmp/map -o /root/map.txt
root@node-18:~# cat map.txt
# begin crush map
tunable choose_local_tries 0
tunable choose_local_fallback_tries 0
tunable choose_total_tries 50
tunable chooseleaf_descend_once 1
tunable straw_calc_version 1

# devices
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3
device 4 osd.4
device 5 osd.5
device 6 osd.6
device 7 osd.7
device 8 osd.8
device 9 osd.9
device 10 osd.10
device 11 osd.11
device 12 osd.12
device 13 osd.13
device 14 osd.14

# types
type 0 osd
type 1 host
type 2 chassis
type 3 rack
type 4 row
type 5 pdu
type 6 pod
type 7 room
type 8 datacenter
type 9 region
type 10 root

# buckets
host node-13 {
        id -2           # do not change unnecessarily
        # weight 1.250
        alg straw
        hash 0  # rjenkins1
        item osd.0 weight 0.270
        item osd.2 weight 0.270
        item osd.4 weight 0.270
        item osd.6 weight 0.270
        item osd.8 weight 0.170
}
host node-16 {
        id -3           # do not change unnecessarily
        # weight 1.250
        alg straw
        hash 0  # rjenkins1
        item osd.1 weight 0.270
        item osd.3 weight 0.270
        item osd.5 weight 0.170
        item osd.7 weight 0.270
        item osd.9 weight 0.270
}
host node-4 {
        id -4           # do not change unnecessarily
        # weight 1.350
        alg straw
        hash 0  # rjenkins1
        item osd.10 weight 0.270
        item osd.11 weight 0.270
        item osd.12 weight 0.270
        item osd.13 weight 0.270
        item osd.14 weight 0.270
}
root default {
        id -1           # do not change unnecessarily
        # weight 3.850
        alg straw
        hash 0  # rjenkins1
        item node-13 weight 1.250
        item node-16 weight 1.250
        item node-4 weight 1.350
}

# rules
rule replicated_ruleset {
        ruleset 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step chooseleaf firstn 0 type host
        step emit
}

# end crush map



RBD的I/O路径很长，要经过网络、文件系统、磁盘：
Librbd -> networking -> OSD -> FileSystem -> Disk
Client的每个写操作在OSD中要经过8种线程，写操作下发到OSD之后，会产生2~3个磁盘seek操作：
把写操作记录到OSD的Journal文件上(Journal是为了保证写操作的原子性)。
把写操作更新到Object对应的文件上。
把写操作记录到PG Log文件上。
在单机情况下，RBD的性能不如传统的RAID10，这是因为RBD的I/O路径很复杂，导致效率很低。
但是Ceph的优势在于它的扩展性，它的性能会随着磁盘数量线性增长，
因此在多机的情况下，RBD的IOPS和吞吐率会高于单机的RAID10(不过性能会受限于网络的带宽)

最近看到了有人的环境出现了出现了卡在active+remapped状态，并且卡住不动的状态，从pg的状态去看，这个pg值分配了主的pg，没有分配到副本的osd，集群的其他设置一切正常
这个从网上搜寻到的资料来看，大多数都是由于不均衡的主机osd引起的，所谓不平衡的osd
一台机器上面的磁盘的容量不一样，有的3T，有的1T
两台主机上面的OSD个数不一样，有的5个，有的2个
这样会造成主机的crush 的weight的差别很大的问题，以及分布算法上的不平衡问题，建议对于一个存储池来说，它所映射的osd至少需要是磁盘大小一致和个数一致的
这个问题我在我的环境下做了复现，确实有卡在remapped的问题
出现这个情况一般是什么操作引起的？
做osd的reweight的操作引起的，这个因为一般在做reweight的操作的时候，根据算法，这个上面的pg是会尽量分布在这个主机上的，
而crush reweight不变的情况下，去修改osd 的reweight的时候，可能算法上会出现无法映射的问题
怎么解决这个问题？
直接做osd crush reweight的调整即可避免这个问题，这个straw算法里面还是有点小问题的，在调整某个因子的时候会引起整个因子的变动
之前看到过sage在回复这种remapped问题的时候，都是不把这个归到bug里面去的，这个我也认为是配置问题引起的极端的问题，正常情况下都能避免的

root@node-6:/var/log/ceph# ceph -s
    cluster 9b4cf431-e850-443f-b8b2-bd0db1137b07
     health HEALTH_WARN
            175 pgs backfill
            398 pgs degraded
            313 pgs down
            3401 pgs peering
            1 pgs recovering
            21 pgs recovery_wait
            396 pgs stuck degraded
            3227 pgs stuck inactive
            3858 pgs stuck unclean
            374 pgs stuck undersized
            375 pgs undersized
            1241 requests are blocked > 32 sec
            recovery 20559/969905 objects degraded (2.120%)
            recovery 46744/969905 objects misplaced (4.819%)
            recovery 197/318139 unfound (0.062%)
            too many PGs per OSD (336 > max 300)
            2/51 in osds are down
     monmap e3: 3 mons at {node-1=192.168.1.4:6789/0,node-2=192.168.1.7:6789/0,node-3=192.168.1.5:6789/0}
            election epoch 78, quorum 0,1,2 node-1,node-3,node-2
     osdmap e10512: 57 osds: 49 up, 51 in; 1777 remapped pgs
      pgmap v12567096: 5920 pgs, 12 pools, 1233 GB data, 310 kobjects
            3914 GB used, 50045 GB / 53960 GB avail
            20559/969905 objects degraded (2.120%)
            46744/969905 objects misplaced (4.819%)
            197/318139 unfound (0.062%)
                2062 active+clean
                1688 peering
                1400 remapped+peering
                 243 active+undersized+degraded
                 174 down+remapped+peering
                 139 down+peering
                 132 active+undersized+degraded+remapped+wait_backfill
                  43 active+remapped+wait_backfill
                  16 active+remapped
                  11 active+recovery_wait+degraded+remapped
                  10 active+recovery_wait+degraded
                   1 active+recovering+degraded+remapped
                   1 active+degraded
