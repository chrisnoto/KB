Secret, configMap, downwardAPI and projected volumes will be mounted as read-only volumes. 
Applications that attempt to write to these volumes will receive read-only filesystem errors.

#############kubectl top #########
[chensen@cobbler ~]$ kubectl top pod --all-namespaces
NAMESPACE       NAME                                    CPU(cores)   MEMORY(bytes)
default         cerebro-7678f89dc5-5qx7m                10m          479Mi
default         kafka-manager-6df59b5596-sxwb7          26m          434Mi
default         logstash-7d56c78f9-pfkfh                174m         6882Mi
default         w3c-7fdc686574-bsqcb                    0m           1Mi
ingress-nginx   default-http-backend-5954bd5d8c-s2m8t   1m           5Mi
ingress-nginx   nginx-ingress-controller-l89hx          15m          166Mi
ingress-nginx   nginx-ingress-controller-lttv5          15m          179Mi
ingress-nginx   nginx-ingress-controller-lzkdr          17m          169Mi
ingress-nginx   nginx-ingress-controller-n2ghj          16m          192Mi
ingress-nginx   nginx-ingress-controller-w79m6          10m          201Mi
ingress-nginx   nginx-ingress-controller-x8flb          13m          171Mi
kube-system     canal-7c6tw                             42m          69Mi
kube-system     canal-95rlc                             40m          66Mi
kube-system     canal-cdxnn                             53m          74Mi
kube-system     canal-dwmv6                             54m          73Mi
kube-system     canal-f8kvc                             57m          74Mi
kube-system     canal-kmgxg                             63m          71Mi
kube-system     canal-zmtzl                             51m          71Mi
kube-system     coredns-86bc4b7c96-jsvtg                11m          19Mi
kube-system     coredns-86bc4b7c96-mbjht                10m          18Mi
kube-system     coredns-autoscaler-5d5d49b8ff-crrpw     1m           8Mi
kube-system     metrics-server-7f6bd4c888-jw5rd         2m           16Mi




##############cronjob##############  cronjob的时间与kubernetes平台时间一致, 不与OS时间一致 可查看任意一个k8s组件的容器时间
[chensen@cobbler curator]$ kubectl get cronjob
NAME      SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
curator   33 03 * * *   False     0        <none>          9m26s
[chensen@cobbler curator]$ kubectl get po -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
cerebro-7678f89dc5-5qx7m         1/1     Running   0          45h     10.42.2.12   10.67.36.59   <none>           <none>
kafka-manager-6df59b5596-sxwb7   1/1     Running   0          26h     10.42.3.13   10.67.36.62   <none>           <none>
logstash-7d56c78f9-pfkfh         1/1     Running   0          2d18h   10.42.1.16   10.67.36.57   <none>           <none>
w3c-7fdc686574-bsqcb             1/1     Running   0          11d     10.42.4.2    10.67.36.60   <none>           <none>
[chensen@cobbler curator]$ kubectl get cronjob
NAME      SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
curator   33 03 * * *   False     1        3s              10m
[chensen@cobbler curator]$ kubectl get po -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
cerebro-7678f89dc5-5qx7m         1/1     Running   0          45h     10.42.2.12   10.67.36.59   <none>           <none>
curator-1564630800-vk9vv         1/1     Running   0          43s     10.42.5.13   10.67.36.61   <none>           <none>
kafka-manager-6df59b5596-sxwb7   1/1     Running   0          26h     10.42.3.13   10.67.36.62   <none>           <none>

[chensen@cobbler curator]$ kubectl logs curator-1564630800-vk9vv
Starting bootup process...

Copying config file...

Replacing vars...

Calling curator...
2019-08-01 03:40:36,101 INFO      Preparing Action ID: 1, "delete_indices"
2019-08-01 03:40:36,120 INFO      Trying Action ID: 1, "delete_indices": Delete indices older than 90 days (based on index name), for ALL the prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.
2019-08-01 03:40:36,701 INFO      Deleting selected indices: ['filebeat-6.8.1-2019.02.22', 'filebeat-6.8.1-2019.02.23']
2019-08-01 03:40:36,702 INFO      ---deleting index filebeat-6.8.1-2019.02.22
2019-08-01 03:40:36,702 INFO      ---deleting index filebeat-6.8.1-2019.02.23
2019-08-01 03:40:37,078 INFO      Action ID: 1, "delete_indices" completed.
2019-08-01 03:40:37,079 INFO      Preparing Action ID: 2, "forcemerge"
2019-08-01 03:40:37,084 INFO      Trying Action ID: 2, "forcemerge": Forcemerge indices older than 2 days (based on index creation_date) to 2 segments per shard.  Delay 60 seconds between each forceMerge operation to allow the cluster to quiesce. Skip indices that have already been forcemerged to the minimum number of segments to avoid reprocessing.
2019-08-01 03:40:37,950 INFO      forceMerging selected indices
2019-08-01 03:40:37,951 INFO      forceMerging index filebeat-6.8.1-2019.07.24 to 1 segments per shard.  Please wait...
[chensen@cobbler ~]$ kubectl get po -o wide
NAME                             READY   STATUS      RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
cerebro-7678f89dc5-5qx7m         1/1     Running     0          47h     10.42.2.12   10.67.36.59   <none>           <none>
curator-1564630800-vk9vv         0/1     Completed   5          114m    10.42.5.13   10.67.36.61   <none>           <none>
kafka-manager-6df59b5596-sxwb7   1/1     Running     0          28h     10.42.3.13   10.67.36.62   <none>           <none>


##############kubectl exec ########
[chensen@cobbler logstash]$ kubectl exec logstash-7d56c78f9-l5pk4 -- cat /usr/share/logstash/config/logstash.yml
http.host: 0.0.0.0
http.port: 9600
pipeline.workers: 8
pipeline.batch.size: 4000
pipeline.batch.delay: 10
config.reload.automatic: true
xpack.monitoring.enabled: true
xpack.monitoring.elasticsearch.url: ["http://elasticsearch:9200"]
xpack.monitoring.collection.interval: 10s

#########cpu affinity#######
[root@worker6 ~]# taskset -cp 12706   (logstash pid)
pid 12706's current affinity list: 0-7
[root@worker6 ~]# docker ps |grep logstash
8ded7a485575        b6a9d6f1254e                         "/usr/local/bin/dock…"   8 minutes ago       Up 8 minutes                            k8s_logstash_logstash-7d56c78f9-tgdb4_default_2645623d-adf4-11e9-bbcf-0050569376db_0
0f7e1e393f5e        rancher/pause:3.1                    "/pause"                 8 minutes ago       Up 8 minutes                            k8s_POD_logstash-7d56c78f9-tgdb4_default_2645623d-adf4-11e9-bbcf-0050569376db_0
[root@worker6 ~]# docker exec -it 8ded taskset -cp 1
pid 1's current affinity list: 0-7
##############outside service ###########
[chensen@cobbler logstash]$ kubectl get endpoints
NAME            ENDPOINTS           AGE
elasticsearch   10.67.51.150:9200   47s
[chensen@cobbler logstash]$ kubectl get svc
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
elasticsearch   ClusterIP   10.43.201.194   <none>        9200/TCP   51s
[root@worker3 ~]# docker run --rm appropriate/curl -s http://10.67.51.150:9200/_cluster/settings
{"persistent":{"xpack":{"monitoring":{"collection":{"enabled":"true"}}}},"transient":{}}
[root@worker3 ~]# docker run --rm appropriate/curl -s http://10.43.201.194:9200/_cluster/settings
{"persistent":{"xpack":{"monitoring":{"collection":{"enabled":"true"}}}},"transient":{}}

[chensen@cobbler logstash]$ kubectl run -it --rm --restart=Never curl --image=appropriate/curl sh
If you don't see a command prompt, try pressing enter.
/ # curl -s http://elasticsearch:9200
{
  "name" : "stjes1",
  "cluster_name" : "es-prod",
  "cluster_uuid" : "3hWr0QRmRomwGRsW_KqMgg",
  "version" : {
    "number" : "6.8.1",
    "build_flavor" : "default",
    "build_type" : "rpm",
    "build_hash" : "1fad4e1",
    "build_date" : "2019-06-18T13:16:52.517138Z",
    "build_snapshot" : false,
    "lucene_version" : "7.7.0",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
########不清楚为什么nslookup不工作, 而上面curl镜像却可以解析elasticsearch
[chensen@cobbler logstash]$ kubectl run -it --rm --restart=Never busybox --image=busybox /bin/sh
If you don't see a command prompt, try pressing enter.
/ # nslookup elasticsearch
Server:         10.43.0.10
Address:        10.43.0.10:53

** server can't find elasticsearch.default.svc.cluster.local: NXDOMAIN

*** Can't find elasticsearch.svc.cluster.local: No answer
*** Can't find elasticsearch.cluster.local: No answer
*** Can't find elasticsearch.worker1: No answer
*** Can't find elasticsearch.default.svc.cluster.local: No answer
*** Can't find elasticsearch.svc.cluster.local: No answer
*** Can't find elasticsearch.cluster.local: No answer
*** Can't find elasticsearch.worker1: No answer
######node上指定internal DNS 可以解析elasticsearch
[root@worker5 ~]# nslookup elasticsearch.default.svc.cluster.local 10.43.0.10 (internal DNS)
Server:         10.43.0.10
Address:        10.43.0.10#53

Non-authoritative answer:
Name:   elasticsearch.default.svc.cluster.local
Address: 10.43.201.194

###########configmap##########
kubectl create configmap --from-file=kibana.yml=./kibana.yml kibana-config
########### HPA #############
kubectl autoscale deployment web1 --cpu-percent=50 --min=1 --max=3
[chensen@cobbler ~]$ kubectl get hpa
NAME      REFERENCE         TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
web1      Deployment/web1   0% / 50%   1         3         1          5m

[root@rancher ~]# cat hpa.yml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: web1
  namespace: default
spec:
  maxReplicas: 3
  minReplicas: 1
  scaleTargetRef:
    kind: Deployment
    name: web1
  targetCPUUtilizationPercentage: 50

#############验证k8s集群健康情况########
[root@cobbler ~]# kubectl cluster-info
Kubernetes master is running at https://10.67.36.58/k8s/clusters/c-vtl4b
KubeDNS is running at https://10.67.36.58/k8s/clusters/c-vtl4b/api/v1/namespaces/kube-system/services/kube-dns/proxy

[root@cobbler ~]# kubectl get componentstatus
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}

[root@cobbler ~]# kubectl get node
NAME      STATUS    AGE       VERSION
master    Ready     54d       v1.11.5
worker1   Ready     53d       v1.11.5
worker2   Ready     53d       v1.11.5
worker3   Ready     53d       v1.11.5

########kubectl -v 详细信息#########
[root@cobbler ~]# kubectl version --v=7
I0109 06:43:10.872375   31737 loader.go:357] Config loaded from file /root/.kube/config
I0109 06:43:10.873332   31737 round_trippers.go:383] GET https://10.67.36.58/k8s/clusters/c-v6qjl/version
I0109 06:43:10.873350   31737 round_trippers.go:390] Request Headers:
I0109 06:43:10.873362   31737 round_trippers.go:393]     Accept: application/json, */*
I0109 06:43:10.873374   31737 round_trippers.go:393]     User-Agent: kubectl/v1.7.5 (linux/amd64) kubernetes/17d7182
I0109 06:43:10.873385   31737 round_trippers.go:393]     Authorization: Bearer kubeconfig-user-kpxwd:r85pf6kgf8m92smlkmqkd6tbkvkbhcq2dj29mfnptk2mdwbbn6szp8
I0109 06:43:13.625597   31737 round_trippers.go:408] Response Status: 503 Service Unavailable in 2752 milliseconds

#########configmap subpath 解决mount时覆盖问题######
    volumeMounts:
    - mountPath: /usr/local/apache2/conf/httpd.conf
      name: vol1
      subPath: httpd.conf
    - mountPath: /usr/local/apache2/conf/extra
      name: vol2
  volumes:
  - configMap:
      defaultMode: 256
      name: httpdconf
      optional: false
    name: vol1
  - configMap:
      defaultMode: 256
      name: proxyconf
      optional: false
    name: vol2
	
######check ENV SERVICE######
[root@cobbler ~]# kubectl exec web1-557d88546-4t9xm -- printenv |grep SERVICE
KUBERNETES_SERVICE_PORT=443
WEB1_SERVICE_PORT=80
WEB1_NODEPORT_SERVICE_PORT=80
WEB1_NODEPORT_SERVICE_PORT_80TCP01=80
KUBERNETES_SERVICE_PORT_HTTPS=443
WEB1_SERVICE_PORT_80TCP01_WEB1=80
WEB1_SERVICE_HOST=10.43.45.18
KUBERNETES_SERVICE_HOST=10.43.0.1
WEB1_NODEPORT_SERVICE_HOST=10.43.103.29

[root@cobbler ~]# kubectl get po -o wide
NAME                   READY     STATUS    RESTARTS   AGE       IP          NODE
web1-557d88546-4t9xm   1/1       Running   0          50d       10.42.3.5   worker1

[root@cobbler ~]# kubectl get svc
NAME                CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
glusterfs-cluster   10.43.174.138   <none>        1/TCP          36d
kubernetes          10.43.0.1       <none>        443/TCP        52d
web1                10.43.45.18     <none>        80/TCP         50d
web1-nodeport       10.43.103.29    <nodes>       80:30979/TCP   50d

###########
[root@cobbler ~]# kubectl auth can-i '*' services --as system:anoymous
yes
[root@cobbler ~]# kubectl auth can-i '*' services --as system:serviceaccount:default:default
yes


############Statefulset###########
#kubectl get statefulsets web
NAME      DESIRED   CURRENT   AGE
web       2         2         1h
#kubectl get pods
web-0                       1/1       Running   0          1h
web-1                       1/1       Running   0          1h
#nslookup nginx.default.svc.cluster.local 10.96.0.10
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   nginx.default.svc.cluster.local
Address: 10.244.2.17
Name:   nginx.default.svc.cluster.local
Address: 10.244.2.18

#nslookup web-1.nginx.default.svc.cluster.local 10.96.0.10
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   web-1.nginx.default.svc.cluster.local
Address: 10.244.2.18

#nslookup web-0.nginx.default.svc.cluster.local 10.96.0.10
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   web-0.nginx.default.svc.cluster.local
Address: 10.244.2.17
如上，web为我们创建的StatefulSets，对应的pod的域名为web-0，web-1，他们之间可以互相访问，这样对于一些集群类型的应用就可以解决互相之间身份识别的问题了。

###Taints and tolerations###
A taint is an attribute added to a node. It prevents pods from running on the node
... Unless they have a matching toleration

When deploying with kubeadm:
a taint is placed on the node dedicated the control plane
the pods running the control plane have a matching toleration

Special tolerations
{
    "operator": "Exists"
  }
Check our nodes specs:
kubectl get node node1 -o json | jq .spec
kubectl get node node2 -o json | jq .spec
Check tolerations for CoreDNS:
kubectl -n kube-system get deployments coredns -o json | jq .spec.template.spec.tolerations


#########List container name in pod#####
[root@cobbler ~]#  kubectl -n kube-system get po -l k8s-app=canal
NAME          READY     STATUS    RESTARTS   AGE
canal-4rtg8   3/3       Running   0          2d
canal-4xc4p   3/3       Running   0          2d
canal-4zr27   3/3       Running   0          2d
canal-gcqh2   3/3       Running   0          2d
canal-mvprn   3/3       Running   3          2d
canal-sjtzc   3/3       Running   0          2d
[root@cobbler ~]# kubectl -n kube-system get po -l k8s-app=kube-dns -o jsonpath={.items[*].spec.containers[*].name}
kubedns dnsmasq sidecar kubedns dnsmasq sidecar
[root@cobbler ~]# kubectl -n kube-system get po canal-4rtg8 -o jsonpath={.spec.containers[*].name}
calico-node install-cni kube-flannel
[root@cobbler ~]# kubectl -n ingress-nginx get po -l app=ingress-nginx
NAME                             READY     STATUS    RESTARTS   AGE
nginx-ingress-controller-gq4h8   1/1       Running   0          2d
nginx-ingress-controller-ktzmg   1/1       Running   0          2d
nginx-ingress-controller-l6wgp   1/1       Running   0          2d
nginx-ingress-controller-m972l   1/1       Running   0          2d
nginx-ingress-controller-qzp9t   1/1       Running   0          2d
[root@cobbler ~]# kubectl -n ingress-nginx get po -l app=ingress-nginx -o jsonpath={.items[*].spec.containers[*].name}
nginx-ingress-controller nginx-ingress-controller nginx-ingress-controller nginx-ingress-controller nginx-ingress-controller

###########jsonpath範例####
[root@k8sv17 ~]# kubectl get nodes -o jsonpath='{range .items[*]}{.spec.externalID}{"\t"}{.spec.podCIDR}{"\n"}{end}'
k8sv17.novalocal        10.96.0.0/24
node-1.novalocal        10.96.1.0/24
node-2.novalocal        10.96.2.0/24
node-3.novalocal        10.96.3.0/24

##########Kubectl output verbosity and debugging#######
--v=0	Generally useful for this to ALWAYS be visible to an operator.
--v=1	A reasonable default log level if you don’t want verbosity.
--v=2	Useful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems.
--v=3	Extended information about changes.
--v=4	Debug level verbosity.
--v=6	Display requested resources.
--v=7	Display HTTP request headers.
--v=8	Display HTTP request contents.
--v=9	Display HTTP request contents without truncation of contents.
eg:  kubectl get nodes --v=9

########導出admin用戶的cluster role####
kubectl get clusterrole admin -o yaml > admin.yaml

#######k8s使用cinder做存儲後端#####
这需要两个前提：
1. OpenStack作为Kubernetes Provider，因为需要从Nova查询Pod所在的机器
2. 官方文档中所说的那些要求

###### service yaml里定義的各種port #######
<cluster ip>: port 是提供给集群内部客户访问service的入口. port是service的虛端口
<nodeIP>:nodePort 是提供给集群外部客户访问service的入口
targetPort  targetPort是pod上的端口。 maps a service port to backend containerPorts
从port和nodePort上到来的数据最终经过kube-proxy流入到后端pod的targetPort上进入容器。

###### pod yaml里定義的各種port #######
containerPort  容器需要監聽的端口號
hostPort 容器所在主機需要監聽的端口號，默認與containerPort相同

Endpoint= Pod IP + containerPort

容易混淆的概念：
1、NodePort和port

前者是将服务暴露给外部用户使用并在node上、后者则是为内部组件相互通信提供服务的，是在service上的端口。

2、targetPort
targetPort是pod上的端口，用来将pod内的container与外部进行通信的端口

3、port、NodePort、ContainerPort和targetPort在哪儿？

port在service上，负责处理对内的通信，clusterIP:port

NodePort在node上，负责对外通信，NodeIP:NodePort

ContainerPort在容器上，用于被pod绑定

targetPort在pod上、负责与kube-proxy代理的port和Nodeport数据进行通信

######綁定機器運行pod######
A toleration does not mean that the pod must be scheduled on a node with such taints. It means that the pod tolerates such a taint.
 If you want your pod to be "attracted" to specific nodes you will need to attach a label to your dedicated=master tainted nodes 
 and set nodeSelector in the pod to look for such label.

Attach the label to each of your special use nodes:

# kubectl label nodes name_of_your_node dedicated=master
Kubernetes 1.6 and above syntax
Add the nodeSelector to your pod:
'''
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
spec:
  replicas: 3
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-lb
        name: nginx-ingress-lb
      annotations:
    spec:
      nodeSelector:
        dedicated: master
      tolerations:
        key: dedicated
        operator: Equal
        value: master
        effect: NoSchedule
    […]
'''
 ###############     
下面生成客户端私钥和证书：

# openssl genrsa -out dashboard.key 2048
# chmod 600 dashboard.key
# openssl req -new -key dashboard.key -subj "/CN=10.67.37.234" -out dashboard.csr
# openssl x509 -req -in dashboard.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out dashboard.crt -days 365
# openssl verify -CAfile /etc/kubernetes/pki/ca.crt dashboard.crt

############murano image udpate metadata ##########
nova image-meta 2c74763a-22f2-41a8-a205-7d9727b9bb6a set murano_image_info='{"title": "ubuntu14.04-x64-kubernetes", "type": "linux.kubernetes"}'

###############kubernetes1.5 install on centos7##################
1 repo
現在使用的是centos7 extra repo

2 yum install
yum install docker etcd kubernetes
for SERVICE in docker etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet; do systemctl enable $SERVICE;done
for SERVICE in docker etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet; do systemctl restart $SERVICE;done
yum install cockpit cockpit-kubernetes

3 remove ServiceAccount    
###解決如下錯誤：
###Error from server: error when creating "nginx-pod.yaml": Pod "nginx" is forbidden:
###no API token found for service account default/default,
###retry after the token is automatically created and added to the service account
vi /etc/kubernetes/apiserver
# default admission control policies
KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota"

4 手動添加pod-infra-container-image: registry.access.redhat.com/rhel7/pod-infrastructure:latest
###这个是一个基础容器，每一个Pod启动的时候都会启动一个这样的容器。
如果你的本地没有这个镜像，kubelet会连接外网把这个镜像下载下来
###registry server上操作
docker pull registry.access.redhat.com/rhel7/pod-infrastructure:latest
docker save -o pod-infrastructure.tar registry.access.redhat.com/rhel7/pod-infrastructure
gzip pod-infrastructure.tar
scp pod-infrastructure.tar.gz 10.67.44.110:~
###kub 上導入image
docker load -i pod-infrastructure.tar.gz

5 指定insecure-registry 
###訪問私有registry server
vi /etc/sysconfig/docker
INSECURE_REGISTRY='--insecure-registry 10.67.51.161:5000'

6 創建kube secret key
###
kubectl create secret docker-registry registrykey-51-161 --docker-server=10.67.51.161:5000 --docker-username=root --docker-password=Foxconn123 --docker-email=team@domain.com

#################################kubernets 使用###########################################
##########使用rc-nginx.yml創建rc#####
###定義rc
[root@kub ~]# cat rc-nginx.yml
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-nginx-2
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx-2
    spec:
      containers:
      - name: nginx-2
        image: 10.67.51.161:5000/chensen/nginx:1.11
        ports:
        - containerPort: 80
      imagePullSecrets:
      - name: registrykey-51-161
###使用rc-nginx.yml創建rc,pod
kubectl create -f rc-nginx.yml
###使用rc-nginx.yml銷毀rc,pod
kubectl delete -f rc-nginx.yml

##########使用svc-nginx.yml為rc-nginx創建服務##########
###定義
[root@kub ~]# cat svc-nginx.yml
apiVersion: v1
kind: Service
metadata:
  name: nginxsvc
spec:
  ports:
  - port: 8081
    targetPort: 80
  selector:
    app: nginx-2
###創建service
kubectl create -f svc-nginx.yml
###get svc
[root@kub ~]# kubectl get svc
NAME         CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes   10.254.0.1     <none>        443/TCP    4d
nginxsvc     10.254.14.10   <none>        8081/TCP   10s
---------------kub describe----------------------------------
[root@kub ~]# kubectl describe svc nginxsvc
Name:                   nginxsvc
Namespace:              default
Labels:                 <none>
Selector:               app=nginx-2
Type:                   ClusterIP
IP:                     10.254.14.10
Port:                   <unset> 8081/TCP
Endpoints:              172.17.0.3:80,172.17.0.4:80
Session Affinity:       None

--------------------curl 測試------------------------------
[root@kub ~]# curl 10.254.14.10:8081

##############etcdctl command#############
etcdctl ls /registry/services/specs/default
etcdctl get /registry/services/specs/default/nginxsvc

-----------------Docker & K8s 網絡----------------------
在Docker 1.9 出世前，跨多主机的容器通信方案大致有如下三种：

1、端口映射
将宿主机A的端口P映射到容器C的网络空间监听的端口P’上，仅提供四层及以上应用和服务使用。
这样其他主机上的容器通过访问宿主机A的端口P实 现与容器C的通信。显然这个方案的应用场景很有局限。

2、将物理网卡桥接到虚拟网桥，使得容器与宿主机配置在同一网段下
在各个宿主机上都建立一个新虚拟网桥设备br0，将各自物理网卡eth0桥接br0上，
eth0的IP地址赋给br0；同时修改Docker daemon的DOCKER_OPTS，设置-b=br0（替代docker0），
并限制Container IP地址的分配范围为同物理段地址（–fixed-cidr）。
重启各个主机的Docker Daemon后，处于与宿主机在同一网段的Docker容器就可以实现跨主机访问了。
这个方案同样存在局限和扩展性差的问题：比如需将物理网段的地址划分 成小块，分布到各个主机上，
防止IP冲突；子网划分依赖物理交换机设置；Docker容器的主机地址空间大小依赖物理网络划分等。

3、使用第三方的基于SDN的方案：比如 使用Open vSwitch – OVS 或CoreOS的Flannel 等。
关于这些第三方方案的细节大家可以参考O’Reilly的《Docker Cookbook》 一书。

Docker在1.9版本中给大家带来了一种原生的跨多主机容器网络的解决方案，
该方案的实质是采用了基于VXLAN 的覆盖网技术。方案的使用有一些前提条件：
1、Linux Kernel版本 >= 3.16；
2、需要一个外部Key-value Store（官方例子中使用的是consul）；
3、各物理主机上的Docker Daemon需要一些特定的启动参数；
4、物理主机允许某些特定TCP/UDP端口可用。

本文将带着大家一起利用Docker 1.9.1创建一个跨多主机容器网络，并分析基于该网络的容器间通信原理。
VIP PaaS在接近两年时间里，基于kubernetes主要经历四次网络方案的变迁：
1. kubernetes + flannel
2. 基于Docker libnetwork的网络定制
3. kubernetes + contiv + kube-haproxy
4. 应用容器IP固定
在k8s + flannel的模型下，容器网络是封闭子网，可以提供平台内部应用之间基于4层和7层的调用，
同时对外部提供应用基于域名（工作在七层）的直接访问，但无法满足用户在平台外部需要直接使用IP访问的需求。
在flannel网络稳定使用后，开始研究network plugin以使应用服务实例以public IP 方式供用户直接使用。
当时docker的版本为1.8, 本身还不支持网络插件.同时 kubernetes本身提供一套基于CNI的网络插件, 但本身有bug
Docker 1.10版本支持指定IP启动容器，并且由于部分应用对实例IP固定有需求，我们开始着手容器IP固定方案的设计与开发


openssl genrsa -out ca-key.pem 2048
openssl req -x509 -new  -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"
openssl genrsa -out admin-key.pem 2048
openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"
openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365
kubectl config set-cluster default-cluster --server=http://10.67.37.233:8080 --certificate-authority=/root/ca.pem
kubectl config set-credentials default-admin --certificate-authority=/root/ca.pem --client-key=/root/admin-key.pem --client-certificate=/root/admin.pem
kubectl config set-context default-system --cluster=default-cluster --user=default-admin
kubectl config use-context default-system
kubectl cluster-info


##############kubectl常用命令##############
kubectl run my-nginx --image=nginx --replicas=1 --port=80
kubectl expose deployment my-nginx --target-port=80 --type=NodePort
kubectl describe service my-nginx
kubectl scale deployment my-nginx --replicas=2
$ kubectl patch statefulset/consul -p '{"spec":{"replicas": 5}}'
"consul" patched
kubectl scale statefulsets <stateful-set-name> --replicas=<new-replicas>
kubectl edit statefulsets <stateful-set-name>

