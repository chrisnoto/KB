#############Pod的Volume与PV的区别################
Volume的生命周期和Pod相同，Pod被删除时，Volume和保存在Volume中的数据就被删除了；
普通Volume和使用它的Pod之间是一种静态绑定关系，在定义Pod的文件里，同时定义了它使用的Volume。
Volume是Pod的附属品，我们无法单独创建一个Volume，因为它不是一个独立的K8S资源对象

PV, 是一个K8S资源对象，我们可以单独创建一个PV, 它不和Pod直接发生关系, 而是通过Persistent Volume Claim, 简称PVC来实现动态绑定, 
我们会在Pod定义里指定创建好的PVC, 然后PVC会根据Pod的要求去自动绑定合适的PV给Pod使用.
对于PV，即使挂载PV的Pod被删除了，PV仍然存在，PV上的数据也还在。
##############扩展 PV 空间#########
ExpandPersistentVolumes 在 v1.8 开始 Alpha，v1.11 升级为 Beta 版。
v1.8 开始支持扩展 PV 空间，支持在不丢失数据和重启容器的情况下扩展 PV 的大小。注意， 当前的实现仅支持不需要调整文件系统大小（XFS、Ext3、Ext4）的 PV，并且只支持以下几种存储插件 ：
AzureDisk
AzureFile
gcePersistentDisk
awsElasticBlockStore
Cinder
glusterfs
rbd
Portworx
开启扩展 PV 空间的功能需要配置
开启 ExpandPersistentVolumes 功能，即配置 --feature-gates=ExpandPersistentVolumes=true
开启准入控制插件 PersistentVolumeClaimResize，它只允许扩展明确配置 allowVolumeExpansion=true 的 StorageClass，比如
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gluster-vol-default
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://192.168.10.100:8080"
  restuser: ""
  secretNamespace: ""
  secretName: ""
allowVolumeExpansion: true
这样，用户就可以修改 PVC 中请求存储的大小（如通过 kubectl edit 命令）请求更大的存储空间
########Pod#########
Kubernetes中的基本组件kube-controller-manager就是用来控制Pod的状态和生命周期的，
在了解各种controller之前我们有必要先了解下Pod本身和其生命周期
同一个Pod中的容器共享存储、网络和容器运行配置项，它们总是被同时调度
Pod中可以共享两种资源：网络和存储

一个Pod多容器   ()内代表容器
[content manager]     [consumers]
(File Puller)        (Web Server)
           (Volume)
		   
########Pod的生命周期#######
Pod phase: Pending, Running, Succeeded, Failed, Unknown
Pod status: PodScheduled, Ready, Initialized, Unschedulable, ContainersReady		   
#######Pause容器的作用####
Kubernetes中的pause容器主要为每个业务容器提供以下功能：
在pod中担任Linux命名空间共享的基础；
启用pid命名空间，开启init进程。		
   
######Init 容器能做什么？#######
因为 Init 容器具有与应用程序容器分离的单独镜像，所以它们的启动相关代码具有如下优势：

它们可以包含并运行实用工具，但是出于安全考虑，是不建议在应用程序容器镜像中包含这些实用工具的。
它们可以包含使用工具和定制化代码来安装，但是不能出现在应用程序镜像中。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具。
应用程序镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。
Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。
它们必须在应用程序容器启动之前运行完成，而应用程序容器是并行运行的，所以 Init 容器能够提供了一种简单的阻塞或延迟应用容器的启动的方法，直到满足了一组先决条件。
示例		   
Init 容器不支持 Readiness Probe，因为它们必须在 Pod 就绪之前运行完成
Init 容器具有应用容器的所有字段。除了 readinessProbe，因为 Init 容器无法定义不同于完成（completion）的就绪（readiness）之外的其他状态。
这会在验证过程中强制执行。
API版本控制
支持watch机制 这意味着API Server的客户端可以使用与etcd相同的协调模式
在 Pod 上使用 activeDeadlineSeconds，在容器上使用 livenessProbe，这样能够避免 Init 容器一直失败。 这就为 Init 容器活跃设置了一个期限。

###################PDB#############
PDB是为了Voluntary Disruption时保障应用的高可用
Voluntary Disruption指用户或者集群管理员触发的，Kubernetes可控的Disruption场景
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: nginx-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: nginx
############preStop##############
lifecycle:
  preStop:
    exec:
      command: [
        "sh", "-c",
        # Introduce a delay to the shutdown sequence to wait for the
        # pod eviction event to propagate. Then, gracefully shutdown
        # nginx.
        "sleep 5 && /usr/sbin/nginx -s quit",
      ]
############postStart#############
    spec:
      containers:
      - env:
        - name: TZ
          value: Asia/Shanghai
        image: nginx:latest
        imagePullPolicy: Always
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - sed -i 's/nginx/apache/g' /usr/share/nginx/html/index.html
        name: web1
postStart和init container的差异
1 init container可以和app container不同image.  postStart只是pod起来后增加一段脚本或命令.  如果image里没有postStart所需的命令，那么只能用init container
2 init container保证执行顺序, postStart不能保证执行顺序 !!

#########该什么时候使用存活（liveness）和就绪（readiness）探针?#######
如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活探针; kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作。
如果您希望容器在探测失败时被杀死并重新启动，那么请指定一个存活探针，并指定restartPolicy 为 Always 或 OnFailure
liveness probes是用来检测你的应用程序是否正在运行。通常情况下，你的程序一崩溃，Kubernetes就会看到这个程序已经终止，然后重启这个程序。
但是liveness probes的目的就是捕捉到当程序还没有终止，还没有崩溃或者还没陷入死锁的情况
Readiness Probes是用来检查你的应用程序是否可以为通信服务

########API Server的具体的功能######
认证和授权
准入控制器
########kube-controller-manager####
节点控制器（Node Controller）
副本控制器（Replication Controller）
端点控制器（Endpoints Controller）
命名空间控制器（Namespace Controller）
身份认证控制器（Serviceaccounts Controller）
########cloud-controller-manager####
节点控制器: 用于检查云提供商以确定节点是否在云中停止响应后被删除
路由控制器: 用于在底层云基础架构中设置路由
服务控制器: 用于创建，更新和删除云提供商负载平衡器
数据卷控制器: 用于创建，附加和装载卷，并与云提供商进行交互以协调卷

########kube-proxy mode#######
默认 iptables,有性能瓶颈,  IPVS性能更高

#########label selector##########
在service、replicationcontroller等object中有对pod的label selector，使用方法只能使用等于操作，例如：
selector:
    component: redis
	
在Job、Deployment、ReplicaSet和DaemonSet这些object中，支持set-based的过滤，例如：
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}

另外在node affinity和pod affinity中的label selector的语法又有些许不同，示例如下：

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value

########rolling update过程#######
当我们更新这个 Deployment 的时候，它会创建一个新的 ReplicaSet（nginx-deployment-1564180365），将它扩容到1个replica，然后缩容原先的 ReplicaSet 到2个 replica，
此时满足至少2个 Pod 是可用状态，同一时刻最多有4个 Pod 处于创建的状态。

接着继续使用相同的 rolling update 策略扩容新的 ReplicaSet 和缩容旧的 ReplicaSet。最终，将会在新的 ReplicaSet 中有3个可用的 replica，旧的 ReplicaSet 的 replica 数目变成0	

###########部署策略###########
1 recreate: terminate the old version and release the new one
Recreate – best for development environment


2 ramped: release a new version on a rolling update fashion, one after the other
3 blue/green: release a new version alongside the old version then switch traffic
4 canary: release a new version to a subset of users, then proceed to a full rollout
5 a/b testing: release a new version to a subset of users in a precise way (HTTP headers, cookie, weight, etc.). 
A/B testing is really a technique for making business decisions based on statistics but we will briefly describe the process. 
This doesn’t come out of the box with Kubernetes, it implies extra work to setup a more advanced infrastructure (Istio, Linkerd, Traefik, custom nginx/haproxy, etc).	

###########stateful set 部署和scaling顺序保证##########
Deployment and Scaling Guarantees
For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.
When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.
Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.
Before a Pod is terminated, all of its successors must be completely shutdown.
The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practice is unsafe and strongly discouraged. 
For further explanation, please refer to force deleting StatefulSet Pods.

When the nginx example above is created, three Pods will be deployed in the order web-0, web-1, web-2. web-1 will not be deployed before web-0 is Running and Ready, 
and web-2 will not be deployed until web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before web-2 is launched, web-2 will not be launched 
until web-0 is successfully relaunched and becomes Running and Ready.

If a user were to scale the deployed example by patching the StatefulSet such that replicas=1, web-2 would be terminated first. web-1 would not be terminated until web-2 is fully 
shutdown and deleted. If web-0 were to fail after web-2 has been terminated and is completely shutdown, but prior to web-1’s termination, web-1 would not be terminated 
until web-0 is Running and Ready.

Pod Management Policies	

#############nginx ingress controller#########
加上tls证书和key             查看证书, 显示 签发者为 Kubernetes Ingress Controller Fake Certificate  不是我选择的cert



Ingress 的策略配置技巧
1 转发到单个后端服务上
2 同一域名下，不同的 URL 路径被转发到不同的服务上
3 不同的域名（虚拟主机名）被转发到不同的服务上
4 不使用域名的转发规则
    不使用域名的转发规则
    这种配置用于一个网站不使用域名直接提供服务的场景，此时通过任意一台运行 ingress-controller 的 Node 都能访问到后端的服务。

以上节的后端服务 webapp 为例，下面的配置将为：

webapp-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - http:
      paths:
      - path: /demo
        backend:
          serviceName: webapp
          servicePort: 8080
注意，使用无域名的 Ingress 转发规则时，将默认禁用非安全 HTTP，强制启用 HTTPS。例如，当使用 Nginx 作为 Ingress Controller 时，其配置文件 /etc/nginx/nginx.conf 中将会自动设置下面的规则，
将全部 HTTP 的访问请求直接返回 308 错误。
客户端使用 HTTP 访问将得到 308 的错误应答：

$ curl http://192.168.23.152/demo/
<html>
<head><title>308 Permanent Redirect</title></head>
<body bgcolor="white">
<center><h1>308 Permanent Redirect</h1></center>
<hr><center>nginx/1.13.7</center>
</body>
</html>
使用 HTTPS 将会访问成功：

$ curl https://192.168.23.152/demo/ -k

可以在 Ingress 的定义中设置一个 Annotation 来关闭强制启用 HTTPS 的设置：

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
	
###### service############
service带selector 会创建endpoints
service不带selector 不会创建endpoints,但可以自己定义endpoints，例如外部glusterfs cluster
如使用glusterfs pv,则配置会先创建glusterfs service不带selector，再创建glusterfs endpoints
如使用glusterfs storageclass,直接配置创建storageclass
An ExternalName service is a special case of service that does not have selectors and uses DNS names instead.
#If the external service has a valid domain name and you don’t need port remapping, then using the “ExternalName” service type is an easy and quick way to map the external service to an internal one. 
#If you don’t have a domain name or need to do port remapping, simply add the IP addresses to an endpoint and use that instead.
## Type ExternalName
Note: ExternalName Services are available only with kube-dns version 1.7 and later.
Services of type ExternalName map a service to a DNS name (specified using the spec.externalName parameter) rather than to a typical selector like my-service or cassandra. 
This Service definition, for example, would map the my-service Service in the prod namespace to my.database.example.com:

kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
When looking up the host my-service.prod.svc.cluster.local, the cluster DNS service will return a CNAME record with the value my.database.example.com. Accessing my-service works 
in the same way as other Services but with the crucial difference that redirection happens at the DNS level rather than via proxying or forwarding. Should you later decide to move your database into your cluster,
 you can start its pods, add appropriate selectors or endpoints, and change the service’s type.

Note: This section is indebted to the Kubernetes Tips - Part 1 blog post from Alen Komljen.

###External IPs
If there are external IPs that route to one or more cluster nodes, Kubernetes services can be exposed on those externalIPs. Traffic that ingresses into the cluster with the external IP 
(as destination IP), on the service port, will be routed to one of the service endpoints. externalIPs are not managed by Kubernetes and are the responsibility of the cluster administrator.

In the ServiceSpec, externalIPs can be specified along with any of the ServiceTypes. In the example below, “my-service” can be accessed by clients on “80.11.12.10:80”” (externalIP:port)

kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 9376
  externalIPs:
  - 80.11.12.10
  
###headless service###
Standard service - you will get the clusterIP value:

$ kubectl exec djangoapp-0 -- nslookup djangoapp 
Server: 10.0.0.12 
Address: 10.0.0.12#51
 
Name: djangoapp.default.svc.cluster.local 
Address: 10.0.0.210
Headless service - you will get the IP of each Pod:

$ kubectl exec djangoapp-0 -- nslookup djangoapp 
Server: 10.0.0.12 
Address: 10.0.0.12#51 
 
Name: djangoapp.default.svc.cluster.local 
Address: 172.17.0.1 
Name: djangoapp.default.svc.cluster.local 
Address: 172.17.0.2 
Name: djangoapp.default.svc.cluster.local 
Address: 172.17.0.3

Headless services
Sometimes you don’t need or want load-balancing and a single service IP. In this case, you can create “headless” services by specifying "None" for the cluster IP (.spec.clusterIP).
This option allows developers to reduce coupling to the Kubernetes system by allowing them freedom to do discovery their own way. Applications can still use a self-registration pattern and
adapters for other discovery systems could easily be built upon this API.
For such Services, a cluster IP is not allocated, kube-proxy does not handle these services, and there is no load balancing or proxying done by the platform for them.
How DNS is automatically configured depends on whether the service has selectors defined.

With selectors  A记录直接指向pod
For headless services that define selectors, the endpoints controller creates Endpoints records in the API, and modifies the DNS configuration to return A records (addresses) that point directly 
to the Pods backing the Service.

Without selectors
For headless services that do not define selectors, the endpoints controller does not create Endpoints records. However, the DNS system looks for and configures either:
CNAME records for ExternalName-type services.
A records for any Endpoints that share a name with the service, for all other types

Headless Service就是没头的Service。有啥用呢？很简单，有时候client想自己来决定使用哪个Real Server，可以通过查询DNS来获取Real Server的信息。
另外，Headless Services还有一个用处。Headless Service的对应的每一个Endpoints，即每一个Pod，都会有对应的DNS域名；这样Pod之间就可以互相访问。

Headless Service
在某些应用场景中，开发人员希望自己控制负载均衡的策略，不使用 Service 提供的默认负载均衡功能，或者应用程序希望知道属于同组服务的其他实例。Kubernetes 提供了 Headless Service（无头服务）来实现这种功能，
即不为 Service 设置 ClusterIP（入口 IP 地址），仅通过 Label Selector 将后端的 Pod 列表返回给调用的客户端

对应 “去中心化” 类的应用集群，Headless Service 将非常有用
Apache Cassandra 是一套开源分布式 NoSQL 数据库系统，主要特点为它不是单个数据库，而是由一组数据库节点工头构成的一个分布式的集群数据库。由于 Cassandra 使用的是 “去中心化” 模式，
所以在集群里的一个节点启动之后，需要一个途径获知集群中新节点的加入。Cassandra 使用了 Seed（种子）来完成在集群中节点之间的互相查找和通信。

通过对 Headless 的使用，实现了 Cassandra 各节点之间的互相查找和集群的自动搭建。主要步骤包括：自定义 SedProvider；通过 Headless Service 自动查找后端 Pod；自动添加新 Cassandra 节点。

#########访问服务的五种方式#########
1 kubernetes api 方式访问my-nginx
https://10.67.37.234:6443/api/v1/namespaces/default/services/my-nginx/proxy/
前提：浏览器导入证书
最新版的k8s默认启用了RBAC，并为未认证用户赋予了一个默认的身份：anonymous。

对於API Server来説，它是使用证书进行认证的，我们需要先创建一个证书：
1.首先找到kubectl命令的配置文档，默认情况下为/etc/kubernetes/admin.conf，在 上一篇 中，我们已经复制到了$HOME/.kube/config中。
2.然後我们使用client-certificate-data和client-key-data生成一个p12文档，可使用下列命令：
# 生成client-certificate-data
grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.crt

# 生成client-key-data
grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.key

# 生成p12
openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client"
3.最後导入上面生成的p12文档，重新打开浏览器，显示如下：
k8s-api-server-select-certificate

1.1 kubectl proxy方式访问my-nginx          proxies from a localhost address to the Kubernetes apiserver(在任意机器上kubectl proxy)
虽然我们从集群外部不能直接访问一个 ClusterIP 服务，但是你可以使用 Kubernetes Proxy API 来访问它。Kubernetes Proxy API 是一种特殊的 API，
Kube-APIServer 只是代理这类 API 的 HTTP 请求，然后将请求转发到某个节点上的 Kubelet 进程监听的端口上。最后实际是由该端口上的 REST API 响应请求。
比如：需要访问一个服务，可以使用 /api/v1/namespaces/<NAMESPACE>/services/<SERVICE-NAME>/proxy/
如果你需要直接访问一个 Pod，可以使用 /api/v1/namespaces/<NAMESPACE>/pods/<POD-NAME>/proxy/。
在 Master 节点上创建 Kubernetes API 的代理服务
kubectl proxy --address=0.0.0.0 --accept-hosts=^*$ --port=8443
http://10.67.37.234:8443/api/v1/namespaces/default/services/my-nginx/proxy/
前提： kubectl proxy --address=0.0.0.0 --accept-hosts=^*$ --port=8443
在cobbler节点上也可以创建kubernetes api的代理服务
[root@cobbler ~]# kubectl proxy --address=0.0.0.0 --accept-hosts=^*$ --port=8443
Starting to serve on [::]:8443
http://10.67.51.164:8443/api/v1/namespaces/default/services/web1/proxy/
2 apiserver proxy:  is a bastion built into the apiserver
Discovering builtin services
Typically, there are several services which are started on a cluster by kube-system.
$ kubectl cluster-info
  Kubernetes master is running at https://104.197.5.247
  elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
  kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy
  kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy
  grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
  heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
  
3 nodeport方式
前提：yaml文件里定义NodePort

4 loadbalancer方式    (balance workers:nodeport)
前提： 结合云平台

5 Ingress方式       http/https lb
前提： 配置域名解析    域名要指向backend service （clusterIP or NodePort）
Ingress 只是一个统称，其由 Ingress 和 Ingress Controller 两部分组成。Ingress 用作将原来需要手动配置的规则抽象成一个 Ingress 对象，使用 YAML 格式的文件来创建和管理。
Ingress Controller 用作通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化。
Kubernetes Ingress 提供了负载均衡器的典型特性：HTTP 路由、粘性会话、SSL 终止、SSL直通、TCP 和 UDP 负载平衡等

###########外部访问pod的方法########
1 hostNetwork: true
2 hostPort 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的IP加上 <hostPort> 来访问 Pod 了，如: <hostIP>:<hostPort>
3 Port Forward:  Forward one or more local ports to a pod
kubectl port-forward 指令来实现数据转发的方法。kubectl port-forward 命令可以为 Pod 设置端口转发，通过在本机指定监听端口，访问这些端口的请求将会被转发到 Pod 的容器中对应的端口上。
$ kubectl port-forward pod-name local-port:container-port
$ kubectl port-forward service/name_of_service local_port:remote_port
If only one port number is specified, it is used for both local and remote ports
注：需要在所有 Kubernetes 节点上都需要安装 Socat
由于这种类型的转发端口是绑定在本地的，这种方式也仅适用于调试服务

############proxy############
kubectl proxy       Run a proxy to the Kubernetes API server 将本机某个端口号映射到apiserver，  http
apiserver proxy     通过api方式访问pod, service等
kube proxy  =kubectl expose      通过iptable，使对service的请求转发到后端pod，或service 端口 NAT到worker的ip+ nodeport 端口
实现 Service 这一功能的关键是由 Kubernetes 中的 Kube-Proxy 来完成的。Kube-Proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过管理 IPtables 来实现网络的转发。
Kube Proxy 目前支持三种模式：UserSpace、IPtables、IPVS。
kubectl expose creates a service for existing pods
Under the hood: kube-proxy is using a userland proxy and a bunch of iptables rules.





#####################无状态服务、普通有状态服务和有状态集群服务######################################
在K8S运行的服务，从简单到复杂可以分成以上三类
下面分别来看K8S是如何运行这三类服务的。

- 无状态服务，K8S使用RC（或更新的Replica Set）来保证一个服务的实例数量，如果说某个Pod实例由于某种原因Crash了，
RC会立刻用这个Pod的模版新启一个Pod来替代它，由于是无状态的服务，新启的Pod与原来健康状态下的Pod一模一样。
在Pod被重建后它的IP地址可能发生变化，为了对外提供一个稳定的访问接口，K8S引入了Service的概念。一个Service后面可以挂多个Pod，实现服务的高可用。

- 普通有状态服务，和无状态服务相比，它多了状态保存的需求。Kubernetes提供了以Volume和Persistent Volume为基础的存储系统，可以实现服务的状态保存。

- 有状态集群服务，与普通有状态服务相比，它多了集群管理的需求。K8S为此开发了一套以Pet Set为核心的全新特性，方便了有状态集群服务在K8S上的部署和管理。
具体来说是通过Init Container来做集群的初始化工作，用 Headless Service 来维持集群成员的稳定关系，用动态存储供给来方便集群扩容，最后用Pet Set来综合管理整个集群。
要运行有状态集群服务要解决的问题有两个,一个是状态保存，另一个是集群管理。
 我们先来看如何解决第一个问题：状态保存。Kubernetes 有一套以Volume插件为基础的存储系统，通过这套存储系统可以实现应用和服务的状态保存。

K8S的存储系统从基础到高级又大致分为三个层次：普通Volume，Persistent Volume 和动态存储供应。
Kubernetes与有状态集群服务相关的两个新特性：Init Container 和 Pet Set  


我们在什么地方会用到 Init Container呢？

第一种场景是等待其它模块Ready，比如我们有一个应用里面有两个容器化的服务，一个是Web Server，另一个是数据库。
其中Web Server需要访问数据库。但是当我们启动这个应用的时候，并不能保证数据库服务先启动起来，所以可能出现在一段时间内Web Server有数据库连接错误。
为了解决这个问题，我们可以在运行Web Server服务的Pod里使用一个Init Container，去检查数据库是否准备好，直到数据库可以连接，
Init Container才结束退出，然后Web Server容器被启动，发起正式的数据库连接请求。

第二种场景是做初始化配置，比如集群里检测所有已经存在的成员节点，为主容器准备好集群的配置信息，这样主容器起来后就能用这个配置信息加入集群。

还有其它使用场景，如将pod注册到一个中央数据库、下载应用依赖等。
这些东西能够放到主容器里吗？从技术上来说能，但从设计上来说，可能不是一个好的设计。首先不符合单一职责原则，
其次这些操作是只执行一次的，如果放到主容器里，还需要特殊的检查来避免被执行多次。

什么是Pet Set？在数据结构里Set是集合的意思，所以顾名思义Pet Set就是Pet的集合，那什么是Pet呢？
我们提到过Cattle和Pet的概念，Cattle代表无状态服务，而Pet代表有状态服务。
具体在K8S资源对象里，Pet是一种需要特殊照顾的Pod。它有状态、有身份、当然也比普通的Pod要复杂一些。
Pet有三个特征。

一是有稳定的存储，这是通过我们前面介绍的PV/PVC 来实现的。
二是稳定的网络身份，这是通过一种叫 Headless Service 的特殊Service来实现的。
普通Service的Cluster IP 是对外的，用于外部访问多个Pod实例。而Headless Service的作用是对内的，
用于为一个集群内部的每个成员提供一个唯一的DNS名字，这样集群成员之间就能相互通信了。
所以Headless Service没有Cluster IP，这是它和普通Service的区别。
三是序号命名规则。Pet是一种特殊的Pod，那么Pet能不能用Pod的命名规则呢？
答案是不能，因为Pod的名字是不稳定的。Pod的命名规则是，如果一个Pod是由一个RC创建的，那么Pod的名字是RC的名字加上一个随机字符串。
为了解决名字不稳定的问题，K8S对Pet的名字不再使用随机字符串，而是为每个Pet分配一个唯一不变的序号，
比如 Pet Set 的名字叫 mysql，那么第一个启起来的Pet就叫 mysql-0，第二个叫 mysql-1，如此下去。
当一个Pet down 掉后，新创建的Pet 会被赋予跟原来Pet一样的名字。由于Pet名字不变所以DNS名字也跟以前一样，同时通过名字还能匹配到原来Pet用到的存储，实现状态保存。

更新 ConfigMap 后：

使用该 ConfigMap 挂载的 Env 不会同步更新
使用该 ConfigMap 挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新
ENV 是在容器启动的时候注入的，启动之后 kubernetes 就不会再改变环境变量的值，且同一个 namespace 中的 pod 的环境变量是不断累加的，
参考 Kubernetes中的服务发现与docker容器间的环境变量传递源码探究。为了更新容器中使用 ConfigMap 挂载的配置，
可以通过滚动更新 pod 的方式来强制重新挂载 ConfigMap，也可以在更新了 ConfigMap 后，先将副本数设置为 0，然后再扩容。

###########Control Panel 核心概念 Pod###########

Kubernetes Control Panel 核心概念是 Pod，大家都知道 Pod 的第一作用是帮我们解耦容器的关系，第二个作用是它是 Kubernetes 里最原子的调度单位，
并且 Pod 的Container 是 Share Namepsace 的，包括 Network、IPC。并且里面的 Pod 容器是 Shear Volume 的，这些东西有什么意义吗？马上会介绍。
但是最终的目的是一个，就是希望在所谓的容器云里能够引入进程组的概念，而不是管理单一的容器或者进程。

Pod 抑制制作“胖”容器
如果说 Pod 有什么作用？首先，第一个 Pod 或者谷歌希望做的事情就是抑制住你去做一个“胖”容器的冲动。什么是“胖”容器？比如说很多人在一个容器里跑很多进程，
用一个 Systemd 去管。其实这是很麻烦的事情，因为监控和整个应用生命周期的管理，都会和真实的应用进程不一致。这个事怎么解决，很棘手。所以 Kubernetes 不希望制作这种“胖”容器。

我经常举的例子是一个 JAVA 的应用，它是有两个东西组成，一个 War 包和 Tomcat。如果不用 Pod，可以把它们两个打包进一个镜像，然后一起 Run，可以，但这时候他们俩耦合在一起了，
我怎么独立更新他们两个，我不可能每次更新 Tomcat 都把线上的镜像都重新 Build 一遍吧。这是第一个事。
我们也可以维护一个 Persistent Volume 。在 Kubernetes 里可以这样做，就是两个容器，两个镜像。一个镜像存一个 War 包，另一个存 Tomcat。接下来怎么定义呢？就是一个 Pod Run 这两个容器，
其中 War 包容会定义成一个 InitContainer。initContainer 的特点是什么呢？它会先于所有的用户容器启动，并且按顺序执行完。这样的话在 War 包容器运行这样一个命令，叫 Copy War 包到一个指定目录，
这个指定目录是一个 Pod 的 Volume。执行了这个之后，War 包容器就可以退出了。

接下来再执行用户容器是 Tomact。Tomcat 就是一个正常的标准 Tomcat 的镜像，执行命令也不变。只不过它会去 Mount 刚刚 Copy 过文件的目录 Volume，到 Webapps 目录下。
这时候再去启动 Tomcat，我的 War 包就已经存在于 App 目录下，成功了。这就是一个解耦容器关系非常典型的例子。


Pod ：容器设计模式
第二个 Pod 所希望给的价值是所谓的容器设计模式，大家可能已经经常提到了，说的是什么呢？
举一个最简单的例子就是 Sidecar 模式。例子非常明确，就是说现在有 Helloworld ，会在 /var/log 下面写日志。然后有一个 Fluentd ，希望把 /var/log 里的内容文件读出来，
发到 ES 里。像这样的两个容器显然就构成了一个 Pod 的协作关系。所以，我会在一个 Pod 定义两个容器，并且因为他们的 Volume 是 Share 的，我就 Share 这样的 /var/log 目录就好了。
所以，这时候，他们之间的文件交换，Helloworld 写一个文件，不需要进行任何的 Copy 和文件交换的操作，这就是 Pod 的文件交互模式，如何应用在生产环境的例子。

但这还没有完，更高级的做法在 Kubernetes 1.7、1.8 之后，我们称之为 Initializer 的模式。比如说现在还是一个 Helloworld，希望里面有一个  Log Agent 运行，
可是这时候没有必要在所有的 Helloworld 都定义一个 Log Agent 容器。因为没有必要，机械的重复劳动不需要。

怎么办呢？Pod 里就定义一个 Helloworld 就可以了，然后声明一下，启动之前要加载一个 Initializer ，它的名字叫 Log Agent。这个 Log Agent 定义在哪儿呢？我额外定义了一个 Config，
这个 Config 的内容就是 Log Agent 这个容器的 yaml 的描述，就是蓝色图框所示的部分。这个 Config 是存在 Etcd 里的，全局存起来。
这样的话，在 Kubernetes 里启动这个 Helloworld Pod，Kubernetes 会自动在这个 Pod 里注入一个容器，名叫 Log Agent，使用的数据就是蓝色图框的内容。所以这个起来之后，
本来定义的是一个容器，起来之后里面其实有两个容器，另一个是 Kubernetes 帮你注入的，这个就是 Initializer 。但这还没有完。
还有一个更高级的做法，大师级的做法，就是 Istio。这个项目非常火，这个是做什么的呢？它是面向容器的微服务框架，他和 Spring 做的工作非常像，比如说服务拆分、流量控制。
两个或者是多个服务之间的访问授权，以及他们之间的流量策略控制，都是交给 Istio 做的。

它是怎么实现的？想想 Spring 为什么能做这个？因为 Spring 有两个王牌，IOC 和 AOP，大家都知道，依赖注入和切面编程。同样这个理念在 Pod 的帮助下也能实现在容器世界里。怎么实现呢？
Istio 里会在每个 Pod 会注入 Envoy 容器，是自动注入的，就和 Spring 切面编程在代码执行前注入逻辑的道理是一样的。
这个 Envoy 容器是干什么的？就可以理解为高级版的 Nginx，是一个双向的 Ingress 和 Egress 都支持的 Proxy 。有了这个，大家都记得 Pod 里的两个容器之间的 Network Namespace 又是共享的，
我就可以让这个 Pod 的进出流量都走 Envoy。Envoy 是系统自动注入进去的，用户不需要定义，所以接下来当我要控制两个服务器之间的访问，比说流量、切片，就是 Istio 自动配置每个 Pod 的里 Envoy 的容器实现。
这里举个例子。现在有这样的微服务，有微服务 A，还有一个微服务 B。他们两个之间有一个 A 调用 B 的关系，这时候要做一个灰度发布，在线上新部署了一个B 的新版本叫 Version 2，
我的要求是 A 访问 B 的流量，99% 还在原来的 V1 上，切 1% 到新版本 V2 上。怎么做？只要在 Istio 里申明要 A、B 之间的流量比例，Istio 会自动配置 A 的 Envoy 容器，使得出来的容量分流一部分到 V1 上，
另一部分按照比例分到 V2 上。这就是 Kubernetes 如何在容器这个世界里实现类似 AOP 的切面控制的原理。
这个项目是谷歌，Lyft 和 IBM 一起做的，其实可以认为它就是一个源自于 Kubernetes 的项目，因为它必须依赖于 Pod 的概念。同样这样的实现在 Cloud Foundry 也可以做，因为Cloud Foundry 的容器是“胖”容器，
本身里面有一个 Init 进程，如果在 Init 帮助下起一个 Envoy 的小容器，小的 Namespace ，这个是可以的。同样这个事在虚拟机里也可以做，无非就是在一个虚拟机启动多个容器的意思。但这个事在现在的 Docker Swarm 模式里就很难做，
没办法在 Docker 容器里说注入一个 Envoy，Swarm 模式的网络也不支持这么干。
当然谷歌对这个事很 Open，一周前 Istio 的人和 Docker Team 就开了一个会，想把 Istio 推到 Swarm 模式 上。最终的结果就是宿主机上 Workaround，比如说现在要做一个切面， Swarm 模式 会给你在宿主机上起一个匿名容器，
就是你不知道也看不见的容器，用来跑 Envoy。所以大家可以感觉到，它其实游离于当前 Schedule 掌控之外的，带来了管理的复杂度和资源管理的问题（比如怎么原子调度），但是现在没别的办法，让 Swarm 支持 Pod 不太可能 。

Pod 的实现

然后我们讲讲 Pod 怎么实现。Kubernetes 的 Pod 实现强依赖于 CRI。我想在 Kubernetes 里创建一个 Pod，名字叫 Foo，有两个容器 A 和 B。它怎么做呢？Kubernetes 的会去 Call Kubelet 里 CRI 的一个 API 叫 RunPodSandbox。
Call 这个之后，如果是 Docker 做 Runtime 的话，，Docker 会启动一个 Container Foo，这个非常小的 Container，我们称之为 Inford Container，它是用来 Hold 整个 Pod 里的 Network Namespace ，其他的容器都去 Join 的一
个 Network Namespace ，就实现了容器间 Network Namespace 的共享。


############NodeAffinity Node 亲和性调度#############
NodeAffinity 意为 Node 亲和性的调度策略，是用于替换 NodeSelector 的全新调度策略。目前有两种节点亲和性表达。

RequiredDuringSchedulingIgnoredDuringExecution：必须满足指定的规则才可以调度 Pod 到 Node 上（功能与 nodeSelector 很像，但是使用的是不同的语法），相当于硬限制。
PreferredDuringSchedulingIgnoredDuringExecution：强调优先满足指定规则，强调器会尝试调度 Pod 到 Node 上，但并不强求，相当于软限制。多个优先级规则还可以设置权重（weight）值，以定义执行的先后顺序。
IgnoredDuringExecution 的意思是：如果一个 Pod 所在的节点在 Pod 运行期间标签发生了变更，不在符合该 Pod 的节点亲和性要求，则系统将忽略 Node 上 Label 的变化，该 Pod 能继续在该节点运行。

下面的例子设置了 NodeAffinity 调度的如下规则。

requiredDuringSchedulingIgnoredDuringExecution 要求只运行在 amd64 的节点上（beta.kubernetes.io/arch In amd64）
preferredDuringSchedulingIgnoredDuringExecution 要求时尽量运行在 “磁盘类型为 SSD” (disk-type In ssd) 的节点上。
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - amd64
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disk-type
            operator: In
            values:
            - ssd
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
	
################## pod eviction ###############
从发起模块的角度，pod eviction 可以分为两类：

Kube-controller-manager: 周期性检查所有节点状态，当节点处于 NotReady 状态超过一段时间后，驱逐该节点上所有 pod。
Kubelet: 周期性检查本节点资源，当资源不足时，按照优先级驱逐部分 pod。
Kube-controller-manger 发起的驱逐
Kube-controller-manager 周期性检查节点状态，每当节点状态为 NotReady，并且超出 podEvictionTimeout 时间后，就把该节点上的 pod 全部驱逐到其它节点，其中具体驱逐速度还受驱逐速度参数，集群大小等的影响。
最常用的 2 个参数如下：

–pod-eviction-timeout：NotReady 状态节点超过该时间后，执行驱逐，默认 5 min。
–node-eviction-rate：驱逐速度，默认为 0.1 pod/秒
当某个 zone 故障节点的数目超过一定阈值时，采用二级驱逐速度进行驱逐。

–large-cluster-size-threshold：判断集群是否为大集群，默认为 50，即 50 个节点以上的集群为大集群。
–unhealthy-zone-threshold：故障节点数比例，默认为 55%
–secondary-node-eviction-rate：当大集群的故障节点超过 55% 时，采用二级驱逐速率，默认为 0.01 pod／秒。当小集群故障节点超过 55% 时，驱逐速率为 0 pod／秒。
Kubelet 发起的驱逐
Kubelet 周期性检查本节点的内存和磁盘资源，当可用资源低于阈值时，则按照优先级驱逐 pod，具体检查的资源如下：

memory.available
nodefs.available
nodefs.inodesFree
imagefs.available
imagefs.inodesFree
以内存资源为例，当内存资源低于阈值时，驱逐的优先级大体为 BestEffort > Burstable > Guaranteed，具体的顺序可能因实际使用量有所调整。当发生驱逐时，kubelet 支持 soft 和 hard 两种模式，
soft 模式表示缓期一段时间后驱逐，hard 模式表示立刻驱逐。
落地经验
对于 kubelet 发起的驱逐，往往是资源不足导致，它优先驱逐 BestEffort 类型的容器，这些容器多为离线批处理类业务，对可靠性要求低。驱逐后释放资源，减缓节点压力，弃卒保帅，保护了该节点的其它容器。
无论是从其设计出发，还是实际使用情况，该特性非常 nice。

对于由 kube-controller-manager 发起的驱逐，效果需要商榷。正常情况下，计算节点周期上报心跳给 master，如果心跳超时，则认为计算节点 NotReady，当 NotReady 状态达到一定时间后，kube-controller-manager 发起驱逐。
然而造成心跳超时的场景非常多，例如：

原生 bug：kubelet 进程彻底阻塞
误操作：误把 kubelet 停止
基础设施异常：如交换机故障演练，NTP 异常，DNS 异常
节点故障：硬件损坏，掉电等
从实际情况看，真正因计算节点故障造成心跳超时概率很低，反而由原生 bug，基础设施异常造成心跳超时的概率更大，造成不必要的驱逐。

理想的情况下，驱逐对无状态且设计良好的业务方影响很小。但是并非所有的业务方都是无状态的，也并非所有的业务方都针对 Kubernetes 优化其业务逻辑。例如，对于有状态的业务，如果没有共享存储，
异地重建后的 pod 完全丢失原有数据；即使数据不丢失，对于 Mysql 类的应用，如果出现双写，重则破坏数据。对于关心 IP 层的业务，异地重建后的 pod IP 往往会变化，虽然部分业务方可以利用 service 和 dns 来解决问题，
但是引入了额外的模块和复杂性。

除非满足如下需求，不然请尽量关闭 kube-controller-manager 的驱逐功能，即把驱逐的超时时间设置非常长，同时把一级／二级驱逐速度设置为 0。否则，非常容易就搞出大大小小的故障，血泪的教训。

业务方要用正确的姿势使用容器，如数据与逻辑分离，无状态化，增强对异常处理等
分布式存储
可靠的 Service／DNS 服务或者保持异地重建后的 IP 不变

##############
下面两种类型应用适合使用local volume。

数据缓存，应用可以就近访问数据，快速处理。
分布式存储系统，如分布式数据库Cassandra ，分布式文件系统ceph/gluster