########Pod#########
Kubernetes中的基本组件kube-controller-manager就是用来控制Pod的状态和生命周期的，
在了解各种controller之前我们有必要先了解下Pod本身和其生命周期
同一个Pod中的容器共享存储、网络和容器运行配置项，它们总是被同时调度
Pod中可以共享两种资源：网络和存储

一个Pod多容器   ()内代表容器
[content manager]     [consumers]
(File Puller)        (Web Server)
           (Volume)
		   
########Pod的生命周期#######
Pod phase: Pending, Running, Succeeded, Failed, Unknown
Pod status: PodScheduled, Ready, Initialized, Unschedulable, ContainersReady		   
#######Pause容器的作用####
Kubernetes中的pause容器主要为每个业务容器提供以下功能：
在pod中担任Linux命名空间共享的基础；
启用pid命名空间，开启init进程。		
   
######Init 容器能做什么？#######
因为 Init 容器具有与应用程序容器分离的单独镜像，所以它们的启动相关代码具有如下优势：

它们可以包含并运行实用工具，但是出于安全考虑，是不建议在应用程序容器镜像中包含这些实用工具的。
它们可以包含使用工具和定制化代码来安装，但是不能出现在应用程序镜像中。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具。
应用程序镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。
Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。
它们必须在应用程序容器启动之前运行完成，而应用程序容器是并行运行的，所以 Init 容器能够提供了一种简单的阻塞或延迟应用容器的启动的方法，直到满足了一组先决条件。
示例		   
Init 容器不支持 Readiness Probe，因为它们必须在 Pod 就绪之前运行完成
Init 容器具有应用容器的所有字段。除了 readinessProbe，因为 Init 容器无法定义不同于完成（completion）的就绪（readiness）之外的其他状态。
这会在验证过程中强制执行。
API版本控制
支持watch机制 这意味着API Server的客户端可以使用与etcd相同的协调模式
在 Pod 上使用 activeDeadlineSeconds，在容器上使用 livenessProbe，这样能够避免 Init 容器一直失败。 这就为 Init 容器活跃设置了一个期限。

############postStart#############
    spec:
      containers:
      - env:
        - name: TZ
          value: Asia/Shanghai
        image: nginx:latest
        imagePullPolicy: Always
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - sed -i 's/nginx/apache/g' /usr/share/nginx/html/index.html
        name: web1
postStart和init container的差异
1 init container可以和app container不同image.  postStart只是pod起来后增加一段脚本或命令.  如果image里没有postStart所需的命令，那么只能用init container
2 init container保证执行顺序, postStart不能保证执行顺序 !!

#########该什么时候使用存活（liveness）和就绪（readiness）探针?#######
如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活探针; kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作。
如果您希望容器在探测失败时被杀死并重新启动，那么请指定一个存活探针，并指定restartPolicy 为 Always 或 OnFailure
liveness probes是用来检测你的应用程序是否正在运行。通常情况下，你的程序一崩溃，Kubernetes就会看到这个程序已经终止，然后重启这个程序。
但是liveness probes的目的就是捕捉到当程序还没有终止，还没有崩溃或者还没陷入死锁的情况
Readiness Probes是用来检查你的应用程序是否可以为通信服务

########API Server的具体的功能######
认证和授权
准入控制器
########kube-controller-manager####
节点控制器（Node Controller）
副本控制器（Replication Controller）
端点控制器（Endpoints Controller）
命名空间控制器（Namespace Controller）
身份认证控制器（Serviceaccounts Controller）
########cloud-controller-manager####
节点控制器: 用于检查云提供商以确定节点是否在云中停止响应后被删除
路由控制器: 用于在底层云基础架构中设置路由
服务控制器: 用于创建，更新和删除云提供商负载平衡器
数据卷控制器: 用于创建，附加和装载卷，并与云提供商进行交互以协调卷

########kube-proxy mode#######
默认 iptables,有性能瓶颈,  IPVS性能更高

#########label selector##########
在service、replicationcontroller等object中有对pod的label selector，使用方法只能使用等于操作，例如：
selector:
    component: redis
	
在Job、Deployment、ReplicaSet和DaemonSet这些object中，支持set-based的过滤，例如：
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}

另外在node affinity和pod affinity中的label selector的语法又有些许不同，示例如下：

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value

########rolling update过程#######
当我们更新这个 Deployment 的时候，它会创建一个新的 ReplicaSet（nginx-deployment-1564180365），将它扩容到1个replica，然后缩容原先的 ReplicaSet 到2个 replica，
此时满足至少2个 Pod 是可用状态，同一时刻最多有4个 Pod 处于创建的状态。

接着继续使用相同的 rolling update 策略扩容新的 ReplicaSet 和缩容旧的 ReplicaSet。最终，将会在新的 ReplicaSet 中有3个可用的 replica，旧的 ReplicaSet 的 replica 数目变成0	

###########部署策略###########
1 recreate: terminate the old version and release the new one
Recreate – best for development environment


2 ramped: release a new version on a rolling update fashion, one after the other
3 blue/green: release a new version alongside the old version then switch traffic
4 canary: release a new version to a subset of users, then proceed to a full rollout
5 a/b testing: release a new version to a subset of users in a precise way (HTTP headers, cookie, weight, etc.). 
A/B testing is really a technique for making business decisions based on statistics but we will briefly describe the process. 
This doesn’t come out of the box with Kubernetes, it implies extra work to setup a more advanced infrastructure (Istio, Linkerd, Traefik, custom nginx/haproxy, etc).	

###########stateful set 部署和scaling顺序保证##########
Deployment and Scaling Guarantees
For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.
When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.
Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.
Before a Pod is terminated, all of its successors must be completely shutdown.
The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practice is unsafe and strongly discouraged. 
For further explanation, please refer to force deleting StatefulSet Pods.

When the nginx example above is created, three Pods will be deployed in the order web-0, web-1, web-2. web-1 will not be deployed before web-0 is Running and Ready, 
and web-2 will not be deployed until web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before web-2 is launched, web-2 will not be launched 
until web-0 is successfully relaunched and becomes Running and Ready.

If a user were to scale the deployed example by patching the StatefulSet such that replicas=1, web-2 would be terminated first. web-1 would not be terminated until web-2 is fully 
shutdown and deleted. If web-0 were to fail after web-2 has been terminated and is completely shutdown, but prior to web-1’s termination, web-1 would not be terminated 
until web-0 is Running and Ready.

Pod Management Policies	

#############nginx ingress controller#########
加上tls证书和key             查看证书, 显示 签发者为 Kubernetes Ingress Controller Fake Certificate  不是我选择的cert

不使用域名的转发规则
这种配置用于一个网站不使用域名直接提供服务的场景，此时通过任意一台运行 ingress-controller 的 Node 都能访问到后端的服务。

以上节的后端服务 webapp 为例，下面的配置将为：

webapp-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
  - http:
      paths:
      - path: /demo
        backend:
          serviceName: webapp
          servicePort: 8080
注意，使用无域名的 Ingress 转发规则时，将默认禁用非安全 HTTP，强制启用 HTTPS。例如，当使用 Nginx 作为 Ingress Controller 时，其配置文件 /etc/nginx/nginx.conf 中将会自动设置下面的规则，
将全部 HTTP 的访问请求直接返回 308 错误。
客户端使用 HTTP 访问将得到 308 的错误应答：

$ curl http://192.168.23.152/demo/
<html>
<head><title>308 Permanent Redirect</title></head>
<body bgcolor="white">
<center><h1>308 Permanent Redirect</h1></center>
<hr><center>nginx/1.13.7</center>
</body>
</html>
使用 HTTPS 将会访问成功：

$ curl https://192.168.23.152/demo/ -k

可以在 Ingress 的定义中设置一个 Annotation 来关闭强制启用 HTTPS 的设置：

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
	
###### service############
service带selector 会创建endpoints
service不带selector 不会创建endpoints,但可以自己定义endpoints，例如外部glusterfs cluster
如使用glusterfs pv,则配置会先创建glusterfs service不带selector，再创建glusterfs endpoints
如使用glusterfs storageclass,直接配置创建storageclass
An ExternalName service is a special case of service that does not have selectors and uses DNS names instead.
## Type ExternalName
Note: ExternalName Services are available only with kube-dns version 1.7 and later.
Services of type ExternalName map a service to a DNS name (specified using the spec.externalName parameter) rather than to a typical selector like my-service or cassandra. 
This Service definition, for example, would map the my-service Service in the prod namespace to my.database.example.com:

kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
When looking up the host my-service.prod.svc.cluster.local, the cluster DNS service will return a CNAME record with the value my.database.example.com. Accessing my-service works 
in the same way as other Services but with the crucial difference that redirection happens at the DNS level rather than via proxying or forwarding. Should you later decide to move your database into your cluster,
 you can start its pods, add appropriate selectors or endpoints, and change the service’s type.

Note: This section is indebted to the Kubernetes Tips - Part 1 blog post from Alen Komljen.

###External IPs
If there are external IPs that route to one or more cluster nodes, Kubernetes services can be exposed on those externalIPs. Traffic that ingresses into the cluster with the external IP 
(as destination IP), on the service port, will be routed to one of the service endpoints. externalIPs are not managed by Kubernetes and are the responsibility of the cluster administrator.

In the ServiceSpec, externalIPs can be specified along with any of the ServiceTypes. In the example below, “my-service” can be accessed by clients on “80.11.12.10:80”” (externalIP:port)

kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 9376
  externalIPs:
  - 80.11.12.10
  
###headless service###
Standard service - you will get the clusterIP value:

$ kubectl exec djangoapp-0 -- nslookup djangoapp 
Server: 10.0.0.12 
Address: 10.0.0.12#51
 
Name: djangoapp.default.svc.cluster.local 
Address: 10.0.0.210
Headless service - you will get the IP of each Pod:

$ kubectl exec djangoapp-0 -- nslookup djangoapp 
Server: 10.0.0.12 
Address: 10.0.0.12#51 
 
Name: djangoapp.default.svc.cluster.local 
Address: 172.17.0.1 
Name: djangoapp.default.svc.cluster.local 
Address: 172.17.0.2 
Name: djangoapp.default.svc.cluster.local 
Address: 172.17.0.3

Headless services
Sometimes you don’t need or want load-balancing and a single service IP. In this case, you can create “headless” services by specifying "None" for the cluster IP (.spec.clusterIP).
This option allows developers to reduce coupling to the Kubernetes system by allowing them freedom to do discovery their own way. Applications can still use a self-registration pattern and
adapters for other discovery systems could easily be built upon this API.
For such Services, a cluster IP is not allocated, kube-proxy does not handle these services, and there is no load balancing or proxying done by the platform for them.
How DNS is automatically configured depends on whether the service has selectors defined.

With selectors  A记录直接指向pod
For headless services that define selectors, the endpoints controller creates Endpoints records in the API, and modifies the DNS configuration to return A records (addresses) that point directly 
to the Pods backing the Service.

Without selectors
For headless services that do not define selectors, the endpoints controller does not create Endpoints records. However, the DNS system looks for and configures either:
CNAME records for ExternalName-type services.
A records for any Endpoints that share a name with the service, for all other types

Headless Service就是没头的Service。有啥用呢？很简单，有时候client想自己来决定使用哪个Real Server，可以通过查询DNS来获取Real Server的信息。
另外，Headless Services还有一个用处。Headless Service的对应的每一个Endpoints，即每一个Pod，都会有对应的DNS域名；这样Pod之间就可以互相访问。

Headless Service
在某些应用场景中，开发人员希望自己控制负载均衡的策略，不使用 Service 提供的默认负载均衡功能，或者应用程序希望知道属于同组服务的其他实例。Kubernetes 提供了 Headless Service（无头服务）来实现这种功能，
即不为 Service 设置 ClusterIP（入口 IP 地址），仅通过 Label Selector 将后端的 Pod 列表返回给调用的客户端

对应 “去中心化” 类的应用集群，Headless Service 将非常有用
Apache Cassandra 是一套开源分布式 NoSQL 数据库系统，主要特点为它不是单个数据库，而是由一组数据库节点工头构成的一个分布式的集群数据库。由于 Cassandra 使用的是 “去中心化” 模式，
所以在集群里的一个节点启动之后，需要一个途径获知集群中新节点的加入。Cassandra 使用了 Seed（种子）来完成在集群中节点之间的互相查找和通信。

通过对 Headless 的使用，实现了 Cassandra 各节点之间的互相查找和集群的自动搭建。主要步骤包括：自定义 SedProvider；通过 Headless Service 自动查找后端 Pod；自动添加新 Cassandra 节点。

#########訪問服務的五種方式#########
1 kubernetes api 方式訪問my-nginx
https://10.67.37.234:6443/api/v1/namespaces/default/services/my-nginx/proxy/
前提：瀏覽器導入證書
最新版的k8s默認啟用了RBAC，併為未認證用户賦予了一個默認的身份：anonymous。

對於API Server來説，它是使用證書進行認證的，我們需要先創建一個證書：
1.首先找到kubectl命令的配置文檔，默認情況下為/etc/kubernetes/admin.conf，在 上一篇 中，我們已經複製到了$HOME/.kube/config中。
2.然後我們使用client-certificate-data和client-key-data生成一個p12文檔，可使用下列命令：
# 生成client-certificate-data
grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.crt

# 生成client-key-data
grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.key

# 生成p12
openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client"
3.最後導入上面生成的p12文檔，重新打開瀏覽器，顯示如下：
k8s-api-server-select-certificate

1.1 kubectl proxy方式訪問my-nginx          proxies from a localhost address to the Kubernetes apiserver(在任意机器上kubectl proxy)
虽然我们从集群外部不能直接访问一个 ClusterIP 服务，但是你可以使用 Kubernetes Proxy API 来访问它。Kubernetes Proxy API 是一种特殊的 API，
Kube-APIServer 只是代理这类 API 的 HTTP 请求，然后将请求转发到某个节点上的 Kubelet 进程监听的端口上。最后实际是由该端口上的 REST API 响应请求。
比如：需要访问一个服务，可以使用 /api/v1/namespaces/<NAMESPACE>/services/<SERVICE-NAME>/proxy/
如果你需要直接访问一个 Pod，可以使用 /api/v1/namespaces/<NAMESPACE>/pods/<POD-NAME>/proxy/。
在 Master 节点上创建 Kubernetes API 的代理服务
kubectl proxy --address=0.0.0.0 --accept-hosts=^*$ --port=8443
http://10.67.37.234:8443/api/v1/namespaces/default/services/my-nginx/proxy/
前提： kubectl proxy --address=0.0.0.0 --accept-hosts=^*$ --port=8443
在cobbler节点上也可以创建kubernetes api的代理服务
[root@cobbler ~]# kubectl proxy --address=0.0.0.0 --accept-hosts=^*$ --port=8443
Starting to serve on [::]:8443
http://10.67.51.164:8443/api/v1/namespaces/default/services/web1/proxy/
2 apiserver proxy:  is a bastion built into the apiserver
Discovering builtin services
Typically, there are several services which are started on a cluster by kube-system.
$ kubectl cluster-info
  Kubernetes master is running at https://104.197.5.247
  elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
  kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy
  kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy
  grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
  heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
  
3 nodeport方式
前提：yaml文件里定義NodePort

4 loadbalancer方式    (balance workers:nodeport)
前提： 結合云平臺

5 Ingress方式       http/https lb
前提： 配置域名解析    域名要指向backend service （clusterIP or NodePort）
Ingress 只是一个统称，其由 Ingress 和 Ingress Controller 两部分组成。Ingress 用作将原来需要手动配置的规则抽象成一个 Ingress 对象，使用 YAML 格式的文件来创建和管理。
Ingress Controller 用作通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化。
Kubernetes Ingress 提供了负载均衡器的典型特性：HTTP 路由、粘性会话、SSL 终止、SSL直通、TCP 和 UDP 负载平衡等

###########外部访问pod的方法########
1 hostNetwork: true
2 hostPort 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的IP加上 <hostPort> 来访问 Pod 了，如: <hostIP>:<hostPort>
3 Port Forward:  Forward one or more local ports to a pod
kubectl port-forward 指令来实现数据转发的方法。kubectl port-forward 命令可以为 Pod 设置端口转发，通过在本机指定监听端口，访问这些端口的请求将会被转发到 Pod 的容器中对应的端口上。
$ kubectl port-forward pod-name local-port:container-port
$ kubectl port-forward service/name_of_service local_port:remote_port
If only one port number is specified, it is used for both local and remote ports
注：需要在所有 Kubernetes 节点上都需要安装 Socat
由于这种类型的转发端口是绑定在本地的，这种方式也仅适用于调试服务

############proxy############
kubectl proxy       Run a proxy to the Kubernetes API server 將本機某個端口號映射到apiserver，  http
apiserver proxy     通過api方式訪問pod, service等
kube proxy  =kubectl expose      通過iptable，使對service的請求轉發到後端pod，或service 端口 NAT到worker的ip+ nodeport 端口
实现 Service 这一功能的关键是由 Kubernetes 中的 Kube-Proxy 来完成的。Kube-Proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过管理 IPtables 来实现网络的转发。
Kube Proxy 目前支持三种模式：UserSpace、IPtables、IPVS。
kubectl expose creates a service for existing pods
Under the hood: kube-proxy is using a userland proxy and a bunch of iptables rules.