# 国内镜像站
docker image pull quay.io/kubevirt/virt-api:v0.45.0
# 换成
docker pull quay.mirrors.ustc.edu.cn/kubevirt/virt-api:v0.45.0

docker pull k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0
# 换成
docker pull registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.3.0

# 远程访问docker
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2376

# docker-compose 设置变量到 .env 文件
The .env file feature only works when you use the docker-compose up command and does not work with docker stack deploy

# docker-compose scale service
docker-compose up --scale redis-sentinel=3 -d
docker-compose up --scale redis-slave=2 -d

# dockerfile里取消环境变量
ENV http_proxy=
ENV https_proxy=


####### docker api #####
curl -Ss --unix-socket /var/run/docker.sock http:/containers/json |jq .
拉鏡像
[root@rancher mysql]# curl -Ss -X POST --unix-socket /var/run/docker.sock http:/images/create?fromImage=nginx:latest
{"status":"Pulling from library/nginx","id":"latest"}
{"status":"Pulling fs layer","progressDetail":{},"id":"a2abf6c4d29d"}
{"status":"Pulling fs layer","progressDetail":{},"id":"a9edb18cadd1"}
{"status":"Pulling fs layer","progressDetail":{},"id":"589b7251471a"}
{"status":"Pulling fs layer","progressDetail":{},"id":"186b1aaa4aa6"}
{"status":"Pulling fs layer","progressDetail":{},"id":"b4df32aa5a72"}
{"status":"Pulling fs layer","progressDetail":{},"id":"a0bcbecc962e"}
{"status":"Waiting","progressDetail":{},"id":"186b1aaa4aa6"}
{"status":"Waiting","progressDetail":{},"id":"a0bcbecc962e"}
{"status":"Waiting","progressDetail":{},"id":"b4df32aa5a72"}
{"status":"Downloading","progressDetail":{"current":602,"total":602},"progress":"[==================================================\u003e]     602B/602B","id":"589b7251471a"}
{"status":"Download complete","progressDetail":{},"id":"589b7251471a"}
........
{"status":"Digest: sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31"}
{"status":"Status: Downloaded newer image for nginx:latest"}

######## docker kill 发送信号 reload 配置 #####
Reloading config
If you used a bind mount for the config and have edited your haproxy.cfg file, you can use HAProxy's graceful reload feature by sending a SIGHUP to the container:

$ docker kill -s HUP my-running-haproxy

########### docker images 按大小排序 ##########
docker images --format '{{.Size}}\t{{.Repository}}:{{.Tag}}\t{{.ID}}' | sort -h -r | column -t

########## 使用docker network替代 --link #########
创建网络
docker network create wp-net
1 启动MySQL数据库
docker run -d -p 3306:3306 --name wp-mysql --network wp-net --network-alias mysql -e MYSQL_ROOT_PASSWORD=123 mysql
说明：
docker run：启动容器
-d：后台运行
-p 3306:3306：将容器的3306端口映射到宿主机的3306端口上
--name wp-mysql：指定容器的名称为wp-mysql
--network wp-net：将容器加入到wp-net网络中
--network-alias mysql：指定容器在wp-net网络中的别名是mysql
-e MYSQL_ROOT_PASSWORD=123：初始化数据库root用户的密码为123

2 启动WordPress网站程序
docker run -d -p 80:80 --name wp-web --network wp-net --network-alias wordpress -e WORDPRESS_DB_PASSWORD=123 wordpress
说明：
docker run：启动容器
-d：后台运行
-p 80:80：将容器内的80端口映射到宿主机的80端口上
--name wp-web：指定容器的名称为wp-web
--network wp-net：将容器加入到wp-net网络中，与mysql数据库在同个网络中
--network-alias wordpress：指定容器在网络中的别名是wordpress，这个可以不要，因为在其他容器中不需要这个名称
-e WORDPRESS_DB_PASSWORD=123：指定wordpress的数据库密码


########## 设置docker日志大小#########
cat > /etc/docker/daemon.json <<EOF
{
    "log-driver": "json-file",
    "log-opts": {
    "max-size": "50m",
    "max-file": "6"
    }
}
EOF


1 收集所有docker容器日志

docker默认日志为json-file, filebeat收集所有容器的json日志

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/lib/docker/containers/*/*json.log*
  json.message_key: log
  json.key_under_root: true
  processors:
  - add_docker_metadata: ~


2 收集指定docker容器日志

2.1 容器指定gelf地址
docker run --log-driver gelf --log-opt gelf-address=udp://localhost:12201

2.2 logstash开启gelf端口

logstash.conf
input {
  gelf {
    type => docker
    port => 12201
  }
}
output {
  stdout {}
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
  }
}

############多阶段构建##########
FROM golang:1.9.4 as build
WORKDIR /go/src/github.com/openfaas/faas/gateway
COPY .   .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o gateway .
FROM alpine:3.6
RUN addgroup -S app \    && adduser -S -g app app
WORKDIR /home/app
EXPOSE 8080
ENV http_proxy      ""
ENV https_proxy     ""
COPY --from=build /go/src/github.com/openfaas/faas/gateway/gateway    .
COPY assets     assets
RUN chown -R app:app ./
USER app
CMD ["./gateway"]

################docker容器注册到systemd##########
[root@fuel multi-user.target.wants]# cat docker-mcollective.service
[Unit]
Name=mcollective container
Requires=docker.service
After=docker.service docker-cobbler.service

[Service]
Restart=on-failure
RestartSec=10
StartLimitBurst=5
StartLimitInterval=60
ExecStartPre=/usr/bin/dockerctl create mcollective
ExecStartPre=/usr/bin/docker start fuel-core-8.0-mcollective
ExecStartPre=/usr/bin/dockerctl check mcollective
ExecStart=/usr/bin/docker attach --no-stdin=true fuel-core-8.0-mcollective
ExecStop=/usr/bin/docker stop -t 30 fuel-core-8.0-mcollective

[Install]
WantedBy=multi-user.target


#########没有安装git时,使用git容器代替git命令行###########
alias git='docker run -it --rm --name git -v $PWD:/git -w /git alpine/git' 
(This alias is only required if git is not already installed on your machine. This alias will allow you to clone the repo using a git container)
[root@worker3 ~]# docker run -ti --rm -v ${HOME}:/root -v $(pwd):/git -e https_proxy=http://10.67.9.210:3128 alpine/git clone https://github.com/alpine-docker/git.git   
Cloning into 'git'...
remote: Enumerating objects: 16, done.
remote: Counting objects: 100% (16/16), done.
remote: Compressing objects: 100% (13/13), done.
remote: Total 99 (delta 5), reused 10 (delta 3), pack-reused 83
Unpacking objects: 100% (99/99), done.


#########查看container config############非常有用
[root@worker2 ~]# docker inspect --format "{{json .Config}}" b1ee |python -mjson.tool

#######dockerd ###########
 You can listen on port 2375 on all network interfaces with -H tcp://0.0.0.0:2375, 
 or on a particular network interface using its IP address: -H tcp://192.168.59.103:2375. 
 It is conventional to use port 2375 for un-encrypted, 
 and port 2376 for encrypted communication with the daemon.
 Docker daemon可以通过3种socket来监听docker engine API:  unix, tcp, fd
 unix:   /var/run/docker.sock   dockerd -H unix:///var/run/docker.sock
 tcp:    tcp://0.0.0.0:2375     docker -H tcp://0.0.0.0:2375
 fd:     fd://                  dockerd -H fd://

######## docker stack 
###find containers in some namespace
[root@worker1 ~]# docker service ps yum_repo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE          ERROR               PORTS
n7cpfaxybol4        yum_repo.1          nginx:alpine        master              Running             Running 47 hours ago            
ihtetgpplhus        yum_repo.2          nginx:alpine        worker2             Running             Running 47 hours ago      
[root@worker1 ~]# docker stack ps yum
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE          ERROR               PORTS
n7cpfaxybol4        yum_repo.1          nginx:alpine        master              Running             Running 47 hours ago            
ihtetgpplhus        yum_repo.2          nginx:alpine        worker2             Running             Running 47 hours ago   
[root@cobbler ~]# ansible swarm -m shell -a "docker ps -a -f label=com.docker.stack.namespace=yum"
10.67.36.71 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
8dccb8a2f5fb        nginx:alpine        "nginx -g 'daemon of…"   47 hours ago        Up 47 hours         80/tcp              yum_rep                                                                                             o.1.n7cpfaxybol4xk3i7d0m6h0i4

10.67.36.70 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

10.67.36.68 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

10.67.36.69 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
a343a3da21a7        nginx:alpine        "nginx -g 'daemon of…"   47 hours ago        Up 47 hours         80/tcp              yum_rep                                                                                             o.2.ihtetgpplhus0u83yb12mk6nm

[root@cobbler ~]# ansible swarm -m shell -a "docker ps -a -f label=com.docker.stack.namespace=kafka"
10.67.36.71 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

10.67.36.68 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE                       COMMAND             CREATED             STATUS              PORTS               NAME                                                                                             S
10ab2ecb25c4        wurstmeister/kafka:latest   "start-kafka.sh"    2 days ago          Up 2 days                               kafk                                                                                             a_kafka-1.1.ukturjf483lq8z6rq8deluas5

10.67.36.69 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE                       COMMAND             CREATED             STATUS              PORTS               NAME                                                                                             S
98755036fbed        wurstmeister/kafka:latest   "start-kafka.sh"    2 days ago          Up 2 days                               kafk                                                                                             a_kafka-2.1.r67z84u6iwy4j6x8njm5dht9a

10.67.36.70 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE                       COMMAND             CREATED             STATUS              PORTS               NAME                                                                                             S
476203b3d3f4        wurstmeister/kafka:latest   "start-kafka.sh"    2 days ago          Up 2 days                               kafk                                                                                             a_kafka-3.1.s0vd2hewmp3nfllx0kng8qfet

####docker清理####
docker system prune
docker volume prune
docker image prune

#########docker清除无用的volume####
docker volume rm $(docker volume ls -qf dangling=true)

########docker volume dirver local ##########
# create a reusable volume
  $ docker volume create --driver local \
      --opt type=nfs \
      --opt o=nfsvers=4,addr=192.168.1.1,rw \
      --opt device=:/path/to/dir \
      foo

  # or from the docker run command
  $ docker run -it --rm \
    --mount type=volume,dst=/container/path,volume-driver=local,volume-opt=type=nfs,\"volume-opt=o=nfsvers=4,addr=192.168.1.1\",volume-opt=device=:/host/path \
    foo

  # or to create a service
  $ docker service create \
    --mount type=volume,dst=/container/path,volume-driver=local,volume-opt=type=nfs,\"volume-opt=o=nfsvers=4,addr=192.168.1.1\",volume-opt=device=:/host/path \
    foo

  # inside a docker-compose file
  ...
  volumes:
    nfs-data:
      driver: local
      driver_opts:
        type: nfs
        o: nfsvers=4,addr=192.168.1.1,rw
        device: ":/path/to/dir"
		
########docker plugin####
[root@worker2 docker]# cat daemon.json
{
  "registry-mirrors": ["http://10.67.51.161:5000"],
  "metrics-addr": "0.0.0.0:9323",
  "experimental": true,
  "log-driver": "fluentd",
  "log-opts": {
    "fluentd-address": "127.0.0.1:24224",
	"tag": "{{.ImageName}}/{{.Name}}/{{.ID}}"
  }
}
The gelf logging driver is a convenient format that is understood by a number of tools such as Graylog, Logstash, and Fluentd. Many tools use this format.
##########docker devicemapper  ##############
With Docker 17.06 and higher, Docker can manage the block device for you, simplifying configuration of direct-lvm mode. 
This is appropriate for fresh Docker setups only. You can only use a single block device. If you need to use multiple block devices, 
configure direct-lvm mode manually instead. 

[root@d1 centos]# cat /etc/docker/daemon.json
{
  "storage-driver": "devicemapper",
  "storage-opts": [
    "dm.directlvm_device=/dev/vdc",
    "dm.thinp_percent=95",
    "dm.thinp_metapercent=1",
    "dm.thinp_autoextend_threshold=80",
    "dm.thinp_autoextend_percent=20",
    "dm.directlvm_device_force=false"
  ]
}

[root@d1 centos]# lsblk
NAME                                                                                         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda                                                                                          253:0    0   40G  0 disk
└─vda1                                                                                       253:1    0   40G  0 part /
vdb                                                                                          253:16   0   64M  0 disk
vdc                                                                                          253:32   0   20G  0 disk
├─docker-thinpool_tmeta                                                                      252:0    0  204M  0 lvm
│ └─docker-thinpool                                                                          252:2    0   19G  0 lvm
│   ├─docker-253:1-29396177-4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb 252:3    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/4a5858
│   ├─docker-253:1-29396177-e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2 252:4    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/e824d2
│   └─docker-253:1-29396177-734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf 252:5    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/734e69
└─docker-thinpool_tdata                                                                      252:1    0   19G  0 lvm
  └─docker-thinpool                                                                          252:2    0   19G  0 lvm
    ├─docker-253:1-29396177-4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb 252:3    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/4a5858
    ├─docker-253:1-29396177-e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2 252:4    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/e824d2
    └─docker-253:1-29396177-734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf 252:5    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/734e69
[root@d1 centos]#

[root@d1 centos]# dmsetup ls
docker-253:1-29396177-4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb  (252:3)
docker-thinpool_tdata   (252:1)
docker-253:1-29396177-734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf  (252:5)
docker-253:1-29396177-e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2  (252:4)
docker-thinpool_tmeta   (252:0)
docker-thinpool (252:2)
[root@d1 centos]# df
Filesystem     1K-blocks    Used Available Use% Mounted on
/dev/vda1       41931756 2042180  39889576   5% /
devtmpfs          921044       0    921044   0% /dev
tmpfs             941684       0    941684   0% /dev/shm
tmpfs             941684   16984    924700   2% /run
tmpfs             941684       0    941684   0% /sys/fs/cgroup
tmpfs             188340       0    188340   0% /run/user/1000
/dev/dm-3       10467328  148440  10318888   2% /var/lib/docker/devicemapper/mnt/4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb
shm                65536       0     65536   0% /var/lib/docker/containers/c8a280ff78f670a6bc6ab3c5a9af90fe803b149b35c265ab3e80fa2660406ea5/mounts/shm
tmpfs             188340       0    188340   0% /run/user/0
/dev/dm-4       10467328  529996   9937332   6% /var/lib/docker/devicemapper/mnt/e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2
shm                65536       0     65536   0% /var/lib/docker/containers/67e939f706ea6c84925401e2bef3705927ee4d2f7db4b47aae7180bb138c872f/mounts/shm
/dev/dm-5       10467328  148460  10318868   2% /var/lib/docker/devicemapper/mnt/734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf
shm                65536       0     65536   0% /var/lib/docker/containers/af628326b4e98344de4b4989b12627e98dcb3d94d1793a31955e9cfa0a1248b0/mounts/shm
[root@d1 centos]# ll /dev/mapper
total 0
crw-------. 1 root root 10, 236 Dec 21 08:54 control
lrwxrwxrwx. 1 root root       7 Dec 21 08:59 docker-253:1-29396177-4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb -> ../dm-3
lrwxrwxrwx. 1 root root       7 Dec 21 09:15 docker-253:1-29396177-734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf -> ../dm-5
lrwxrwxrwx. 1 root root       7 Dec 21 09:12 docker-253:1-29396177-e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2 -> ../dm-4
lrwxrwxrwx. 1 root root       7 Dec 21 08:54 docker-thinpool -> ../dm-2
lrwxrwxrwx. 1 root root       7 Dec 21 08:54 docker-thinpool_tdata -> ../dm-1
lrwxrwxrwx. 1 root root       7 Dec 21 08:54 docker-thinpool_tmeta -> ../dm-0


#######change docker data path #####
ExecStart=/usr/bin/dockerd --graph=/data --insecure-registry=10.67.38.90         docker.service
新版 20.10.12 参数已变
ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock  --data-root=/data

########weave scope########
scope launch 10.67.36.68 10.67.36.69 10.67.36.70 10.67.36.71

######docker healthcheck#######
自 1.12 版本之后，Docker 引入了原生的健康检查实现，可以在 Dockerfile 中声明应用自身的健康检测配置。 HEALTHCHECK 指令声明了健康检测命令，
用这个命令来判断容器主进程的服务状态是否正常，从而比较真实的反应容器实际状态。
缺点是如果healthcheck用curl来检查，那么image里必须装curl命令
Dockerfile example
FROM elasticsearch:5.5
HEALTHCHECK --interval=5s --timeout=2s --retries=12 \
  CMD curl --silent --fail localhost:9200/_cluster/health || exit 1
  
Docker run example
$ docker run --rm -d \
    --name=elasticsearch \
    --health-cmd="curl --silent --fail localhost:9200/_cluster/health || exit 1" \
    --health-interval=5s \
    --health-retries=12 \
    --health-timeout=2s \
    elasticsearch:5.5
	
Composefile example
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost"]
  interval: 1m30s
  timeout: 10s
  retries: 3
  start_period: 40s

Docker Swarm mode command line
$ docker service create -d \
    --name=elasticsearch \
    --health-cmd="curl --silent --fail localhost:9200/_cluster/health || exit 1" \
    --health-interval=5s \
    --health-retries=12 \
    --health-timeout=2s \
    elasticsearch  
	
Docker service update	
[root@worker1 ~]# docker service update \
> --health-cmd "curl --silent -f http://10.67.36.71/ubuntu/ || exit 1" \
> --health-interval 5s \
> --health-retries 5 \
> yum_repo

##########docker设置timezone######
以下对centos有效
[root@registry ~]# docker run -it --rm  -e 'TZ=Asia/Shanghai' centos:6.10
[root@79e78b181715 /]# date
Mon Oct 22 15:15:59 CST 2018
[root@registry ~]# docker run -it --rm  -v /etc/localtime:/etc/localtime centos:6.10
以下对ubuntu有效
docker run -d -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime rancher/rancher:v2.0.8


############无法安装过期的docker-ce 17.03
A new obsoletes restriction was introduced in docker-ce 17.03.0 and it looks like the yum repo is applying it to all versions of the docker-ce indiscriminately. 
To work around this when installing older versions of docker-ce, you can pass a flag to ignore obsoletes:

$ yum install -y --setopt=obsoletes=0 docker-ce-17.03.2.ce 

docker stop e104
docker create --volumes-from e104f49bc05f --name rancher-stable-data rancher/rancher:v2.0.2
docker run -d --volumes-from rancher-stable-data --restart=unless-stopped -p 80:80 -p 443:443 -e "http_proxy=http://10.67.36.72:3128" -e "https_proxy=http://10.67.36.72:3128" rancher/rancher:stable


5. 使容器内时间与宿主机同步
我们下载的很多容器内的时区都是格林尼治时间，与北京时间差8小时，这将导致容器内的日志和文件创建时间与实际时区不符，有两种方式解决这个问题：

修改镜像中的时区配置文件
将宿主机的时区配置文件/etc/localtime使用volume方式挂载到容器中
第二种方式比较简单，不需要重做镜像，只要在应用的yaml文件中增加如下配置：

volumeMounts:
  - name: host-time
    mountPath: /etc/localtime
    readOnly: true
  volumes:
  - name: host-time
    hostPath:
      path: /etc/localtime

用docker inspect查看容器的volume
[root@rancher ~]# docker inspect --format '{{ .Mounts }}' 80c

docker-compose -f prometheus.yml build
docker-compose -f prometheus.yml push
docker stack deploy prometheus --compose-file prometheus.yml

Registry vs Index
The next weird thing is the idea of a Registry and an Index, and how these are separate things.
An index manages user accounts, permissions, search, tagging, and all that nice stuff that's in the public web interface.

A registry stores and serves up the actual image assets, and it delegates authentication to the index.

When you run docker search, it's searching the index, not the registry. In fact, it might be searching multiple registries that the index is aware of.

When you run docker push or docker pull, the index determines if you are allowed to access or modify the image, 
but the registry is the piece that stores it or sends it down the wire to you after the index approves the operation. 
Also, the index figures out which registry that particular image lives in and forwards the request appropriately.

Beyond that, when you're working locally and running commands like docker images, you're interacting with 
something that is neither an index or a registry, but a little of both.

#########docker rmi清除的是本地image####
清除步骤
untag  当有多個tag关联到同一個image ID
delete  只有一個tag关联到image ID

#########docker容器里使用apt-get####
export http_proxy=

#########build image时使用apt-get####
docker build -t curl:v1.0.0 --build-arg http_proxy=http://10.62.32.27:33128 .

docker build -t chrisnoto/mysql-client-alpine:v1.0.0 --build-arg http_proxy=http://10.62.32.27:33128 .

或dockerfile里设置
ENV http_proxy <HTTP_PROXY>
ENV https_proxy <HTTPS_PROXY>

############docker repo#################
[root@registry yum.repos.d]# cat docker.repo
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/7/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg

##############docker api##################
[root@cobbler ~]# curl http://10.67.51.161:5000/v2/_catalog
{"repositories":["chensen/nginx","chensen/ubuntu"]}
[root@cobbler ~]# curl http://10.67.51.161:5000/v2/chensen/ubuntu/tags/list
{"name":"chensen/ubuntu","tags":["14.04"]}
[root@cobbler ~]# curl http://10.67.51.161:5000/v2/chensen/nginx/tags/list
{"name":"chensen/nginx","tags":["1.11"]}


############setup docker proxy  for centos7.2 docker-engine1.13#######
[root@registry docker.service.d]# cat http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://h7108579:pqhkr88ctw@10.36.6.65:3128" "NO_PROXY=localhost,127.0.0.1,10.67.51.161,registry"
[root@registry docker.service.d]# pwd
/etc/systemd/system/docker.service.d

########### docker registry swift backend############
[root@registry ~]# cat start_registry.sh
#!/bin/bash
docker run -d --name registry -p 5000:5000 \
-e "REGISTRY_STORAGE=swift" \
-e "REGISTRY_STORAGE_SWIFT_USERNAME=admin" \
-e "REGISTRY_STORAGE_SWIFT_PASSWORD='F0xc0nn!23'" \
-e "REGISTRY_STORAGE_SWIFT_AUTHURL=https://10.67.44.66:5000/v2.0" \
-e "REGISTRY_STORAGE_SWIFT_CONTAINER=docker-image" \
-e "REGISTRY_STORAGE_SWIFT_REGION=RegionOne" \
-e "REGISTRY_STORAGE_SWIFT_TENANT=admin" \
-e "REGISTRY_STORAGE_SWIFT_TENANTID=31e6d008df414104ac5e1d42beae316c" \
-e "REGISTRY_STORAGE_SWIFT_INSECURESKIPVERIFY=true" \
registry:2.4

#############docker acc & insecure-registries############
docker daemon.json
[root@registry docker]# cat daemon.json
{
  "registry-mirrors": ["https://aasx6lzt.mirror.aliyuncs.com"],
  "insecure-registries":["10.67.51.161:5000"]
}

############docker push image##################
docker tag nginx 10.67.51.161:5000/chensen/nginx:1.11
docker push 10.67.51.161:5000/chensen/nginx

docker history ubuntu:14.04
docker history 10.67.51.161:5000/chensen/nginx:1.11

#############docker run#####################
docker run -d --name mynginx -p 8080:80 -it  10.67.51.161:5000/chensen/nginx:1.11
docker进入到正在运行的容器内部
docker exec -it test_redis_1 /bin/bash

用docker inspect查看容器的PID
[root@kub ~]# docker inspect --format '{{ .State.Pid }}' 978b5d190b7e
2703
若不存在/var/run/netns目录則创建它，在該目录下创建软链接
ln -s /proc/2703/ns/net /var/run/netns/k8s_nginx-2.549907a9_rc-nginx-2-ksj7g_default_79c4ff77-f804-11e6-a924-fa163e6c70f1_6fcc8701
測試是否成功
[root@kub ~]# ip netns
k8s_nginx-2.549907a9_rc-nginx-2-ksj7g_default_79c4ff77-f804-11e6-a924-fa163e6c70f1_6fcc8701 (id: 1)

docker镜像元数据
1 repository元数据      
/var/lib/docker/image/overlay2/repositories.json
{
    "Repositories": {
        "docker.io/alpine": {
            "docker.io/alpine:latest": "sha256:11cd0b38bc3ceb958ffb2f9bd70be3fb317ce7d255c8a4c3f4af30e298aa1aab",
            "docker.io/alpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430": "sha256:11cd0b38bc3ceb958ffb2f9bd70be3fb317ce7d255c8a4c3f4af30e298aa1aab",
            "docker.io/alpine@sha256:7b848083f93822dd21b0a2f14a110bd99f6efb4b838d499df6d04a49d0debf8b": "sha256:3fd9065eaf02feaf94d68376da52541925650b81698c53c6824d92ff63f98353"
        },
        "docker.io/httpd": {
            "docker.io/httpd:latest": "sha256:94af1f61475235154673372c1f46334c5601a6b182a818b15e6b519c479f9010",
            "docker.io/httpd@sha256:2edbf09d0dbdf2a3e21e4cb52f3385ad916c01dc2528868bc3499111cc54e937": "sha256:94af1f61475235154673372c1f46334c5601a6b182a818b15e6b519c479f9010"
        },
        "docker.io/nginx": {
            "docker.io/nginx:latest": "sha256:73acd1f0cfadf6f56d30351ac633056a4fb50d455fd95b229f564ff0a7adecda",
            "docker.io/nginx@sha256:23e4dacbc60479fa7f23b3b8e18aad41bd8445706d0538b25ba1d575a6e2410b": "sha256:36f3464a21975e5779d081a9e8a78a024c549d1895fc9981d6bd8b67075ebd7b"
        },

2 image元数据           如image history  env  exposedPorts volume  workingDir
/var/lib/docker/image/overlay2/imagedb/content/sha256/[image_id]
3 layer元数据
容器层/可读写层/mountedLayer
/var/lib/docker/image/overlay2/layerdb/mounts/[container_id]
roLayer/只读层
/var/lib/docker/image/overlay2/layerdb/sha256/[chainID]

Docker overlay/overlay存储驱动的目录结构          镜像层+容器层+容器init层

[root@worker2 overlay]# mount| grep 826
overlay on /var/lib/docker/overlay/8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00/merged type overlay (rw,relatime,
lowerdir=/var/lib/docker/overlay/285f2e23139c39f16e810cb617dc95d0e0a491073048d988b77720c581c96b54/root,
upperdir=/var/lib/docker/overlay/8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00/upper,
workdir=/var/lib/docker/overlay/8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00/work)
容器层目录
[root@worker2 overlay]# ls 8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00
lower-id  merged  upper  work
[root@worker2 overlay]# ls 8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00-init
lower-id  merged  upper  work
镜像层目录     (overlay)
[root@worker2 overlay]# ls fee35e5dc34aa817025dd1271b72cf5034789d4720c7f14df4fc3cf8e970dbbb/
root

Docker overlay/overlay2存储驱动 

[root@worker2 overlay2]# mount |grep overlay2
overlay on /var/lib/docker/overlay2/53fd96ea5f28fb08dda295bc8fb73ccb9889a798e219312f84790735757a453d/merged type overlay (rw,relatime,
lowerdir=/var/lib/docker/overlay2/l/PGO7LKP35KHVFZFGB5RE7SW66I:/var/lib/docker/overlay2/l/CDGM4CNSUUG22YUP5KQJL43KPZ:/var/lib/docker/overlay2/l/STHBNIBDZZF3MOJI6EJEVVQV7D:
/var/lib/docker/overlay2/l/YJUUMI6HNKVHMU4NJVBPLBX7WM:/var/lib/docker/overlay2/l/7PMEBTG4UY3LRFUCDBAOKLRWKC:/var/lib/docker/overlay2/l/TM6LZJET5MT75HTQR6BCDAR6HO:
/var/lib/docker overlay2/l/ODCAW6J5NR7UKMXXWNF34UVQX2:/var/lib/docker/overlay2/l/FHYLNDBYPIZIPDLJVJTIRXP46O:/var/lib/docker/overlay2/l/QH6CMH6GLBOHKWZCPP5W5IUHLQ:
/var/lib/docker/overlay2/l/GDCFNASK3B2TGNTABX4ZRH7JRS:/var/lib/docker/overlay2/l/AA4BWASGONR7L62N2RMWREBYVV,
upperdir=/var/lib/docker/overlay2/53fd96ea5f28fb08dda295bc8fb73ccb9889a798e219312f84790735757a453d/diff,
workdir=/var/lib/docker/overlay2/53fd96ea5f28fb08dda295bc8fb73ccb9889a798e219312f84790735757a453d/work)

/var/lib/docker/overlay2/l    取代了原先的镜像层目录
容器层目录有点变化
[root@worker2 53fd96ea5f28fb08dda295bc8fb73ccb9889a798e219312f84790735757a453d]# ls
diff  link  lower  merged  work


##### docker 特权模式
CRI(Container Runtime Interface) 中特权模式的说明如下：

// If set, run container in privileged mode.
// Privileged mode is incompatible with the following options. If
// privileged is set, the following features MAY have no effect:
// 1. capabilities
// 2. selinux_options
// 4. seccomp
// 5. apparmor
//
// Privileged mode implies the following specific options are applied:
// 1. All capabilities are added.
// 2. Sensitive paths, such as kernel module paths within sysfs, are not masked.
// 3. Any sysfs and procfs mounts are mounted RW.
// 4. AppArmor confinement is not applied.
// 5. Seccomp restrictions are not applied.
// 6. The device cgroup does not restrict access to any devices.
// 7. All devices from the host's /dev are available within the container.
// 8. SELinux restrictions are not applied (e.g. label=disabled).

###### linux capacities
简单说就是把root权限细分成很多更细的权限。capability可以作用在進程上(受限),也可以作用在程序文件上
CAP_CHOWN:修改文件屬主的權限
CAP_DAC_OVERRIDE:忽略文件的DAC訪問限制
CAP_DAC_READ_SEARCH:忽略文件讀及目錄搜索的DAC訪問限制
CAP_FOWNER：忽略文件屬主ID必須和進程用戶ID相匹配的限制
CAP_FSETID:允許設置文件的setuid位
CAP_KILL:允許對不屬於自己的進程發送信號
CAP_SETGID:允許改變進程的組ID
CAP_SETUID:允許改變進程的用戶ID
CAP_SETPCAP:允許向其他進程轉移能力以及刪除其他進程的能力
CAP_LINUX_IMMUTABLE:允許修改文件的IMMUTABLE和APPEND屬性標誌
CAP_NET_BIND_SERVICE:允許綁定到小於1024的端口
CAP_NET_BROADCAST:允許網絡廣播和多播訪問
CAP_NET_ADMIN:允許執行網絡管理任務                          
CAP_NET_RAW:允許使用原始套接字
CAP_IPC_LOCK:允許鎖定共享內存片段
CAP_IPC_OWNER:忽略IPC所有權檢查
CAP_SYS_MODULE:允許插入和刪除內核模塊
CAP_SYS_RAWIO:允許直接訪問/devport,/dev/mem,/dev/kmem及原始塊設備
CAP_SYS_CHROOT:允許使用chroot()系統調用
CAP_SYS_PTRACE:允許跟蹤任何進程
CAP_SYS_PACCT:允許執行進程的BSD式審計
CAP_SYS_ADMIN:允許執行系統管理任務，如加載或卸載文件系統、設置磁盤配額等
CAP_SYS_BOOT:允許重新啓動系統
CAP_SYS_NICE:允許提升優先級及設置其他進程的優先級
CAP_SYS_RESOURCE:忽略資源限制
CAP_SYS_TIME:允許改變系統時鐘
CAP_SYS_TTY_CONFIG:允許配置TTY設備
CAP_MKNOD:允許使用mknod()系統調用
CAP_LEASE:允許修改文件鎖的FL_LEASE標誌
查看capacities
[root@repo 8760]# getpcaps 8760
Capabilities for `8760': = cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap+eip

实例 ping
[root@vstjlinuxtrans1  ~]# getcap /usr/bin/ping   #原本的cap
/usr/bin/ping = cap_net_admin,cap_net_raw+p
[root@vstjlinuxtrans1  ~]# setcap  cap_net_admin+p /usr/bin/ping    #修改cap
[root@vstjlinuxtrans1  ~]# getcap /usr/bin/ping
/usr/bin/ping = cap_net_admin+p
[root@vstjlinuxtrans1  ~]# su - chensen
Last login: Mon Jun 13 16:27:06 CST 2022 on pts/1
[chensen@vstjlinuxtrans1  ~]$ ping 10.67.51.164
ping: socket: Operation not permitted

[root@vstjlinuxtrans1  ~]# setcap cap_net_raw=eip /usr/bin/ping          # 再次修改
[root@vstjlinuxtrans1  ~]# getcap /usr/bin/ping
/usr/bin/ping = cap_net_raw+eip
[root@vstjlinuxtrans1  ~]# su - chensen
Last login: Mon Jun 13 16:28:24 CST 2022 on pts/1
[chensen@vstjlinuxtrans1  ~]$ ping 10.67.51.164
PING 10.67.51.164 (10.67.51.164) 56(84) bytes of data.
64 bytes from 10.67.51.164: icmp_seq=1 ttl=64 time=0.270 ms
64 bytes from 10.67.51.164: icmp_seq=2 ttl=64 time=0.300 ms
^C
--- 10.67.51.164 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.270/0.285/0.300/0.015 ms

或者设置/usr/bin/ping = cap_net_admin,cap_net_raw+p
[root@vstjlinuxtrans1  ~]# setcap cap_net_admin,cap_net_raw+p /usr/bin/ping
[root@vstjlinuxtrans1  ~]# getcap /usr/bin/ping
/usr/bin/ping = cap_net_admin,cap_net_raw+p


实例route   普通用户无权限添加删除路由
[root@vstjlinuxtrans1  ~]# getcap `which route`
[root@vstjlinuxtrans1  ~]# which route
/sbin/route
[root@vstjlinuxtrans1  ~]# su - chensen
Last login: Mon Jun 13 16:29:49 CST 2022 on pts/1
[chensen@vstjlinuxtrans1  ~]$ /sbin/route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         gateway         0.0.0.0         UG    100    0        0 ens32
10.67.50.0      0.0.0.0         255.255.254.0   U     100    0        0 ens32
[chensen@vstjlinuxtrans1  ~]$ /sbin/route add -host 192.168.27.139 gw 192.168.27.2
SIOCADDRT: Operation not permitted
如果setcap cap_net_admin=eip /sbin/route
普通用户就可以增加路由了

实例 date
普通用戶不能改變系統時鐘,如下:
date -s 2012-01-01
date: cannot set date: Operation not permitted
Sun Jan  1 00:00:00 EST 2012

CAP_SYS_TIME可以幫助普通用戶改變系統時鐘,如下:
setcap cap_sys_time=eip /bin/date
切換到普通用戶再次改變時間,發現已經可以改變了
su - test
date -s 2012-01-01
Sun Jan  1 00:00:00 EST 2012
date
Sun Jan  1 00:00:02 EST 2012

###### seccomp 就是 syscall 防火墙
Seccomp（Secure Computing 安全计算模式的简称）是Linux内核中的一个安全特性，它可以限制用户态程序对syscall的使用，

######## mount /sys/fs/cgroup #######
如果希望在容器里运行systemd,那么需要以只读方式mount /sys/fs/cgroup
Containers running systemd need access to /sys/fs/cgroup. We should mount it into the container as a read only mount.
此外，还需要
Running systemd inside a Docker container
So to be able to easily troubleshoot this problem and quickly iterate over possible solutions I utilized Docker. To start systemd inside a Docker container a few pre-requisites have to be met:

systemd has to be installed inside the container of course. It provides e.g. the /sbin/init binary. Using the fedora:34 Docker image this can be achieved by installing httpd for example (the Apache web server).

The cgroup file system has to be mounted inside the container (-v /sys/fs/cgroup:/sys/fs/cgroup:ro). We do this in read-only mode.

The /tmp and /run mount points have to be present inside the container (--tmpfs /tmp --tmpfs /run).

So if you just want to get systemd and the Apache web server up and running inside a Docker container just clone the source code repository located here and execute the following command inside the folder
 running-systemd-inside-a-docker-container:

Build the container image: docker build . -t sysd

Start a container: docker run --tmpfs /tmp --tmpfs /run -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 9090:80 --name sysd --rm sysd

To get a shell inside the container execute the following command in a second shell: docker exec -it sysd bash

Now you may also open the URL http://localhost:9090/ in your web browser to view he page served by the Apache web server running as a child process of systemd inside the container.
