############多阶段构建##########
FROM golang:1.9.4 as build
WORKDIR /go/src/github.com/openfaas/faas/gateway
COPY .   .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o gateway .
FROM alpine:3.6
RUN addgroup -S app \    && adduser -S -g app app
WORKDIR /home/app
EXPOSE 8080
ENV http_proxy      ""
ENV https_proxy     ""
COPY --from=build /go/src/github.com/openfaas/faas/gateway/gateway    .
COPY assets     assets
RUN chown -R app:app ./
USER app
CMD ["./gateway"]
################docker容器注册到systemd##########
[root@fuel multi-user.target.wants]# cat docker-mcollective.service
[Unit]
Name=mcollective container
Requires=docker.service
After=docker.service docker-cobbler.service

[Service]
Restart=on-failure
RestartSec=10
StartLimitBurst=5
StartLimitInterval=60
ExecStartPre=/usr/bin/dockerctl create mcollective
ExecStartPre=/usr/bin/docker start fuel-core-8.0-mcollective
ExecStartPre=/usr/bin/dockerctl check mcollective
ExecStart=/usr/bin/docker attach --no-stdin=true fuel-core-8.0-mcollective
ExecStop=/usr/bin/docker stop -t 30 fuel-core-8.0-mcollective

[Install]
WantedBy=multi-user.target


#########技巧  没有安装git时,使用git容器代替git命令行###########
alias git='docker run -it --rm --name git -v $PWD:/git -w /git alpine/git' 
(This alias is only required if git is not already installed on your machine. This alias will allow you to clone the repo using a git container)
[root@worker3 ~]# docker run -ti --rm -v ${HOME}:/root -v $(pwd):/git -e https_proxy=http://10.67.9.210:3128 alpine/git clone https://github.com/alpine-docker/git.git   
Cloning into 'git'...
remote: Enumerating objects: 16, done.
remote: Counting objects: 100% (16/16), done.
remote: Compressing objects: 100% (13/13), done.
remote: Total 99 (delta 5), reused 10 (delta 3), pack-reused 83
Unpacking objects: 100% (99/99), done.


#########查看container config############非常有用
[root@worker2 ~]# docker inspect --format "{{json .Config}}" b1ee |python -mjson.tool

#######dockerd ###########
 You can listen on port 2375 on all network interfaces with -H tcp://0.0.0.0:2375, 
 or on a particular network interface using its IP address: -H tcp://192.168.59.103:2375. 
 It is conventional to use port 2375 for un-encrypted, 
 and port 2376 for encrypted communication with the daemon.
 Docker daemon可以通过3种socket来监听docker engine API:  unix, tcp, fd
 unix:   /var/run/docker.sock   dockerd -H unix:///var/run/docker.sock
 tcp:    tcp://0.0.0.0:2375     docker -H tcp://0.0.0.0:2375
 fd:     fd://                  dockerd -H fd://
 
###find containers in some namespace
[root@worker1 ~]# docker service ps yum_repo
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE          ERROR               PORTS
n7cpfaxybol4        yum_repo.1          nginx:alpine        master              Running             Running 47 hours ago            
ihtetgpplhus        yum_repo.2          nginx:alpine        worker2             Running             Running 47 hours ago      
[root@worker1 ~]# docker stack ps yum
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE          ERROR               PORTS
n7cpfaxybol4        yum_repo.1          nginx:alpine        master              Running             Running 47 hours ago            
ihtetgpplhus        yum_repo.2          nginx:alpine        worker2             Running             Running 47 hours ago   
[root@cobbler ~]# ansible swarm -m shell -a "docker ps -a -f label=com.docker.stack.namespace=yum"
10.67.36.71 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
8dccb8a2f5fb        nginx:alpine        "nginx -g 'daemon of…"   47 hours ago        Up 47 hours         80/tcp              yum_rep                                                                                             o.1.n7cpfaxybol4xk3i7d0m6h0i4

10.67.36.70 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

10.67.36.68 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

10.67.36.69 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
a343a3da21a7        nginx:alpine        "nginx -g 'daemon of…"   47 hours ago        Up 47 hours         80/tcp              yum_rep                                                                                             o.2.ihtetgpplhus0u83yb12mk6nm

[root@cobbler ~]# ansible swarm -m shell -a "docker ps -a -f label=com.docker.stack.namespace=kafka"
10.67.36.71 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

10.67.36.68 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE                       COMMAND             CREATED             STATUS              PORTS               NAME                                                                                             S
10ab2ecb25c4        wurstmeister/kafka:latest   "start-kafka.sh"    2 days ago          Up 2 days                               kafk                                                                                             a_kafka-1.1.ukturjf483lq8z6rq8deluas5

10.67.36.69 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE                       COMMAND             CREATED             STATUS              PORTS               NAME                                                                                             S
98755036fbed        wurstmeister/kafka:latest   "start-kafka.sh"    2 days ago          Up 2 days                               kafk                                                                                             a_kafka-2.1.r67z84u6iwy4j6x8njm5dht9a

10.67.36.70 | SUCCESS | rc=0 >>
CONTAINER ID        IMAGE                       COMMAND             CREATED             STATUS              PORTS               NAME                                                                                             S
476203b3d3f4        wurstmeister/kafka:latest   "start-kafka.sh"    2 days ago          Up 2 days                               kafk                                                                                             a_kafka-3.1.s0vd2hewmp3nfllx0kng8qfet

####docker清理####
docker system prune
docker volume prune
#########docker清除无用的volume####
docker volume rm $(docker volume ls -qf dangling=true)
########docker volume dirver local ##########
# create a reusable volume
  $ docker volume create --driver local \
      --opt type=nfs \
      --opt o=nfsvers=4,addr=192.168.1.1,rw \
      --opt device=:/path/to/dir \
      foo

  # or from the docker run command
  $ docker run -it --rm \
    --mount type=volume,dst=/container/path,volume-driver=local,volume-opt=type=nfs,\"volume-opt=o=nfsvers=4,addr=192.168.1.1\",volume-opt=device=:/host/path \
    foo

  # or to create a service
  $ docker service create \
    --mount type=volume,dst=/container/path,volume-driver=local,volume-opt=type=nfs,\"volume-opt=o=nfsvers=4,addr=192.168.1.1\",volume-opt=device=:/host/path \
    foo

  # inside a docker-compose file
  ...
  volumes:
    nfs-data:
      driver: local
      driver_opts:
        type: nfs
        o: nfsvers=4,addr=192.168.1.1,rw
        device: ":/path/to/dir"
########docker plugin####
[root@worker2 docker]# cat daemon.json
{
  "registry-mirrors": ["http://10.67.51.161:5000"],
  "metrics-addr": "0.0.0.0:9323",
  "experimental": true,
  "log-driver": "fluentd",
  "log-opts": {
    "fluentd-address": "127.0.0.1:24224",
	"tag": "{{.ImageName}}/{{.Name}}/{{.ID}}"
  }
}
The gelf logging driver is a convenient format that is understood by a number of tools such as Graylog, Logstash, and Fluentd. Many tools use this format.
##########docker devicemapper  ##############
With Docker 17.06 and higher, Docker can manage the block device for you, simplifying configuration of direct-lvm mode. 
This is appropriate for fresh Docker setups only. You can only use a single block device. If you need to use multiple block devices, 
configure direct-lvm mode manually instead. 

[root@d1 centos]# cat /etc/docker/daemon.json
{
  "storage-driver": "devicemapper",
  "storage-opts": [
    "dm.directlvm_device=/dev/vdc",
    "dm.thinp_percent=95",
    "dm.thinp_metapercent=1",
    "dm.thinp_autoextend_threshold=80",
    "dm.thinp_autoextend_percent=20",
    "dm.directlvm_device_force=false"
  ]
}

[root@d1 centos]# lsblk
NAME                                                                                         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda                                                                                          253:0    0   40G  0 disk
└─vda1                                                                                       253:1    0   40G  0 part /
vdb                                                                                          253:16   0   64M  0 disk
vdc                                                                                          253:32   0   20G  0 disk
├─docker-thinpool_tmeta                                                                      252:0    0  204M  0 lvm
│ └─docker-thinpool                                                                          252:2    0   19G  0 lvm
│   ├─docker-253:1-29396177-4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb 252:3    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/4a5858
│   ├─docker-253:1-29396177-e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2 252:4    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/e824d2
│   └─docker-253:1-29396177-734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf 252:5    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/734e69
└─docker-thinpool_tdata                                                                      252:1    0   19G  0 lvm
  └─docker-thinpool                                                                          252:2    0   19G  0 lvm
    ├─docker-253:1-29396177-4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb 252:3    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/4a5858
    ├─docker-253:1-29396177-e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2 252:4    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/e824d2
    └─docker-253:1-29396177-734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf 252:5    0   10G  0 dm   /var/lib/docker/devicemapper/mnt/734e69
[root@d1 centos]#

[root@d1 centos]# dmsetup ls
docker-253:1-29396177-4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb  (252:3)
docker-thinpool_tdata   (252:1)
docker-253:1-29396177-734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf  (252:5)
docker-253:1-29396177-e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2  (252:4)
docker-thinpool_tmeta   (252:0)
docker-thinpool (252:2)
[root@d1 centos]# df
Filesystem     1K-blocks    Used Available Use% Mounted on
/dev/vda1       41931756 2042180  39889576   5% /
devtmpfs          921044       0    921044   0% /dev
tmpfs             941684       0    941684   0% /dev/shm
tmpfs             941684   16984    924700   2% /run
tmpfs             941684       0    941684   0% /sys/fs/cgroup
tmpfs             188340       0    188340   0% /run/user/1000
/dev/dm-3       10467328  148440  10318888   2% /var/lib/docker/devicemapper/mnt/4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb
shm                65536       0     65536   0% /var/lib/docker/containers/c8a280ff78f670a6bc6ab3c5a9af90fe803b149b35c265ab3e80fa2660406ea5/mounts/shm
tmpfs             188340       0    188340   0% /run/user/0
/dev/dm-4       10467328  529996   9937332   6% /var/lib/docker/devicemapper/mnt/e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2
shm                65536       0     65536   0% /var/lib/docker/containers/67e939f706ea6c84925401e2bef3705927ee4d2f7db4b47aae7180bb138c872f/mounts/shm
/dev/dm-5       10467328  148460  10318868   2% /var/lib/docker/devicemapper/mnt/734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf
shm                65536       0     65536   0% /var/lib/docker/containers/af628326b4e98344de4b4989b12627e98dcb3d94d1793a31955e9cfa0a1248b0/mounts/shm
[root@d1 centos]# ll /dev/mapper
total 0
crw-------. 1 root root 10, 236 Dec 21 08:54 control
lrwxrwxrwx. 1 root root       7 Dec 21 08:59 docker-253:1-29396177-4a58582a85801e6923228bd1c74c52025270e15d8109861e6c1737799791c7bb -> ../dm-3
lrwxrwxrwx. 1 root root       7 Dec 21 09:15 docker-253:1-29396177-734e698eee7fcedff2266792975aee4ece1995f0e0fe349f618c32391a418fbf -> ../dm-5
lrwxrwxrwx. 1 root root       7 Dec 21 09:12 docker-253:1-29396177-e824d29bc3cc89c30e12bcbd6dd740843a51ddd64f3847974cbda73c2e98c9f2 -> ../dm-4
lrwxrwxrwx. 1 root root       7 Dec 21 08:54 docker-thinpool -> ../dm-2
lrwxrwxrwx. 1 root root       7 Dec 21 08:54 docker-thinpool_tdata -> ../dm-1
lrwxrwxrwx. 1 root root       7 Dec 21 08:54 docker-thinpool_tmeta -> ../dm-0


#######change docker data path #####
ExecStart=/usr/bin/dockerd --graph=/data --insecure-registry=10.67.38.90         docker.service
########weave scope########
scope launch 10.67.36.68 10.67.36.69 10.67.36.70 10.67.36.71
######docker healthcheck#######
自 1.12 版本之后，Docker 引入了原生的健康检查实现，可以在 Dockerfile 中声明应用自身的健康检测配置。 HEALTHCHECK 指令声明了健康检测命令，
用这个命令来判断容器主进程的服务状态是否正常，从而比较真实的反应容器实际状态。
缺点是如果healthcheck用curl来检查，那么image里必须装curl命令
Dockerfile example
FROM elasticsearch:5.5
HEALTHCHECK --interval=5s --timeout=2s --retries=12 \
  CMD curl --silent --fail localhost:9200/_cluster/health || exit 1
  
Docker run example
$ docker run --rm -d \
    --name=elasticsearch \
    --health-cmd="curl --silent --fail localhost:9200/_cluster/health || exit 1" \
    --health-interval=5s \
    --health-retries=12 \
    --health-timeout=2s \
    elasticsearch:5.5
	
Composefile example
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost"]
  interval: 1m30s
  timeout: 10s
  retries: 3
  start_period: 40s

Docker Swarm mode command line
$ docker service create -d \
    --name=elasticsearch \
    --health-cmd="curl --silent --fail localhost:9200/_cluster/health || exit 1" \
    --health-interval=5s \
    --health-retries=12 \
    --health-timeout=2s \
    elasticsearch  
Docker service update	
[root@worker1 ~]# docker service update \
> --health-cmd "curl --silent -f http://10.67.36.71/ubuntu/ || exit 1" \
> --health-interval 5s \
> --health-retries 5 \
> yum_repo

##########docker设置timezone######
以下对centos有效
[root@registry ~]# docker run -it --rm  -e 'TZ=Asia/Shanghai' centos:6.10
[root@79e78b181715 /]# date
Mon Oct 22 15:15:59 CST 2018
[root@registry ~]# docker run -it --rm  -v /etc/localtime:/etc/localtime centos:6.10
以下对ubuntu有效
docker run -d -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime rancher/rancher:v2.0.8


############无法安装过期的docker-ce 17.03
A new obsoletes restriction was introduced in docker-ce 17.03.0 and it looks like the yum repo is applying it to all versions of the docker-ce indiscriminately. 
To work around this when installing older versions of docker-ce, you can pass a flag to ignore obsoletes:

$ yum install -y --setopt=obsoletes=0 docker-ce-17.03.2.ce 

docker stop e104
docker create --volumes-from e104f49bc05f --name rancher-stable-data rancher/rancher:v2.0.2
docker run -d --volumes-from rancher-stable-data --restart=unless-stopped -p 80:80 -p 443:443 -e "http_proxy=http://10.67.36.72:3128" -e "https_proxy=http://10.67.36.72:3128" rancher/rancher:stable


5. 使容器内时间与宿主机同步
我们下载的很多容器内的时区都是格林尼治时间，与北京时间差8小时，这将导致容器内的日志和文件创建时间与实际时区不符，有两种方式解决这个问题：

修改镜像中的时区配置文件
将宿主机的时区配置文件/etc/localtime使用volume方式挂载到容器中
第二种方式比较简单，不需要重做镜像，只要在应用的yaml文件中增加如下配置：

volumeMounts:
  - name: host-time
    mountPath: /etc/localtime
    readOnly: true
  volumes:
  - name: host-time
    hostPath:
      path: /etc/localtime

用docker inspect查看容器的volume
[root@rancher ~]# docker inspect --format '{{ .Mounts }}' 80c

docker-compose -f prometheus.yml build
docker-compose -f prometheus.yml push
docker stack deploy prometheus --compose-file prometheus.yml

Registry vs Index
The next weird thing is the idea of a Registry and an Index, and how these are separate things.
An index manages user accounts, permissions, search, tagging, and all that nice stuff that's in the public web interface.

A registry stores and serves up the actual image assets, and it delegates authentication to the index.

When you run docker search, it's searching the index, not the registry. In fact, it might be searching multiple registries that the index is aware of.

When you run docker push or docker pull, the index determines if you are allowed to access or modify the image, 
but the registry is the piece that stores it or sends it down the wire to you after the index approves the operation. 
Also, the index figures out which registry that particular image lives in and forwards the request appropriately.

Beyond that, when you're working locally and running commands like docker images, you're interacting with 
something that is neither an index or a registry, but a little of both.
#########docker rmi清除的是本地image####
清除步骤
untag  当有多個tag关联到同一個image ID
delete  只有一個tag关联到image ID

#########docker容器里使用apt-get####
export http_proxy=

#########build image时使用apt-get####
docker build -t curl:v1.0.0 --build-arg http_proxy=http://10.62.32.27:33128 .

docker build -t chrisnoto/mysql-client-alpine:v1.0.0 --build-arg http_proxy=http://10.62.32.27:33128 .

或dockerfile里设置
ENV http_proxy <HTTP_PROXY>
ENV https_proxy <HTTPS_PROXY>
############docker repo#################
[root@registry yum.repos.d]# cat docker.repo
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/7/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg

##############docker api##################
[root@cobbler ~]# curl http://10.67.51.161:5000/v2/_catalog
{"repositories":["chensen/nginx","chensen/ubuntu"]}
[root@cobbler ~]# curl http://10.67.51.161:5000/v2/chensen/ubuntu/tags/list
{"name":"chensen/ubuntu","tags":["14.04"]}
[root@cobbler ~]# curl http://10.67.51.161:5000/v2/chensen/nginx/tags/list
{"name":"chensen/nginx","tags":["1.11"]}


############setup docker proxy  for centos7.2 docker-engine1.13#######
[root@registry docker.service.d]# cat http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://h7108579:pqhkr88ctw@10.36.6.65:3128" "NO_PROXY=localhost,127.0.0.1,10.67.51.161,registry"
[root@registry docker.service.d]# pwd
/etc/systemd/system/docker.service.d

########### docker registry swift backend############
[root@registry ~]# cat start_registry.sh
#!/bin/bash
docker run -d --name registry -p 5000:5000 \
-e "REGISTRY_STORAGE=swift" \
-e "REGISTRY_STORAGE_SWIFT_USERNAME=admin" \
-e "REGISTRY_STORAGE_SWIFT_PASSWORD='F0xc0nn!23'" \
-e "REGISTRY_STORAGE_SWIFT_AUTHURL=https://10.67.44.66:5000/v2.0" \
-e "REGISTRY_STORAGE_SWIFT_CONTAINER=docker-image" \
-e "REGISTRY_STORAGE_SWIFT_REGION=RegionOne" \
-e "REGISTRY_STORAGE_SWIFT_TENANT=admin" \
-e "REGISTRY_STORAGE_SWIFT_TENANTID=31e6d008df414104ac5e1d42beae316c" \
-e "REGISTRY_STORAGE_SWIFT_INSECURESKIPVERIFY=true" \
registry:2.4

#############docker acc & insecure-registries############
docker daemon.json
[root@registry docker]# cat daemon.json
{
  "registry-mirrors": ["https://aasx6lzt.mirror.aliyuncs.com"],
  "insecure-registries":["10.67.51.161:5000"]
}

############docker push image##################
docker tag nginx 10.67.51.161:5000/chensen/nginx:1.11
docker push 10.67.51.161:5000/chensen/nginx

docker history ubuntu:14.04
docker history 10.67.51.161:5000/chensen/nginx:1.11

#############docker run#####################
docker run -d --name mynginx -p 8080:80 -it  10.67.51.161:5000/chensen/nginx:1.11
docker进入到正在运行的容器内部
docker exec -it test_redis_1 /bin/bash

用docker inspect查看容器的PID
[root@kub ~]# docker inspect --format '{{ .State.Pid }}' 978b5d190b7e
2703
若不存在/var/run/netns目录則创建它，在該目录下创建软链接
ln -s /proc/2703/ns/net /var/run/netns/k8s_nginx-2.549907a9_rc-nginx-2-ksj7g_default_79c4ff77-f804-11e6-a924-fa163e6c70f1_6fcc8701
測試是否成功
[root@kub ~]# ip netns
k8s_nginx-2.549907a9_rc-nginx-2-ksj7g_default_79c4ff77-f804-11e6-a924-fa163e6c70f1_6fcc8701 (id: 1)

docker镜像元数据
1 repository元数据      
/var/lib/docker/image/overlay2/repositories.json
{
    "Repositories": {
        "docker.io/alpine": {
            "docker.io/alpine:latest": "sha256:11cd0b38bc3ceb958ffb2f9bd70be3fb317ce7d255c8a4c3f4af30e298aa1aab",
            "docker.io/alpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430": "sha256:11cd0b38bc3ceb958ffb2f9bd70be3fb317ce7d255c8a4c3f4af30e298aa1aab",
            "docker.io/alpine@sha256:7b848083f93822dd21b0a2f14a110bd99f6efb4b838d499df6d04a49d0debf8b": "sha256:3fd9065eaf02feaf94d68376da52541925650b81698c53c6824d92ff63f98353"
        },
        "docker.io/httpd": {
            "docker.io/httpd:latest": "sha256:94af1f61475235154673372c1f46334c5601a6b182a818b15e6b519c479f9010",
            "docker.io/httpd@sha256:2edbf09d0dbdf2a3e21e4cb52f3385ad916c01dc2528868bc3499111cc54e937": "sha256:94af1f61475235154673372c1f46334c5601a6b182a818b15e6b519c479f9010"
        },
        "docker.io/nginx": {
            "docker.io/nginx:latest": "sha256:73acd1f0cfadf6f56d30351ac633056a4fb50d455fd95b229f564ff0a7adecda",
            "docker.io/nginx@sha256:23e4dacbc60479fa7f23b3b8e18aad41bd8445706d0538b25ba1d575a6e2410b": "sha256:36f3464a21975e5779d081a9e8a78a024c549d1895fc9981d6bd8b67075ebd7b"
        },

2 image元数据           如image history  env  exposedPorts volume  workingDir
/var/lib/docker/image/overlay2/imagedb/content/sha256/[image_id]
3 layer元数据
容器层/可读写层/mountedLayer
/var/lib/docker/image/overlay2/layerdb/mounts/[container_id]
roLayer/只读层
/var/lib/docker/image/overlay2/layerdb/sha256/[chainID]

Docker overlay/overlay存储驱动的目录结构          镜像层+容器层+容器init层

[root@worker2 overlay]# mount| grep 826
overlay on /var/lib/docker/overlay/8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00/merged type overlay (rw,relatime,
lowerdir=/var/lib/docker/overlay/285f2e23139c39f16e810cb617dc95d0e0a491073048d988b77720c581c96b54/root,
upperdir=/var/lib/docker/overlay/8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00/upper,
workdir=/var/lib/docker/overlay/8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00/work)
容器层目录
[root@worker2 overlay]# ls 8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00
lower-id  merged  upper  work
[root@worker2 overlay]# ls 8262a3f76d673872d7e0b508171254ddd945d958eca50da6030897938fe00e00-init
lower-id  merged  upper  work
镜像层目录     (overlay)
[root@worker2 overlay]# ls fee35e5dc34aa817025dd1271b72cf5034789d4720c7f14df4fc3cf8e970dbbb/
root

Docker overlay/overlay2存储驱动 

[root@worker2 overlay2]# mount |grep overlay2
overlay on /var/lib/docker/overlay2/53fd96ea5f28fb08dda295bc8fb73ccb9889a798e219312f84790735757a453d/merged type overlay (rw,relatime,
lowerdir=/var/lib/docker/overlay2/l/PGO7LKP35KHVFZFGB5RE7SW66I:/var/lib/docker/overlay2/l/CDGM4CNSUUG22YUP5KQJL43KPZ:/var/lib/docker/overlay2/l/STHBNIBDZZF3MOJI6EJEVVQV7D:
/var/lib/docker/overlay2/l/YJUUMI6HNKVHMU4NJVBPLBX7WM:/var/lib/docker/overlay2/l/7PMEBTG4UY3LRFUCDBAOKLRWKC:/var/lib/docker/overlay2/l/TM6LZJET5MT75HTQR6BCDAR6HO:
/var/lib/docker overlay2/l/ODCAW6J5NR7UKMXXWNF34UVQX2:/var/lib/docker/overlay2/l/FHYLNDBYPIZIPDLJVJTIRXP46O:/var/lib/docker/overlay2/l/QH6CMH6GLBOHKWZCPP5W5IUHLQ:
/var/lib/docker/overlay2/l/GDCFNASK3B2TGNTABX4ZRH7JRS:/var/lib/docker/overlay2/l/AA4BWASGONR7L62N2RMWREBYVV,
upperdir=/var/lib/docker/overlay2/53fd96ea5f28fb08dda295bc8fb73ccb9889a798e219312f84790735757a453d/diff,
workdir=/var/lib/docker/overlay2/53fd96ea5f28fb08dda295bc8fb73ccb9889a798e219312f84790735757a453d/work)

/var/lib/docker/overlay2/l    取代了原先的镜像层目录
容器层目录有点变化
[root@worker2 53fd96ea5f28fb08dda295bc8fb73ccb9889a798e219312f84790735757a453d]# ls
diff  link  lower  merged  work



